{"title":"Data Science (Advanced)","markdown":{"headingText":"Data Science (Advanced)","containsRefs":false,"markdown":"\n::: {.callout-tip}\n## Conteúdos\n\nAprendizagem supervisionada utilizando o `scikit`.\n\n:::\n\n## KNN\n\n![](images\\knn.png)\n\nK-nearest neighbors (KNN) depende do número de vizinhos (k) e da medida de distância:\n\n+ *euclidean*\n+ *manhattan*\n+ *chebyshev*\n+ *minkowski*\n+ *seuclidean*\n+ *mahalanobis*\n\né também comum parametrizar *weights* (*uniform* ou *distance*)\n\n`sklearn.neighbors.KNeighborsClassifier`\n\nO algoritno pode ser:\n+ `ball_tree`\n+ `kd_tree`\n+ `brute`\n+ `auto`\n\nExemplo\n\ngerar dados de exemplo:  \n```{python}\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\ncenters = [[2, 3], [5, 5], [1, 8]]\nn_classes = len(centers)\ndata, labels = make_blobs(n_samples=150, \n                          centers=np.array(centers),\n                          random_state=1)\n```\n\n```{python}\nimport matplotlib.pyplot as plt\n\n# define cores                          \ncolours = ('green', 'red', 'blue')\n\nfig, ax = plt.subplots()\nfor n_class in range(0, n_classes):\n    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n               c=colours[n_class], s=10, label=str(n_class))\n\nax.legend(loc='upper right')\n\nplt.show()\n```\n\ndividir os dados em teste e treino  \n```{python}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels,\n                                                    train_size=0.8, test_size=0.2, random_state=1)\n```\n\n\n```{python}\ny_train\n```\n\n```{python}\n# Create and fit a nearest-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# a instância do KNN está a ser criada sem parâmetros\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train) \n\ny_pred = knn.predict(X_test)\nprint(\"Previsões do classificador:\")\nprint(y_pred)\nprint(\"Target:\")\nprint(y_test)\n```\n\n```{python}\nfrom sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y_pred, y_test))\n```\nexemplo *ad hoc* para calcular *accuracy*\n```{python}\nfrom sklearn.metrics import accuracy_score\n\nexample_predictions = [0, 2, 1, 3, 2, 0, 1]\nexample_labels      = [0, 1, 2, 3, 2, 1, 1]\nprint(accuracy_score(example_predictions, example_labels))\n```\n\n### KNN com Dataset Iris\n\n```{python}\nfrom sklearn import datasets\n#from sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\ndata, labels = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.7, test_size=0.3, random_state=1)\n```\n\n\n```{python}\n#import matplotlib.pyplot as plt\n\ncolours = ('purple', 'red', 'blue')\nn_classes = 3\n\nfig, ax = plt.subplots()\nfor n_class in range(0, n_classes):\n    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n               c=colours[n_class], s=10, label=str(n_class))\n\nax.legend(loc='upper right')\n\nplt.show()\n```\n\n```{python}\n# Create and fit a nearest-neighbor classifier\n# sklearn.neighbors import KNeighborsClassifier\n\n# instanciação com parâmetros\nknn2 = KNeighborsClassifier(algorithm='auto', \n                           leaf_size=30, \n                           metric='minkowski',\n                           p=2,\n                           metric_params=None, \n                           n_jobs=1, \n                           n_neighbors=3, \n                           weights='uniform')\n                           \nknn2.fit(X_train, y_train) \n\ny_pred = knn2.predict(X_test)\nprint(\"Previsões do classificador:\")\nprint(y_pred)\nprint(\"Target:\")\nprint(y_test)                           \n```\n\n```{python}\n#from sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y_pred, y_test))\n```\n\n```{python}\ny_test_proba = knn2.predict_proba(X_test)\ny_test_proba[:10]\n```\n\n\n```{python}\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\ncm_thr50 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr50,columns = ['pred: 0','pred: 1','pred: 2'],\n                   index = ['real: 0','real: 1','real: 2']))\n```\n\n```{python}\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\n# por omissão average='binary' mas pode também ser None 'micro', 'macro' e 'weighted'\nprint('Precision: %.3f' % precision_score(y_test, y_pred, average='micro'))\nprint('Recall: %.3f' % recall_score(y_test, y_pred, average='micro'))\nprint('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n```\n\n```{python}\nfrom sklearn.metrics import classification_report\n\nprint(\n    f\"Classification report for classifier {knn2}:\\n\"\n    f\"{classification_report(y_test, y_pred)}\\n\"\n)\n```\n\n### Avaliação do modelo\n\n![](images\\complexity.png)\n\n:::: {.columns}\n\n::: {.column}\n![](images\\bias.png)\n:::\n\n::: {.column}\n![](images\\variance.png)\n\n:::\n:::: \n\nBias / Variance trade-off\n\n$Total Error = Bias^2 + Variance + Irreductible Error$\n\n![](images\\balance.png)\nExemplo\n\nCriar dois grupos e prever o grupo a que pertence cada observação de acordo com os seus vizinhos:  \n```{python}\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncenters = [[2, 3], [5, 5]]\nn_classes = len(centers)\ndata, labels = make_blobs(n_samples=200, \n                          centers=np.array(centers),\n                          random_state=1)\n```\n\n```{python}\nimport matplotlib.pyplot as plt\n\ncolours = ('orange', 'blue')\n# n_classes = 2\n\nfig, ax = plt.subplots()\nfor n_class in range(0, n_classes):\n    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n               c=colours[n_class], s=10, label=str(n_class))\n\nax.legend(loc='upper right')\n\nplt.show()\n```\n\ndividir os dados  em treino e teste:  \n```{python}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.8, test_size=0.2, random_state=1)\n```\n\n\nmodelar para diferentes K:  \n```{python}\n# Create and fit a nearest-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\n# vamos aumentar a complexidade aumentando o nº de vizinhos\nk_range = range(1,20)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores[k] = metrics.accuracy_score(y_test, y_pred)\n    scores_list.append(metrics.accuracy_score(y_test, y_pred))\n    \nprint(*scores_list, sep = \", \")\n```\n\n```{python}\nimport matplotlib.pyplot as plt\n\n# vamos fazer o plot da relação entre o aumento do nº de vizinhos k\n# e o valor da accuracy medido nos dados de Teste\nplt.plot(k_range, scores_list)\nplt.xlabel(\"Valor de k no KNN\")\nplt.ylabel(\"Testing Accuracy\")\nplt.show()\n```\n\n```{python}\n# parece não haver ganhos a partir dos 8 vizinhos\nknn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X_train, y_train) \n\nclasses = {0: 'doces', 1: 'salgados'}\n\nx_new = [[5.88213357, 3.75041164], [1.6546771, 3.6924546 ]]\ny_predict = knn.predict(x_new)\ny_predict_prob = knn.predict_proba(x_new)\n\nprint('O elemento {0} gosta mais de {1}'.format(x_new[0],classes[y_predict[0]]))\nprint('O elemento {0} gosta mais de {1}'.format(x_new[1],classes[y_predict[1]]))\n```\n\n### Matriz de confusão\n\nmodelo KNN com K = 6  \n```{python}\n# Create and fit a nearest-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nknn = KNeighborsClassifier(n_neighbors = 6)\n    \nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nscore = metrics.accuracy_score(y_test, y_pred)\nscore\n```\n\nprobabilidades previstas para os positivos (2ª coluna)  \n```{python}\nknn.predict_proba(X_test)[:, 1]\n```\n\n```{python}\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)\n```\n\ncom *labels*  \n```{python}\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n```\n\n```{python}\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\nprint('Precision: %.3f' % precision_score(y_test, y_pred))\nprint('Recall: %.3f' % recall_score(y_test, y_pred))\nprint('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n```\n\ncálculo \"manual\":  \n```{python}\n# Accuracy = TP + TN / P + N\nf_acc = (23 + 14)/ (( 2 + 23) + (14 + 1))\nf_acc\n\n# Recall ou sensitivity ou TPR - linha yes da matriz\n# Dos que são positivos quantos previu?\n# TPR = TP / P\nf_recall = 23 / 25\nf_recall\n```\n\n\n```{python}\n# Precision - coluna yes da matriz\n# Dos que previu como positivos quantos eram realmente positivos? \n# Precision = TP / TP + FP\nf_precision = 23 / (23 + 1)\nf_precision\n\n# Specificity ou Selectivity ou TNR - Linha no da matriz\n# TNR = TN / N \nf_specificity = 14 / (14 + 1)\nf_specificity\n```\n\n### ROC e AUC\n\nReceiver Operating Characteristics (ROC) e Area Under the Curve (AUC)\n\n```{python}\nmodel = knn.fit(X_train, y_train)\n\nfrom sklearn.metrics import roc_curve, RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,model.predict_proba(X_test)[:,1], # \n                                 drop_intermediate=False)\n\nknn_disp = RocCurveDisplay.from_estimator(model, X_train, y_train)\nroc_disp = RocCurveDisplay.from_estimator(model, X_test, y_test, ax=knn_disp.ax_) # o ax diz que este novo objecto fica no objecto anterior\n\nplt.show()\n```\n\n```{python}\nthresholds\n```\n\n```{python}\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n```\n\n```{python}\n# FPR = FP / N\nFPR_50 = 1 / 15\nFPR_50\n```\n\n```{python}\n# TPR = TP / P\nTPR_50 = 23 / 25\nTPR_50\n```\n\n```{python}\nthreshold = 0.33\ny_pred = (model.predict_proba(X_test)[:, 1] > threshold)\n\ncm_thr33 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr33,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n```\n\n```{python}\n# FPR = FP / N\nFPR_33 = 2 / 15\n# TPR = TP / P\nTPR_33 = 24 / 25\nprint('O FPR é {0} quando o TPR é {1}' . format(FPR_33,TPR_33))\n```\n\n```{python}\nthreshold = 0.83\ny_pred = (model.predict_proba(X_test)[:, 1] > threshold)\n\ncm_thr83 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr83,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n```\n\n```{python}\n# FPR = FP / N\nFPR_83 = 1 / 15\n# TPR = TP / P\nTPR_83 = 21 / 25\nprint('O FPR é {0} quando o TPR é {1}' . format(FPR_83,TPR_83))\n```\n\n```{python}\nknn_disp = RocCurveDisplay.from_estimator(model, X_test, y_test)\n\nplt.show()\n```\n\n## Regressão Linear\n\nProblemas mais comuns:\n\n+ relação não linear entre a resposta e predictor\n+ correlação dos termos de erro\n+ variância não constante dos erros\n+ outliers\n+ pontos com *high-leverage*\n+ colinearidade\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"df_prep.csv\"\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n```\n```{python}\ndf_hosp.describe()\n```\n\n```{python}\n# passa indices para coluna\ndf_hosp = df_hosp.reset_index()\n\n# deixar cair a coluna 't_cirurgia'\ndf_hosp = df_hosp.drop(columns=['t_cirurgia']) \n```\n\n```{python}\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = df_hosp.corr().abs()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n\n# seleciona as colunas a remover por serem colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\nprint(to_drop)\n\ndf_hosp = df_hosp.drop(columns=to_drop, axis=1)\n\n```\n\n\n```{python}\n# seleciona para remover as colunas ano e ordem\n# que não trazem informação e queremos evitar que ruído seja aprendido\nto_drop = ['ANO','NORDEM']\ndf = df_hosp.drop(columns=to_drop, axis=1)\n\n# excluimos os registos com valores vazios\ndf1 = df.dropna()\n\n# define a variável target e as features\nX = df1.drop(columns=['C31011']) \ny = df1['C31011'].values # array com os valores\n\n## Typecast da coluna para categórica em pandas\nX['NUTS2'] = pd.Categorical(X.NUTS2)\n\nX.shape\n\n# cria variáveis dummy e faz drop da baseline\nX = pd.get_dummies(X, drop_first = True)\n\n# O nº de colunas aumentou por causa das dummy variables\nX.shape\n```\n\ndivide os dados em treino e teste  \n```{python}\nfrom sklearn.model_selection import train_test_split \n\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, test_size = 0.2 ,random_state = 2002)\nprint(X_train.shape)\nprint(X_test.shape)\n```\n\nfaz o modelo  \n```{python}\nfrom sklearn.linear_model import LinearRegression \n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n```\n\ncalcula o R^2  \n```{python}\ny_pred = lr.predict(X_test)\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n```\n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\n## Regressão logística\n\npara prever um valor contínuo  \n```{python}\n#| warning: false\nfrom sklearn.linear_model import LogisticRegression\n\n# sem qualquer parâmetro\nlogisticRegr = LogisticRegression()\n\nlogisticRegr.fit(X_train, y_train)\n```\n\n```{python}\ny_pred = logisticRegr.predict(X_test)\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n```\n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred)\nplt.xlim(0, 8)\nplt.ylim(0, 8)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\npara classificar  \n```{python}\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\n\n# Vamos ver que há 1797 images (imagens 8 por 8 com uma dimensionalidade de 64)\nprint('Image Data Shape' , digits.data.shape)\n# Print to show there are 1797 labels (integers from 0–9)\nprint(\"Label Data Shape\", digits.target.shape)\n```\n\nvisualizar algumas imagens  \n```{python}\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n plt.subplot(1, 5, index + 1)\n # imshow() função dó módulo pyplot module do matplotlib para mostrar dados como imagem; i.e. num raster 2D.\n plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n plt.title('Training: %i\\n' % label, fontsize = 20)\n \nplt.show()\n\n```\n\n```{python}\n#| warning: false\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=2023)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# sem qualquer parâmetro\nlogisticRegr = LogisticRegression()\n\nlogisticRegr.fit(X_train, y_train)\n```\nprevisão da categoria/classe  \n```{python}\n# Retorna um Array NumPy \n# Prevê para um único caso (imagem)\nprv1 = logisticRegr.predict(X_test[0].reshape(1,-1))\nprv1\n```\nvisualizar a imagem que foi prevista  \n```{python}\n# vamos ver a imagem\nplt.figure(figsize=(20,4))\nplt.imshow(np.reshape(X_test[0], (8,8)), cmap=plt.cm.gray)\n\nplt.show()\n```\n\n```{python}\n# probabilidade de que seja um 5 é de 9.93529410e-01\nprob1 = logisticRegr.predict_proba(X_test[0].reshape(1,-1))\nprob1\n```\n\n```{python}\n#| eval: false\nlogisticRegr.coef_\n```\n\narray com as previões  \n```{python}\n#| eval: false\ny_pred = logisticRegr.predict(X_test)\ny_pred\n```\n\nconfusion matrix 9x9  \n```{python}\n#| eval: false\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n\n#cm = metrics.confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nall_sample_title = 'Accuracy Score: {0}'.format(metrics.accuracy_score(y_pred, y_test))\nplt.title(all_sample_title, size = 15)\n\nplt.show()\n```\n\n## Regularização\n\nregularizações que penalizam os coeficientes grandes\n\n### Ridge Regression \n\n$Função Perda = OLS + \\lambda \\times slope^2$\n\nA recta da ridge regressioné menos inclinada dos que a recta da regressão linear, ou seja, as previsões são menos sensíveis ao predictor (processo de dessensitização dos predictores) \n\n```{python}\n#Importa desta feita os datasets\nfrom sklearn import datasets\n\n#Load dataset de diabetes\ndiabetes_X,diabetes_y = datasets.load_diabetes(return_X_y = True , as_frame = True)\n\n# Colunas que temos nos dados\ndiabetes_X.keys\n```\n\n```{python}\ndiabetes_y\n```\n\n\n```{python}\n# print data(feature)shape\ndiabetes_X.shape\n```\ndividir os dados  \n```{python}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.3,random_state=2510) \n\nX_test\n```\n\nNa função de perda $\\alpha$ é o parâmetro que precisamos selecionar. Um valor $\\alpha$ baixo pode levar a um ajuste excessivo, enquanto um valor $\\alpha$ alto pode levar a um ajuste insuficiente.\n\n```{python}\nfrom sklearn.linear_model import Ridge\n\n# instancia o modelo de regressão Ridge\n# com um valor alfa \nmodel_ridge = Ridge(alpha=0.01)\n\nmodel_ridge.fit(X_train, y_train) \n\ny_pred_ridgetrain= model_ridge.predict(X_train)\n\ny_pred_ridgetest = model_ridge.predict(X_test)\n\ny_pred_ridgetest[0:3]\n```\n\n```{python}\ny_test[0:3]\n```\n\n```{python}\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('RMSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_ridgetrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_ridgetrain)))\n\nprint('RMSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_ridgetest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_ridgetest)))\n```\n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_ridgetest)\nplt.xlim(0, 400)\nplt.ylim(0, 400)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\nExemplo com ficheiro `Hitters`  \n\n```{python}\ndatadir =\"data\\\\\"\nfilename = \"Hitters.csv\"\n\ndf = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf.head()\n\n```\n\n```{python}\ndf.reset_index()\n```\n\n```{python}\n# Vamos descobrir em que colunas há valores em falta\n[col for col in df.columns if df[col].isnull().sum()>0]\n\n# só a coluna salary tem valores em falta\ndf.dropna(inplace = True)\n```\n\n```{python}\n# vamos ver que colunas são do tipo Object\nqual_vars = [col for col in df.columns if df[col].dtype == 'object']\nprint(qual_vars)\n\n# e criar variáveis dummy para as nossas variáveis categóricas\ndf = pd.get_dummies(df,columns= qual_vars,drop_first=True)\ndf.head()\n```\n\n```{python}\n# Separamos a coluna target das outras\nX = df.drop('Salary',axis = 1)\ny = df['Salary']\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Como vemos nas colunas grandes diferenças de escala vamos standardizar\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=2510) \nprint(X_train.shape)\nprint(y_test.shape)\n```\n\n```{python}\nfrom sklearn.linear_model import Ridge\n\n# instanciar o modelo de regressão Ridge\n# com um valor alfa \nmodel_ridge = Ridge(alpha=0.01)\n\nmodel_ridge.fit(X_train, y_train) \n```\n\nver os primeiros resultados\n```{python}\ny_pred_ridgetrain= model_ridge.predict(X_train)\n\ny_pred_ridgetest= model_ridge.predict(X_test)\n\ny_pred_ridgetest[0:3]\n\ny_test[0:3]\n```\n\navaliar o modelo  \n```{python}\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('MSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_ridgetrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_ridgetrain)))\n\nprint('MSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_ridgetest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_ridgetest)))\n```\n\nvisualizar a comparação do previsto com o real  \n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_ridgetest)\nplt.xlim(0, 1000)\nplt.ylim(0, 1000)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\n#### explorar o alpha\n\n```{python}\n#| echo: false\nlist_alpha = 10**np.linspace(-2,10,100)\nlist_alpha\n```\nvizualizar o valor dos coeficientes para cada alpha. Quanto maior é o alpha menor é o valor dos coeficientes  \n```{python}\n#| echo: false  \ncoeff_matrix = {}\n\nfor alpha in list_alpha:\n    model = Ridge(alpha=alpha)\n    model.fit(X,y)\n    coeff_matrix[alpha] = list(model.coef_)\n    \ntmp = pd.DataFrame(coeff_matrix).T\ntmp.index = list_alpha\n\ntmp.head()\ntmp.tail()\n```\n\n```{python}\n#| warning: false\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nplt.figure(figsize = (16,8))\nsns.lineplot(data = tmp, dashes=False)\nplt.axhline(y = 0,linestyle = 'dashed',lw = 0.1,color = 'black')\nplt.xscale('log')\nplt.xticks(list_alpha)\nplt.ylabel('Coeffients',fontsize = 14)\nplt.xlabel('Alpha')\nplt.legend(loc='right')\n\nplt.show()\n```\nalguns alphas  \n```{python}\nprint(list_alpha[0])\nprint(list_alpha[3])\nprint(list_alpha[97])\nprint(list_alpha[99])\n```\n\n```{python}\n# para um valor baixo de alpha os coeficientes são elevados\nprint(coeff_matrix[list_alpha[8]])\n\n# para um valor elevado de alpha os coeficientes são próximos de zero\n# mas não realmente zero\nprint(coeff_matrix[list_alpha[90]])\n```\n\n```{python}\n# linalg.norm(x, ord=None, axis=None, keepdims=False)\n# Calcula a norma matricial ou vetorial, dependendo do ord, 8 normas diferentes\n# Quando ord é None para um vector x retorna a 2-norm\n\nlist_l2_norm = []\n\nfor alpha in list_alpha:\n    list_l2_norm.append(np.linalg.norm(coeff_matrix[alpha]))\n\nprint(list_l2_norm[0])\nprint(list_l2_norm[-1])\n```\n\n```{python}\nplt.figure(figsize = (14,6))\nplt.plot(list_alpha,list_l2_norm)\nplt.xlabel('Alpha')\nplt.ylabel('L2 Norm')\nplt.xlim(0,100)\n```\n\n#### seleccionar o melhor $\\alpha$\n\n```{python}\n#| echo: false\n# Usando o método de validação\nX_train,X_test,y_train,y_test = train_test_split(df.drop('Salary',axis = 1),df['Salary'],test_size = 0.3,random_state = 2023)\n\n# score de validação MSE para cada alpha\nvalidation_score = []\nfor alpha in list_alpha:\n    model = Ridge(alpha=alpha)\n    model.fit(X_train,y_train)\n    validation_score.append(mean_squared_error(model.predict(X_test),y_test)*len(y_test))\n\nvalidation_score[:5]\n```\n\n```{python}\n# visualização dos scores\nsns.lineplot(x=list_alpha,y=validation_score)\nplt.xscale('log')\n\nplt.show()\n```\nver valores por posição\n```{python}\nnp.argmin(validation_score)\n\nvalidation_score[69]\n```\nescolhido o score aplicamos o respectivo alpha  \n```{python}\nmodel_ridge = Ridge(alpha=validation_score[69])\nmodel_ridge.fit(X_train, y_train) \ny_pred_ridgetrain= model_ridge.predict(X_train)\ny_pred_ridgetest= model_ridge.predict(X_test)\ny_pred_ridgetest[0:3]\n```\n\n```{python}\n# reparem que como estamos a partir/split com seed diferente \n# temos diferentes elementos no conjunto de teste\ny_test[0:3]\n```\n\n### Lasso Regression\n\n$Função Perda = OLS + \\alpha \\times \\sum|\\beta|)$\n\nExemplo com dataset do Scikit\n\n```{python}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.3,random_state=2510) \n\nfrom sklearn.linear_model import Lasso\n\n# instancia o modelo de regressão Lasso\n# com um valor alfa \nmodel_lasso = Lasso(alpha=0.01)\n\nmodel_lasso.fit(X_train, y_train) \n```\n\n```{python}\ny_pred_lassotrain = model_lasso.predict(X_train)\n\ny_pred_lassotest = model_lasso.predict(X_test)\n\ny_pred_lassotest[:5]\n\ny_test[:5]\n```\n\n```{python}\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('MSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_lassotrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_lassotrain)))\n\nprint('MSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_lassotest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_lassotest)))\n```\n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_lassotest)\nplt.xlim(0, 400)\nplt.ylim(0, 400)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\n#### explorar o $\\alpha$\n\n```{python}\ndf.head()\n```\n\n```{python}\n# Separamos a coluna target das outras\nX = df.drop('Salary',axis = 1)\ny = df['Salary']\n```\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\n\n# Como vemos nas colunas grandes diferenças de escala vamos standardizar\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=2510) \nprint(X_train.shape)\nprint(y_test.shape)\n```\n\n```{python}\n#| echo: false\ncoeff_matrix_lasso = {}\n\nfor alpha in list_alpha:\n    model = Lasso(alpha=alpha)\n    model.fit(X,y)\n    coeff_matrix_lasso[alpha] = list(model.coef_)\n```\n\n```{python}\ntmp = pd.DataFrame(coeff_matrix_lasso).T\ntmp.index = list_alpha\ntmp.head()\n```\n\n```{python}\n#| echo: false\n# Visualização dos Coeficientes usando Lasso\nplt.figure(figsize = (16,8))\nsns.lineplot(data = tmp, dashes=False)\nplt.axhline(y = 0,linestyle = 'dashed',lw = 0.1,color = 'black')\nplt.xscale('log')\nplt.xticks(list_alpha)\nplt.ylabel('Coeffients',fontsize = 14)\nplt.xlabel('Alpha')\nplt.legend(loc='right')\n```\n\n```{python}\n# para valores pequenos de alpha os coeficientes são grandes\nprint(coeff_matrix_lasso[list_alpha[8]])\n\n# para valores maiores de alpha alguns coeficientes são zero\nprint(coeff_matrix_lasso[list_alpha[30]])\n```\n\n```{python}\n# para valores suficientemente grandes de alpha\n# são todos encolhidos até ao zero \n# ao contrário do que sucedia com o Ridge em que se\n# aproximavam mas nunca eram mesmo zero\nprint(coeff_matrix_lasso[list_alpha[90]])\n```\n\n### Elastic Net Regression\n\n$Função Perda = OLS + \\alpha_1 \\times \\sum\\beta^2 + \\alpha_2 \\times \\sum|\\beta|)$\n\n*ElasticNet* combina as propriedades da regressão *Ridge* e *Lasso.* Funciona penalizando o modelo usando a norma l2 e a norma l1.\n\nNo *scikit-learn*, um modelo de regressão *ElasticNet* é construído usando a classe `ElasticNet`.\n\n```{python}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.3,random_state=2510) \n\nfrom sklearn.linear_model import ElasticNet\n\n# instancia o modelo de regressão ElasticNet\n# com um valor alfa \nmodel_elnet = ElasticNet(alpha = 0.01)\n\nmodel_elnet.fit(X_train, y_train) \n```\n\n```{python}\ny_pred_elnettrain= model_elnet.predict(X_train)\n\ny_pred_elnettest= model_elnet.predict(X_test)\ny_pred_elnettest[:5]\n\nprint()\ny_test[:5]\n```\n\n```{python}\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('MSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_elnettrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_elnettrain)))\n\nprint('MSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_elnettest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_elnettest)))\n```\n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_elnettest)\nplt.xlim(0, 400)\nplt.ylim(0, 400)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\n\n## Cross Validation\n\n![](images\\crossval.png)\n\n### hold-out\n\n```{python}\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n```\n\n```{python}\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.35,random_state=2023)\nX_train.shape\n```\n\n```{python}\n#| eval: true\nmodel=DecisionTreeClassifier()\nmodel.fit(X_train,y_train)\nmod_score = model.score( X_test, y_test) # devolve a accuracy\nmod_score\n```\n\n### Leave-one-out CV\n\n![](images\\loocv.png)\n\nverifica as partições  \n```{python}\nimport numpy as np\nfrom sklearn.model_selection import LeaveOneOut\n\n# criar dados de exemplo\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 3, 4])\n\n# iniciar o metodo de CV\nloo = LeaveOneOut()\n# métodos que dá o número de splits para cross validadtion\nloo.get_n_splits(X)\n\nprint(loo)\n\n# a função split faz os splits\nfor i, (train_index, test_index) in enumerate(loo.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n```\n\nexemplo para calcular Score com dados do Iris  \n```{python}\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n```\n\n```{python}\nmodel = DecisionTreeClassifier()\n\nleave_val = LeaveOneOut()\nleave_val\n\nmod_score = cross_val_score( model, X, y, cv = leave_val) # acertos\nmod_score\n```\n\n```{python}\nprint(np.mean(mod_score)) # accuracy\n```\n\n### Leave-P-out\n\nverificar as partições  \n```{python}\nimport numpy as np\nfrom sklearn.model_selection import LeavePOut\n\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 3, 4])\n\nlpo = LeavePOut(2)\n# métodos que dá o número de splits para cross validadtion\nlpo.get_n_splits(X)\n\nprint(lpo)\n\n# a função split faz os splits\nfor i, (train_index, test_index) in enumerate(lpo.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n```\n\nexemplo com os dados do Iris  \n```{python}\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import LeavePOut\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n```\n\nlistar os acertos\n```{python}\nmodel = DecisionTreeClassifier()\n\nleave_val = LeavePOut(2)\nleave_val\n\nmod_score = cross_val_score( model, X, y, cv = leave_val)\nmod_score\n```\n\ncalcular a *accuracy* que é a média dos acertos  \n```{python}\nprint(np.mean(mod_score))\n```\n\n### K-fold\n\n![](images\\kfold.png)\n\nverifica partições  \n```{python}\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4])\n\nkf = KFold(n_splits=2)\nkf.get_n_splits(X)\n```\n\n```{python}\nprint(kf)\n\nKFold(n_splits=2, random_state=None, shuffle=False)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n```\n\nexemplo com treino do modelo para os Iris  \n```{python}\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import datasets\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n```\n\n```{python}\n# K-fold split\nk_folds = 5\nkf = KFold(n_splits=k_folds, shuffle=True, random_state=1972) # faz o sufle dos dados antes de criar os folds\n\nkf\n```\n\naccuracy  \n```{python}\n# se quisessemos só calcular o score\nmodel=DecisionTreeClassifier()\n\nmod_score =  cross_val_score( model, X, y, cv = kf) # accuracy de cada fold\n\nprint(np.mean(mod_score))\n```\npodemos também calcular 'manualmente' a *accuracy* de cada *fold*  \n```{python}\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = []\n# Aqui só estamos a obter os indices de split\n# por isso servem tanto para X como para y\nfor train_idx, test_idx in kf.split(X):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    #... treina o modelo, etc\n    model=DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n    mod_score = model.score(X_test, y_test)\n    scores.append(mod_score)\n    \n    \nprint(X_train.shape)\nprint(scores)\nprint(np.mean(scores))\n```\n\n### Repeated K-fold\n\nrepete o k-fold mas com folds diferentes  \n```{python}\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 1], [2, 2]])\ny = np.array([0, 0, 1, 1, 0, 1])\n\n\nrkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\nrkf.get_n_splits(X, y)\n```\n\n```{python}\n#| echo: fenced\nRepeatedKFold(n_repeats=2, n_splits=2, random_state=2652124)\nfor i, (train_index, test_index) in enumerate(rkf.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n```\n\ncalcular o score *accuracy* para Iris   \n```{python}\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import datasets\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n```\n\n```{python}\nmodel = DecisionTreeClassifier()\n\nrkf = RepeatedKFold(n_splits=10, n_repeats=2, random_state=2020)\nrkf\n\nmod_score = cross_val_score( model, X, y, cv = rkf)\nprint(mod_score)\n```\n\n### Stratifies K-fold\n\na importância de instanciar  \n```{python}\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, KFold\n\n# criar dados sinteticos para classificação\nmake_class = make_classification(n_samples=500,n_features=3,\n                                 n_redundant=0,n_informative=2,\n                                 n_classes=3,n_clusters_per_class=1,\n                                 random_state=11)\n\ndata = pd.DataFrame(make_class[0],columns=range(make_class[0].shape[1]))\ndata['target'] = make_class[1]\ndata.head()\n```\nver como se distribuiram os dados entre treino e teste  \n```{python}\ntrain_df,test_df = train_test_split(data,test_size=0.2,random_state=11)\nprint(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\\n{data[\"target\"].value_counts() / len(data)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TRAINING SET\\n{train_df[\"target\"].value_counts() / len(train_df)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TEST SET\\n{test_df[\"target\"].value_counts() / len(test_df)}')\n```\n\n```{python}\n# na divisão dos dados em treino e test podemos estartificar pelo 'target'\ntrain_df,test_df = train_test_split(data,test_size=0.2,stratify=data['target'],random_state=11)\nprint(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\\n{data[\"target\"].value_counts() / len(data)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TRAINING SET\\n{train_df[\"target\"].value_counts() / len(train_df)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TEST SET\\n{test_df[\"target\"].value_counts() / len(test_df)}')\n```\n\nexemplo para Iris  \n```{python}\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import datasets\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n```\n\n\n```{python}\nk = 5 # numero de blocos\nskf = StratifiedKFold(n_splits=k, shuffle=True)\nskf\n```\n\n```{python}\nmodel = DecisionTreeClassifier()\nmod_score = cross_val_score(model, X, y,cv=skf)\nmod_score\n\nprint(np.mean(mod_score))\n```\n\n'manualmente'  \n```{python}\nscores = []\n\n# o split do stratified recebe 2 argumentos\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    #... treina o modelo, etc\n    model=DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n    mod_score = model.score(X_test, y_test)\n    scores.append(mod_score)\n    \n    \nprint(X_train.shape)\nprint(scores)\nprint(np.mean(scores))\n```\n\n## Random Forest\n\n![](images\\randomforest.png)\n\n\nconstruir uma Random Forest\n\n```{python}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=4,\n                            n_informative=2, n_redundant=0,\n                            random_state=2023, shuffle=False)\n# print(X)\n# print(y)\n```\n\n```{python}\nclf = RandomForestClassifier(max_depth=2, oob_score=True,random_state=2023)\nclf.fit(X, y)\n\nclf.score(X, y)\n\nclf.oob_score_\n```\n\nprevisoes  \n```{python}\nprint(clf.predict([[0.8, 1.43, 0.65, 2.1]]))\n\nprint(clf.predict_proba([[0.8, 1.43, 0.65, 2.1]]))\n\nprint(clf.predict([[0.5, -0.3, -0.25, 0.1]]))\n```\n\n### Out-of-bag error\n\n![](images\\oob_error.png)\n\nos registos que não foram usados pelo bootsrap podem ser usados para avaliar a árvore criada  \n```{python}\nfrom collections import OrderedDict\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\nRANDOM_STATE = 2023\n\n# Fazer a classificação com 500 datapoints (dados sinteticos)\nX, y = make_classification(\n    n_samples=500,\n    n_features=25,\n    n_clusters_per_class=1,\n    n_informative=15,\n    random_state=RANDOM_STATE,\n)\n\n# X tem 25 features/colunas\nX\n```\n\nParâmetros impotantes\n\n- `n_estimators` será o número de árvores a crescer na floresta, por omissão 100\n- `max_depth` será o máximo para cada árvore, se vazio continua a crescer a árvore até atingir folhas puras ou até que todas as folhas tenham menos que min_samples_split exemplos nas folhas\n- `min_samples_split` será o minimo de exemplo num nó para fazer um split e continuar a crescer a árvore\n- `max_features` será o número de features/colunas a considerar quando procuramos o melhor split para cada crescimento da árvore\n- `oob_score` se quisermos ter o erro out-of-bag tem de estar a True, por omissão está a false e usa o `accuracy_score`\n\n*ensemble* de varias *random forest* mudando o `max_feat`  \n```{python}\n#| echo: false\n# warm_start a True impede a paralelização mas é necessário \n# para registar o OOB error durante o treino\nensemble_clfs = [\n    (\n        \"RandomForestClassifier, max_features='sqrt'\",\n        RandomForestClassifier(\n            warm_start=True, #\n            oob_score=True,\n            max_features=\"sqrt\",\n            random_state=RANDOM_STATE,\n        ),\n    ),\n    (\n        \"RandomForestClassifier, max_features='log2'\",\n        RandomForestClassifier(\n            warm_start=True,\n            max_features=\"log2\",\n            oob_score=True,\n            random_state=RANDOM_STATE,\n        ),\n    ),\n    (\n        \"RandomForestClassifier, max_features=None\",\n        RandomForestClassifier(\n            warm_start=True,\n            max_features=None,\n            oob_score=True,\n            random_state=RANDOM_STATE,\n        ),\n    ),\n]\n\n# Mapear o nome do classificador para uma lista de pares (<n_estimators>, <error rate>) \nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n\n# Range dos valores de `n_estimators` a explorar (numero de árvores para cada random forest)\nmin_estimators = 15\nmax_estimators = 150\n\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1, 5):\n        clf.set_params(n_estimators=i)\n        clf.fit(X, y)\n\n        # Regista o erro OOB para cada setting `n_estimators=i` \n        oob_error = 1 - clf.oob_score_\n        error_rate[label].append((i, oob_error))\n\n\nerror_rate\n```\n\nvisualização do OOB error de acordo com o número de arvores usadas e para os três `max_features`    \n```{python}\n# Prepara o gráfico \"OOB error rate\" vs. \"n_estimators\" \nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"OOB error rate\")\nplt.legend(loc=\"upper right\")\nplt.show()\n```\n\n## Tuning dos hiperparametros\n\n*Grid Search* e *Randomized Search* são as duas técnicas mais amplamente utilizadas no ajuste de hiperparâmetros.\n\n### *GridSearchCV*\n\npesquisa exaustiva para cada combinação de hiperparametros especificados  \n\nexemplo com dados Breast_Cancer  \n```{python}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import datasets\n\n# Load  X e y data a partir do dataset\ncancer = datasets.load_breast_cancer()\n\ncancer.keys()\n\nprint()\ncancer['feature_names']\nprint()\ncancer['target_names']\n```\n\n```{python}\nX = cancer.data\ny = cancer.target\n\nX.shape\n```\n\ndefinir o algoritmo a usar e os respectivosparametros a testar   \n```{python}\nlogModel = LogisticRegression()\n\n# Contruimos a grid de parâmetros\nparam_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]\n\nparam_grid\n```\n\n\na função `GridSearchCV` tem o parametro `refit` por defeito TRUE o que significa quje o treino é feito com os melhores parametros encontrados apos CV.\n\n```{python}\n# por omissão refit = True\nclf = GridSearchCV(logModel, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\nclf\n```\n\nVamos fazer o fit para 3 Folds de:\n- 20 valores possíveis de C\n- 4 possíveis penalty\n- 5 possíveis solvers\n- 4 máximos para iterações <br>\n\nPortanto no total vamos fazer 20x4x5x4 = 1600 candidatos x número de folds (3)\n\nOu seja, 4800 fits\n\n```{python}\n#| eval: false\nbest_clf = clf.fit(X,y)\n\nbest_clf.best_estimator_\n\nprint (f'Accuracy : {best_clf.score(X,y):.3f}')\n```\n\nvamos fazer a previsão para para o primeiro array de X  \n```{python}\n#| eval: false\nX[0]\n\nprint()\npred = clf.predict(X[0].reshape(1, -1))\npred[0]\n\nprint()\npred_prob = clf.predict_proba(X[0].reshape(1, -1))\npred_prob\n```\n\n### *RandomizedSerachCV*\n\n```{python}\nfrom scipy.stats import randint as sp_randint\n\nexemplo = sp_randint(1, 11)\nexemplo\n```\n\n```{python}\n# sp_randint pode ser usado nos parâmetros \n# exemplo é um objecto da rv_class\n# o método rvs vai gerar random variate sample\n# random_state pode ser usado\nexemplo.rvs(20)\n```\n\npode ser usado qualquer `rvs`  \n```{python}\nimport scipy\n\na = scipy.stats.expon(scale=.1)\na.rvs(10)\n```\n\n```{python}\n# distribuição normal com mean .25 stddev 0.1, entre 0 e 1\nb = scipy.stats.truncnorm(a=0, b=1, loc=0.25, scale=0.1)\nb.rvs(10)\n\n# distribuição uniforme entre .01 e .2\nc = scipy.stats.uniform(0.01, 0.199)\nc.rvs(10)\n```\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import datasets\n\n# Load the X and y data from the dataset\ncancer = datasets.load_breast_cancer()\n```\n\n```{python}\nX = cancer.data\ny = cancer.target\n\nX.shape\n```\n\n```{python}\nparam_dist = {\"max_depth\": [3, 5], \n    \"max_features\": sp_randint(1, 11), \n    \"min_samples_split\": sp_randint(2, 11), \n    \"bootstrap\": [True, False], \n    \"criterion\": [\"gini\", \"entropy\"]} \n\nparam_dist\n```\n\n```{python}\n# construir o classificador indicando o nº \n# de árvores a crescer na floresta\nclf = RandomForestClassifier(n_estimators=50) # 50 árvores\nclf\n```\n\n```{python}\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15,random_state=2023)\nX_train.shape\n```\n\n```{python}\nrandom_search = RandomizedSearchCV(clf, \n                                   param_distributions=param_dist, \n                                   n_iter=20, \n                                   cv=5) \n\nrandom_search.fit(X_train, y_train)\nprint(random_search.best_params_)\n```\n\n```{python}\nX_test[12]\n\nprint()\ny_test[12]\n```\n\n```{python}\nrandom_search.predict(X_test[12].reshape(1, -1))[0]\n\nprint()\ncm = confusion_matrix(y_test, random_search.predict(X_test))\nprint(pd.DataFrame(cm,columns = ['pred: 0','pred: 1'],\n                   index = ['real: 0','real: 1']))\n```\n\n### *NestedCV*\n\n\ncomeçamos por gerar dados sintéticos  \n```{python}\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# cria o dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=1, n_informative=10, n_redundant=10)\n\nX\n```\nsão feitos dois loops  \n```{python}\n# configura o loop exterior\ncv_outer = KFold(n_splits=10, shuffle=True, random_state=1999)\n\n# prepara a lista para o loop exterior\nouter_results = list()\n\n# configura o loop CV interior\ncv_inner = KFold(n_splits=3, shuffle=True, random_state=1999)\n\n# o loop interior serve para escolher os parametros\n# define o espaço de procura\nparam_grid = {'n_estimators':  [10, 100, 500],\n              'max_features': [2, 4, 6] }\n```\n\n```{python}\n# define o modelo\nmodel = RandomForestClassifier(random_state=1999)\n```\n\n```{python}\n# para o loop exterior em que vamos ter KFold (10 blocos)\nfor train_idx, test_idx in cv_outer.split(X):\n    \n    # split dos dados com os indexes dados pelo split\n    X_train, X_test = X[train_idx, :], X[test_idx, :]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # define a procura dos hyperparâmetros\n    search = GridSearchCV(model, param_grid, \n                          scoring='accuracy',\n                          cv=cv_inner,\n                          refit=True)\n    # executa a procura fazendo o fit\n    result = search.fit(X_train, y_train)\n    \n    # regista o melhor fit\n    best_model = result.best_estimator_\n    # prevê com o modelo usando o hold-out dataset\n    y_pred = best_model.predict(X_test)\n    # avalia o modelo\n    acc = accuracy_score(y_test, y_pred)\n    \n    # regista os resultados do outer CV na lista\n    outer_results.append(acc)\n    # reporta o progresso\n    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n    \n# sumariza a performance estimada do modelo\nprint('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))\n```\n\nde forma mais condensada  \n```{python}\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import datasets\n\n# Carrega os Dados\ncancer = datasets.load_breast_cancer()\n\nX = cancer.data\ny = cancer.target\n\n# escolhe o modelo\nrf = RandomForestClassifier()\n\n# define a grid\nparam_grid = {'n_estimators': [50, 100, 200],\n              'max_depth': [None, 5, 10],\n              'min_samples_split': [2, 5, 10]}\n\n# define os loops de CV\nouter_cv = KFold(n_splits=3, shuffle=True, random_state=2023)\ninner_cv = KFold(n_splits=5, shuffle=True, random_state=2023)\n\n# Define a procura de hyperparâmetros do loop interior de CV \nmodel = GridSearchCV(\n    estimator=rf, param_grid=param_grid, cv=inner_cv, n_jobs=-1\n)\n\n# Define a seleção e avaliação de modelo no loop exterior de CV \nscores = cross_val_score(model, X, y,\n                        scoring='accuracy',\n                        cv=outer_cv, n_jobs=-1)\n\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n```\n\n## Suport Vector Machines (SVM)\n\n![](images\\svm.png)\n\n```{python}\n# importar os datasets\nfrom sklearn import datasets\n\n# load do dataset\ncancer = datasets.load_breast_cancer()\n\n# ver os dois primeiros registos\nprint(cancer.data[0:2])\n```\ndividir em treino/test  \n```{python}\n# importar a função train_test_split \nfrom sklearn.model_selection import train_test_split\n\n# Split do dataset\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, \n                                                    cancer.target, \n                                                    test_size=0.3,\n                                                    random_state=1980) \n```\n\naplicar o SVM  \n```{python}\n# importar o modelo svm\nfrom sklearn import svm\n\n# criar o classificador svm \n# com um kernel linear\n# para fazer classificação SVC\n# para regressão seria SVR\nclf = svm.SVC(kernel='linear')\n\n# treinar o modelo (fazer o fit)\nclf.fit(X_train, y_train)\n\n# prever com o dataset de teste\ny_pred = clf.predict(X_test)\n```\nver resultados  \n```{python}\n# importar as métricas para avaliar o modelo\nfrom sklearn import metrics\n\n# Model Accuracy: Quantas vezes acerta o classificador\n# Diagonal\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# Model Precision: Dos que previu como positivos quantos eram realmente positivos\n# Coluna direita\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n# Model Recall: Dos que são positivos quantos previu\n# Linha inferior\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n```\n\n```{python}\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: 0','pred: 1'],\n                   index = ['real: 0','real: 1']))\n```\n\n```{python}\nimport numpy as np\nfrom sklearn.model_selection import LearningCurveDisplay\nfrom sklearn.model_selection import cross_validate, ShuffleSplit\n\ncv = ShuffleSplit(random_state=0)\n\ntrain_sizes = np.linspace(0.1, 1, num=10)\ndisp = LearningCurveDisplay.from_estimator(\n    clf,\n    cancer.data, \n    cancer.target,\n    train_sizes=train_sizes,\n    cv=cv,\n    score_type=\"both\",\n    scoring=\"accuracy\",  \n    score_name=\"Accuracy\",\n    std_display_style=\"errorbar\",\n    errorbar_kw={\"alpha\": 0.7},  \n    n_jobs=2,\n)\n\n_ = disp.ax_.set(title=\"Learning curve for support vector machine\")\n```\n\n### SVM kernels\n\n```{python}\nimport pandas as pd  \nimport numpy as np  \nfrom sklearn import datasets\nfrom sklearn.svm import SVC  \nfrom sklearn.metrics import classification_report, confusion_matrix  \nimport matplotlib.pyplot as plt\n\n# Load  X e y data a partir do dataset\niris = datasets.load_iris()\n\n_, ax = plt.subplots()\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\nplt.show()\n```\n\n```{python}\nX = iris.data\ny = iris.target\n\nX.shape\n```\n\n```{python}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\nkernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']\n\n# função para instanciar classificador ed acordo com o tipo escolhido (ktype)\ndef getClassifier(ktype):\n    if ktype == 0:\n        # Polynomial kernal\n        return SVC(kernel='poly', degree=8, gamma=\"auto\")\n    elif ktype == 1:\n        # Radial Basis Function kernal\n        return SVC(kernel='rbf', gamma=\"auto\")\n    elif ktype == 2:\n        # Sigmoid kernal\n        return SVC(kernel='sigmoid', gamma=\"auto\")\n    elif ktype == 3:\n        # Linear kernal\n        return SVC(kernel='linear', gamma=\"auto\")\n```\n\n```{python}\n#| warning: false\nfor i in range(4):\n    # Separate data into test and training sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)# Train a SVC model using different kernels\n    svclassifier = getClassifier(i) \n    svclassifier.fit(X_train, y_train)# Make prediction\n    y_pred = svclassifier.predict(X_test)# Evaluate our model\n    print(\"Evaluation:\", kernels[i], \"kernel\")\n    print(classification_report(y_test,y_pred))\n```\nCross validation para *tunning* de parametros\n\n```{python}\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n```\n\n```{python}\n#| eval: true\n#| echo: false\n\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\ngrid.fit(X_train,y_train)\n```\n\n```{python}\nprint(grid.best_estimator_)\n```\n\n```{python}\ny_pred = grid.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: 0','pred: 1','pred: 2'],\n                   index = ['real: 0','real: 1','real: 2']))\n\nprint(classification_report(y_test,y_pred))\n```\n\n\n### Decision Boundary\n\n```{python}\n# Como vamos mostrar em 2D só podemos ver 2 caracteristicas de cada vez\nX_01 = iris.data[:, :2]\nclf = svm.SVC(C=0.1, gamma=0.1, kernel='poly')\n\nclf.fit(X_01, y)\n```\n\n```{python}\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(clf, X_01, response_method=\"predict\",\n                                              xlabel=iris.feature_names[0], ylabel=iris.feature_names[1],\n                                              alpha=0.5)\n\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=iris.target, edgecolor=\"k\")\nplt.show()\n```\n\n### Boosting - XGBoost\n\n![](images\\boosting.png)\n\n```{python}\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\ndiamonds = sns.load_dataset(\"diamonds\")\ndiamonds.head()\n```\ndataset Diamonds  \n```{python}\n#| warning: false\ndiamonds.shape\n\nprint()\ndiamonds.describe(exclude = np) # sem exclusão de variáveis\n```\n\n\n```{python}\nfrom sklearn.model_selection import train_test_split\n\nX, y = diamonds.drop('price', axis=1), diamonds[['price']]\n\n# Extrair features de texto/categoricas\ncats = X.select_dtypes(exclude=np.number).columns.tolist() # lista com as colunas que não são numericas\n\n# Converter para categorias Pandas\nfor col in cats:\n    X[col] = X[col].astype('category')\n    \nX.dtypes\n```\n\n```{python}\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1923)\n\nimport xgboost as xgb\n\n# Cria matrizes de regressão\ndtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n\n# Define hyperparametros\nparams = {\"objective\": \"reg:squarederror\", \"tree_method\": \"exact\"}\n\nn = 100 # numero de vezes que faz o re-train \nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n)\n```\n\n\n```{python}\nfrom sklearn.metrics import mean_squared_error\n\npreds = model.predict(dtest_reg)\n\nrmse = mean_squared_error(y_test, preds, squared=False)\n\nprint(f\"RMSE of the base model: {rmse:.3f}\")\n```\ncom validation sets\n\n```{python}\n#| echo: false\nevals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n\nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n, # 100\n   evals=evals,\n)\n```\n\n```{python}\n# com early stop\nn = 10000\n\nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n   evals=evals,\n   verbose_eval=50,\n   # Activate early stopping\n   early_stopping_rounds=50 \n)\n```\n\ncom Cross-Validation\n\n```{python}\nparams = {\"objective\": \"reg:squarederror\", \"tree_method\": \"exact\"}\nn = 1000\n\nresults = xgb.cv(\n   params, dtrain_reg,\n   num_boost_round=n,\n   nfold=5, # adicionado o nº de folds\n   early_stopping_rounds=20\n)\n\nresults.tail()\n```\n\n### Stacking\n\n![](images\\stacking.png)\n\navaliar e comparar modelos incluindo stacking  \n```{python}\n# importar a função para fazer um dataset\nfrom sklearn.datasets import make_classification\n\n# definir o dataset com dados sinteticos\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1973)\n\n# caracteristicas do dataset\nprint(X.shape, y.shape)\n```\n\n```{python}\n# todos os imports\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\nfrom matplotlib import pyplot\n\n# função para dar o dataset\n# tal como exemplificado em cima\ndef get_dataset():\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n    return X, y\n\n# função para definir modelos  \ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    return models\n  \n# avaliar um modelo usando cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # estamos a usar a accuracy para o scoring\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n# e agora podemos aplicar as funções definidas \nX, y = get_dataset()\nmodels = get_models()\n\n# avalia os modelos e regista os resultados\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n```\n\n\n```{python}\n# faz boxplot do desempenho dos modelos para comparação\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n```\n\n```{python}\n# define o stacking ensemble dos modelos\ndef get_stacking():\n    # define os modelos de base\n    level0 = list()\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('cart', DecisionTreeClassifier()))\n    level0.append(('svm', SVC()))\n    level0.append(('bayes', GaussianNB()))\n    # define o modelo meta learner\n    level1 = LogisticRegression()\n    # define o stacking ensemble usando o Stacking Classifier e cross-validation\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model\n  \n# redefinimos a lista de modelos a avaliar acrescentando o stacking\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    models['stacking'] = get_stacking()\n    return models\n  \nX, y = get_dataset()\nmodels = get_models()\n\n# avalia os modelos e regista os resultados\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n\n\n```\n\n```{python}\n# faz boxplot do desempenho dos modelos para comparação\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n```\n\n\nprever usando o sctaking de modelos  \n```{python}\n# imports\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# definir o dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n\n```\n\n\n```{python}\n# definir os modelos de base\nlevel0 = list()\nlevel0.append(('lr', LogisticRegression()))\nlevel0.append(('knn', KNeighborsClassifier()))\nlevel0.append(('cart', DecisionTreeClassifier()))\nlevel0.append(('svm', SVC()))\nlevel0.append(('bayes', GaussianNB()))\n\n# definir o modelo meta learner\nlevel1 = LogisticRegression()\n\n# definir o stacking ensemble\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n```\n\n```{python}\n# treinar o modelo com todos os dados (fazer o fit)\nmodel.fit(X, y)\n```\n\n```{python}\n# data é um novo dado\ndata = [[2.47475454,0.40165523,1.68081787,2.88940715,0.91704519,-3.07950644,4.39961206,0.72464273,-4.86563631,-6.06338084,-1.22209949,-0.4699618,1.01222748,-0.6899355,-0.53000581,6.86966784,-3.27211075,-6.59044146,-2.21290585,-3.139579]]\n\n# prever para esse novo dado\nynew = model.predict(data)\nprint('Predicted Class: %d' % (ynew))\n```\n\nstacking para regressão  \n```{python}\n# importar\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import StackingRegressor\nfrom matplotlib import pyplot\n\n# definir o dataset\n# repare que estamos a usar a função make_regression e não a make_classification que usámos antes\ndef get_dataset():\n    X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1985)\n    return X, y\n\n# definir o stacking\ndef get_stacking():\n    # definir os modelos base\n    level0 = list()\n    level0.append(('knn', KNeighborsRegressor()))    # em vez de KNeighborsClassifier\n    level0.append(('cart', DecisionTreeRegressor())) # em vez de DecisionTreeClassifier\n    level0.append(('svm', SVR()))                    # em vez de SVC\n    # definir o modelo meta learner\n    level1 = LinearRegression()\n    # definir o stacking ensemble\n    # usando o StackingRegressor em vez do StackingClassifier\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return model\n  \n# definir a lista dos modelos a avaliar\ndef get_models():\n    models = dict()\n    models['knn'] = KNeighborsRegressor()    # em vez de KNeighborsClassifier\n    models['cart'] = DecisionTreeRegressor() # em vez de DecisionTreeClassifier\n    models['svm'] = SVR()                    # em vez de SVC\n    models['stacking'] = get_stacking()\n    return models\n  \n# definir avaliação de um modelo usando cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # estamos a usar neg_mean_absolute_error em vez da accuracy para o scoring\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n  \nX, y = get_dataset()\nmodels = get_models()\n\n# avalia os modelos e regista os resultados\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n\n# boxplot do desempenho dos modelos para comparação\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n```\n\n## Bonus\n\n### Comparar os modelos com Decision Boundaries \n\n```{python}\n# Code source: Gaël Varoquaux\n#              Andreas Müller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.datasets import make_circles, make_classification, make_moons\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nnames = [\n    \"Nearest Neighbors\",\n    \"Linear SVM\",\n    \"RBF SVM\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"Neural Net\",\n    \"AdaBoost\",\n    \n]\n\n# instanciação dos métodos a usar, já com parametros\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025, random_state=42),\n    SVC(gamma=2, C=1, random_state=42),\n    DecisionTreeClassifier(max_depth=5, random_state=42),\n    RandomForestClassifier(\n        max_depth=5, n_estimators=10, max_features=1, random_state=42\n    ),\n    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n    AdaBoostClassifier(random_state=42),\n]\n\n# cria os dados sinteticos\nX, y = make_classification(\n    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n)\nrng = np.random.RandomState(2000)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [\n    make_moons(noise=0.3, random_state=0),\n    make_circles(noise=0.2, factor=0.5, random_state=1),\n    linearly_separable,\n]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.4, random_state=42\n    )\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n    # Plot the testing points\n    ax.scatter(\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n    )\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers): # a função zip cia o par num tuplo\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n\n        clf = make_pipeline(StandardScaler(), clf)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        DecisionBoundaryDisplay.from_estimator(\n            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n        )\n\n        # Plot the training points\n        ax.scatter(\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n        )\n        # Plot the testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            edgecolors=\"k\",\n            alpha=0.6,\n        )\n\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(\n            x_max - 0.3,\n            y_min + 0.3,\n            (\"%.2f\" % score).lstrip(\"0\"),\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        i += 1\n\nplt.tight_layout()\nplt.show()\n```\n\n\n## Comparação dos Modelos Complexos\n\n![](images\\modelos_complexos.png)\n\n\n<br>","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["epub.css"],"output-file":"800-mod8.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["references.bib"],"theme":{"light":"flatly","dark":"solar"}},"extensions":{"book":{"multiFile":true}}},"epub":{"identifier":{"display-name":"ePub","target-format":"epub","base-format":"epub"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"epub","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":false,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"default-image-extension":"png","html-math-method":"mathml","to":"epub","toc":true,"css":["epub.css"],"output-file":"800-mod8.epub"},"language":{"toc-title-document":"Índice","toc-title-website":"Nesta página","related-formats-title":"Outros formatos","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Fonte","section-title-abstract":"Resumo","section-title-appendices":"Apêndices","section-title-footnotes":"Notas de rodapé","section-title-references":"Referências","section-title-reuse":"Reuso","section-title-copyright":"Direito autoral","section-title-citation":"Citação","appendix-attribution-cite-as":"Por favor, cite este trabalho como:","appendix-attribution-bibtex":"BibTeX","title-block-author-single":"Autor","title-block-author-plural":"Autores","title-block-affiliation-single":"Afiliação","title-block-affiliation-plural":"Afiliações","title-block-published":"Data de Publicação","title-block-modified":"Data de Modificação","callout-tip-title":"Dica","callout-note-title":"Nota","callout-warning-title":"Aviso","callout-important-title":"Importante","callout-caution-title":"Cuidado","code-summary":"Código","code-tools-menu-caption":"Código","code-tools-show-all-code":"Mostrar o código","code-tools-hide-all-code":"Esconder o código","code-tools-view-source":"Ver o código fonte","code-tools-source-code":"Código fonte","code-line":"Linha","code-lines":"Linhas","copy-button-tooltip":"Copiar para a área de transferência","copy-button-tooltip-success":"Copiada","repo-action-links-edit":"Editar essa página","repo-action-links-source":"Ver o código fonte","repo-action-links-issue":"Criar uma issue","back-to-top":"De volta ao topo","search-no-results-text":"Nenhum resultado","search-matching-documents-text":"documentos correspondentes","search-copy-link-title":"Copiar link para a busca","search-hide-matches-text":"Esconder correspondências adicionais","search-more-match-text":"mais correspondência neste documento","search-more-matches-text":"mais correspondências neste documento","search-clear-button-title":"Limpar","search-detached-cancel-button-title":"Cancelar","search-submit-button-title":"Enviar","search-label":"Procurar","toggle-section":"Alternar seção","toggle-sidebar":"Alternar barra lateral","toggle-dark-mode":"Alternar modo escuro","toggle-reader-mode":"Alternar modo de leitor","toggle-navigation":"Alternar de navegação","crossref-fig-title":"Figura","crossref-tbl-title":"Tabela","crossref-lst-title":"Listagem","crossref-thm-title":"Teorema","crossref-lem-title":"Lema","crossref-cor-title":"Corolário","crossref-prp-title":"Proposição","crossref-cnj-title":"Conjetura","crossref-def-title":"Definição","crossref-exm-title":"Exemplo","crossref-exr-title":"Exercício","crossref-ch-prefix":"Capítulo","crossref-apx-prefix":"Apêndice","crossref-sec-prefix":"Seção","crossref-eq-prefix":"Equação","crossref-lof-title":"Lista de Figuras","crossref-lot-title":"Lista de Tabelas","crossref-lol-title":"Lista de Listagens","environment-proof-title":"Comprovação","environment-remark-title":"Comentário","environment-solution-title":"Solução","listing-page-order-by":"Ordenar por","listing-page-order-by-default":"Pré-selecionado","listing-page-order-by-date-asc":"Mais velho","listing-page-order-by-date-desc":"O mais novo","listing-page-order-by-number-desc":"Decrescente","listing-page-order-by-number-asc":"Crescente","listing-page-field-date":"Data","listing-page-field-title":"Título","listing-page-field-description":"Descrição","listing-page-field-author":"Autor","listing-page-field-filename":"Nome do arquivo","listing-page-field-filemodified":"Arquivo modificado","listing-page-field-subtitle":"Subtítulo","listing-page-field-readingtime":"Tempo de leitura","listing-page-field-categories":"Categorias","listing-page-minutes-compact":"{0} minutos","listing-page-category-all":"Tudo","listing-page-no-matches":"Nenhum item correspondente"},"metadata":{"bibliography":["references.bib"],"lang":"pt","date":"2024-03-31"},"extensions":{"book":{"selfContainedOutput":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":true,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":"H","fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":{"text":"\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\areaset[0.50in]{4.5in}{8in}\n"},"include-before-body":{"text":"\\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n   showspaces = false,\n   showtabs = false,\n   breaksymbolleft={},\n   breaklines\n   % Note: setting commandchars=\\\\\\{\\} here will cause an error \n}  \n"},"output-file":"800-mod8.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrbook","classoption":["paper=6in:9in","pagesize=pdftex","headinclude=on","footinclude=on","12pt"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","epub","pdf"]}