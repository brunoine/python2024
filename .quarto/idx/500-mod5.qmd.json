{"title":"Data Science (Intermediate)","markdown":{"headingText":"Data Science (Intermediate)","containsRefs":false,"markdown":"\n::: {.callout-tip}\n## Conteúdos\n\nConceitos de *Data Science* com `Pandas` e `Scikit`.\n\nManipulação de dataFrames para extrair, filtrar e transformar conjuntos de dados.\n\nIntrodução a aprendizagem supervisionada (Regressão e Classificação).\n\n:::\n\n\n## Exploração com Pandas\n\n```{python}\n# usamos por convenção np para Numpy\n# usamos por convenção pd para Pandas\nimport numpy as np\nimport pandas as pd\n```\n\n### Series\n\n```{python}\ndata = pd.Series([0.25, 0.5, 0.75, 1.0])\ndata\n```\n\n```{python}\ndata.values\n\ndata.index\n```\n\n```{python}\ndel data[2]\n```\n\n```{python}\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n```\n\n```{python}\n\npopulation_dict = {'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233,\n                   'Porto': 231834, 'Cascais': 214239, 'Loures': 201349,\n                   'Braga': 193324, 'Almada': 177943}\n                   \npopulation = pd.Series(population_dict)\npopulation\n```\n\n```{python}\npopulation['Braga']\n\npopulation['Braga'] = 201000\n\npopulation['Braga']\n```\n\n\n### DataFrames\n\n```{python}\narea_dict = {'Lisboa': 100.1,'Sintra': 23.8, 'Vila Nova de Gaia': 56.3,\n                   'Porto': 41.4, 'Cascais': 97.1, 'Loures': 11.8,\n                   'Braga': 41, 'Almada': 14.7}\narea = pd.Series(area_dict)\narea\n```\ncriar uma fataframe a partir de series:  \n```{python}\ncities = pd.DataFrame({'population': population,\n                       'area': area})\ncities\n```\n\n```{python}\n#| eval: false\n# reset index com o nome cidade\ncities.reset_index()\n\ncities\n\n```\n\n### Index\n\n```{python}\nindA = pd.Index([1, 3, 5, 7, 9])\nindA\n\nindB = pd.Index([2, 3, 5, 7, 11])\nindA.intersection(indB)\n```\n\na intersecção é muito útil para descobrirmos registos com a mesma identificação em vários conjuntos  \n```{python}\naerod_dict = {'Lisboa': 3, 'Porto': 4, 'Cascais': 1, 'Braga': 7, 'Viseu': 2}\naerod = pd.Series(aerod_dict)\naerod.index\n\naerod.index.intersection(cities.index)\n```\n\n### Reorganizar as DataFrames\n\nPara juntar duas dataframes podemos usar os métodos:\n+ Concatenate (`pd.concat()`)\n+ Append (`df.append`): *As of pandas 2.0, append (previously deprecated) was removed*\n+ Merge (`pd.merge()`)\n\nPor conveniência vamos definir uma função para criar dataframes\n\n```{python}\ndef make_df(cols, ind):\n    \"\"\"Quickly make a DataFrame\"\"\"\n    data = {c: [str(c) + str(i) for i in ind]\n            for c in cols}\n    return pd.DataFrame(data, ind)\n  \n# exemplo de DataFrame\nmake_df('ABC', range(3))\n\n```\n\n#### Método concatenate  \n```{python}\ndf1 = make_df('AB', [1, 2])\ndf2 = make_df('AB', np.arange(3,5))\n\ndf1\ndf2\n```\n\n```{python}\npd.concat([df1, df2])\n```\n```{python}\ndf3 = make_df('AB', range(2))\ndf4 = make_df('CD', range(2))\n\ndf3 \ndf4\n\npd.concat([df3, df4], axis='columns')\n```\n\nDuplicação de indexes\n\n(Uma diferença importante entre np.concatenate e pd.concat é que a concatenação do Pandas preserva os índices, mesmo que o resultado tenha índices duplicados.)  \n```{python}\nx = make_df('AB', [0, 1])\ny = make_df('AB', [2, 3])\ny.index = x.index  # fazer o match dos indices\n\nx\ny\n\npd.concat([x, y])\n```\n\n```{python}\n# Tratar índices repetidos como um erro, fazendo apenas a verificação\ntry:\n    pd.concat([x, y], verify_integrity=True)\nexcept ValueError as e:\n    print(\"ValueError:\", e)\n```\n\n```{python}\n# Ignorando o index das dataframes de origem, e refazendo na nova dataframe o index\nx\ny\n\npd.concat([x, y], ignore_index=True)\n```\n\n```{python}\n# Adicionando chaves MultiIndex para especificar um rótulo para as fontes de dados\nx\ny\n\npd.concat([x, y], keys=['x', 'y'])\n\n```\n\n```{python}\nteste = pd.concat([x, y], keys=['x', 'y'])\n\nteste.reset_index()\n```\n\n\nConcatenação com joins  \n```{python}\ndf5 = make_df('ABC', [1, 2])\ndf6 = make_df('BCD', [3, 4])\n\ndf5\ndf6\npd.concat([df5, df6])\n```\n\npara juntar fazendo a união das colunas de entrada usamos join='outer', que é o valor por omissão  \npara juntar fazendo a interseção das colunas de entrada usamos join='inner'  \n```{python}\npd.concat([df5, df6], join='inner')\n\n```\n\nse quisermos preservar todas as colunas de uma das dataframes devemos fazer reindex das colunas a preservar na outra dataframe axis=1  \n```{python}\npd.concat([df6, df5.reindex(df6.columns, axis=1)])\n\n```\n\n#### Método merge\n\n```{python}\ndf1 = pd.DataFrame({'cidade': [ 'Braga','Lisboa','Sintra', 'Vila Nova de Gaia','Porto', 'Cascais', \n                               'Loures', 'Almada'],\n                    'populacao': [ 193324, 544325, 385989, 304233, 231834, 214239, 201349, 177943]})\ndf2 = pd.DataFrame({'cidade': ['Lisboa','Sintra', 'Vila Nova de Gaia',\n                   'Porto', 'Cascais', 'Loures', 'Braga', 'Almada'],\n                    'area': [ 100.1, 23.8, 56.3, 41.4, 97.1, 11.8, 41,  14.7],\n                   'nuts3': [ '170', '170', '11A', '11A', '170', '170', '112', '170']})\ndf1\ndf2\n```\n\none-to-one join\n\n```{python}\ndf3 = pd.merge(df1, df2)\n\ndf3\n\n```\n\none-to-many join\n\n```{python}\ndf4 = pd.DataFrame({'nuts3': [ '112', '11A', '170'],\n                   'nuts3_dsg': ['Cávado', 'Área Met. Porto', 'Área Met. Lisboa']})\n                   \ndf5 = pd.merge(df3, df4)\n\ndf4\ndf5\n\n```\n\nmany-to-many join\n\n```{python}\ndf6 = pd.DataFrame({'nuts3': [ '112', '112','11A', '170'],\n                   'class': ['Urbano', 'Rural','Urbano', 'Urbano']})\ndf6\npd.merge(df5, df6)\n```\n\nmerge key\n\npodemos indicar a chave para ligar, o primeiro exemplo é equivalente a display('df1', 'df2', \"pd.merge(df1, df2, on='cidade')\")\n\nmas nem sempre as colunas por onde queremos fazer o join têm o mesmo nome,\nnesse caso podemos usar o left_on e o right_on\n\n```{python}\ndf1a = pd.DataFrame({'cidade': ['Lisboa','Sintra', 'Vila Nova de Gaia','Porto', 'Cascais', \n                               'Loures', 'Braga', 'Almada'],\n                    'populacao': [544325, 385989, 304233, 231834, 214239, 201349, 193324, 177943]})\ndf2a = pd.DataFrame({'cidade+100khab': ['Lisboa','Sintra', 'Vila Nova de Gaia',\n                   'Porto', 'Cascais', 'Loures', 'Braga', 'Almada'],\n                    'area': [ 100.1, 23.8, 56.3, 41.4, 97.1, 11.8, 41,  14.7],\n                   'nuts3': [ '170', '170', '11A', '11A', '170', '170', '112', '170']})\n\ndf1a\nprint()\ndf2a\nprint()\npd.merge(df1a, df2a, left_on=\"cidade\", right_on=\"cidade+100khab\")\n```\n\npodemos fazer drop da coluna repetida\n\n```{python}\npd.merge(df1a, df2a, left_on=\"cidade\", right_on=\"cidade+100khab\").drop('cidade+100khab', axis=1)\n\n```\n\nLeft_index e Right_index Keywords\n\n```{python}\ndf1i = df1.set_index('cidade')\ndf2i = df2.set_index('cidade')\n\ndf1i\ndf2i\n```\n\n```{python}\npd.merge(df1i, df2i, left_index=True, right_index=True)\n```\n\nquando temos os indices dos dois lados podemos usar apenas o join\n\n```{python}\n# método antigo\ndf1i.join(df2i)\n```\n\nas keywords left_index e right_index são mais úteis quando pretendemos misturar index e colunas\n\n```{python}\npd.merge(df1i, df2a, left_index=True, right_on='cidade+100khab')\n```\n\n```{python}\n# para fazer reset de um index\ndf1i\ndf1i.reset_index()\n```\n\n```{python}\n# posso continuar a fazer reset do index\n# isso irá acrescentando colunas\ndf1i.reset_index(inplace = True) # o 'inplace = True' altera o dataframe original \ndf1i\ndf1i.reset_index()\n```\n\nInner e Outer Joins\n\n```{python}\ndf11 = pd.DataFrame({'cidade': ['Lisboa','Sintra'],\n                    'populacao': [544325, 385989]})\ndf12 = pd.DataFrame({'cidade': ['Lisboa','Porto', ],\n                    'area': [ 100.1, 97.1]})\ndf11\nprint()\ndf12\nprint()\npd.merge(df11, df12)\n```\n\npor omissão é realizado o inner join mas podemos especificar o tipo de join\n\n```{python}\npd.merge(df11, df12, how='outer')\nprint() # paenas para acrescentr uma linha vazia\npd.merge(df11, df12, how='left')\n```\n\nSobreposição de Nomes de Colunas  \n\n```{python}\ndf13 = pd.DataFrame({'cidade': ['Lisboa','Porto'],\n                    'area': [ 100, 97.5]})\ndf12\ndf13\nprint()\npd.merge(df12, df13, on='cidade')\n```\n\npodemos indicar os sufixos que prentedemos para conhecermos a origem  \n```{python}\npd.merge(df12, df13, on=\"cidade\", suffixes=[\"_12\", \"_13\"])\n```\n\n#### Agregar e Agrupar\n\n```{python}\ndf5\n\ndf5.describe()\n```\n\n```{python}\nprint(df5['populacao'].sum(), df5['populacao'].mean())\n```\n\nvalores agrupados\n\n```{python}\ndf5.groupby('nuts3').populacao.mean()\n\n# ou de forma equivalente\ndf5.groupby('nuts3')['populacao'].mean()\n```\n\no object `groupby` suporta iteração sobre os grupos, isto pode ser útil para inspeccionarmos manualmente os grupos   \n```{python}\n# inspecao da estrutura\nfor (group_name, group_data) in df5.groupby('nuts3'):\n    print(\"{0} shape={1}\".format(group_name, group_data.shape))\n    \n```\n\n```{python}\n# summary statistics por grupo\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Nuts3:\", group_name)\n    print(\"Mean value:\", group_data['populacao'].mean())\n    print(\"Median value:\", group_data['populacao'].median())\n    print(\"Standard deviation:\", group_data['populacao'].std())\n    print()\n```\n\n```{python}\n# inspeccionar valores unicos\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(\"Unique values:\", group_data['cidade'].count())\n    print(\"Unique values:\", group_data['cidade'].nunique())\n    print(\"Unique values:\", group_data['cidade'].unique())\n    print()\n```\n\n```{python}\n# inspeccionar os tops\nN = 1\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(group_data.nlargest(N, 'area'))\n    print()\n```\n\n```{python}\n# inspecao visual\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(group_data.head())\n    print()\n```\n\n```{python}\n# filtrar grupos\nfor group_name, group_data in df5.groupby('nuts3'):\n    if group_data['area'].max() > 100:\n        print(\"Categorias com area > 100:\", group_name)\n```\n\n```{python}\n# Criar visualizacoes por grupo\nimport matplotlib.pyplot as plt\n\ndf5.set_index('cidade', inplace = True )\nfor group_name, group_data in df5.groupby('nuts3'):\n    group_data['populacao'].plot(kind='bar', title=group_name)\n    plt.show()\n```\n\n```{python}\n# para aplicar funcao dentro do grupo\n# Function to calculate percentage change within each group\ndef calculate_relative_percentage(group,col_name):\n    total_sum = group[col_name].sum()\n    group['relative_percentage'] = (group[col_name] / total_sum) * 100\n    return group\n\n# Apply the custom analysis to each group\nresult_df = pd.DataFrame()\nfor name, group in df5.groupby('nuts3'):\n    group = calculate_relative_percentage(group, 'populacao')\n    result_df = pd.concat([result_df, group])\n\nresult_df\n```\n\n```{python}\ndf5.groupby('nuts3')['populacao'].describe()\n```\n\nFunções de Agregação\n\nvarias funções de agregação podem ser aplicadas em simultâneo  \n```{python}\ndf5.groupby('nuts3')['populacao'].aggregate([\"min\", \"median\", \"mean\", \"max\"])\n```\n\nUtilização de filtros\n\n```{python}\ndef filter_func(x):\n    \"\"\"Defino a função de filtro\"\"\"\n    return x['populacao'].std() > 100000\n\n# a função de filtro é aplicado ao grupo\ndf5.groupby('nuts3').filter(filter_func)\n```\n\n```{python}\ndf5.groupby('nuts3')['populacao'].std()\n```\n\nTambém é comum passar as colunas de mapeamento dum dicionário para operações a serem aplicadas nessa coluna    \n```{python}\ndf5.groupby('nuts3').aggregate({'populacao': 'min', 'area': 'max'})\n```\n\nMétodo Transform (conserva o nr de linhas original)\n\nNa transformação, a saída tem o mesmo formato da entrada  \n```{python}\ndef center(x):\n    return x - x.mean()\n\ndf5\nprint()\ndf5.groupby('nuts3')['populacao'].transform(center)\n```\n\nMétodo Apply\n\n```{python}\ndef norm_by_area(x):\n    # x is a DataFrame of group values\n    x['populacao'] /= x['area'].sum()\n    return x\n\ndf5.groupby('nuts3').apply(norm_by_area)\n```\n\n```{python}\n# com o group by por nuts3 somam-se as áreas da nuts3 \n# por exemplo na 11A será 56.3 + 41.4 = 97.7\n# como pop VNGaia = 304233 Porto = 231834 \nprint(\"Pop normalizada por nuts3 de {0} é {1}\". format(\"VNGaia\", 304233/97.7))\nprint(\"Pop normalizada por nuts3 de {0} é {1}\". format(\"Porto\", 231834/97.7))\n```\n\n```{python}\ndf5.groupby('cidade').apply(norm_by_area)\n```\n\n```{python}\n# com o group by por cidade não há lugar a somas... \n# as áreas são VNGaia = 56.3 Porto = 41.4 \n# como pop VNGaia = 304233 Porto = 231834 \nprint(\"Pop normalizada por cidade de{0} é {1}\". format(\"VNGaia\", 304233/56.3))\nprint(\"Pop normalizada por cidade de{0} é {1}\". format(\"Porto\", 231834/41.4))\n```\n\nDiferenças entre Apply e Transform\n\n+ transform() pode receber uma função, uma função de string, uma lista de funções e um dicionário. No entanto, apply() só é pode receber uma função.  \n+ transform() não pode produzir resultados agregados  \n+ apply() funciona com várias séries (várias colunas) ao mesmo tempo. No entanto, transform() só pode funcionar com uma série de cada vez.\n\n```{python}\n# Função de string\ndf5['populacao'].transform('sqrt')\n\n# lista de funções\ndf5['area'].transform([np.sqrt, np.exp])\n\n# Dicionário\ndf5.transform({\n    'populacao': np.sqrt,\n    'area': np.exp,\n})\n```\n\n```{python}\n# Apply consegue produzir agregados\ndf5.apply(lambda x:x.sum())\n```\n\n```{python}\n#| eval: false\n\n## mas não funciona com o transform\ndf5.transform(lambda x:x.sum())\n```\n\n```{python}\ndef subtract_two(x):\n    return x['populacao'] - x['area']\n  \n# apply funciona com várias séries em simultâneo\ndf5.apply(subtract_two, axis=1)\n```\n\n```{python}\n#| eval: false\n\n# mas o transform não\ndf5.transform(subtract_two, axis=1)\n```\n\nEspecificar as Split Keys para os grupos\n\nPodemos fazer grupos com uma lista, série ou index a especificar as keys pelas quais se faz o agrupamento. A key pode ser uma série ou lista com o comprimento da DataFrame.\n\n```{python}\nL = [0, 1, 0, 1, 2, 0, 3, 1]\ndf5.groupby(L).sum()\n```\n\n```{python}\n# forma mais verbosa equivalente ao que temos usado até agora\n# aqui explictamos que a key é df5['nuts3'] e não apenas 'nuts3'\ndf5.groupby(df5['nuts3']).sum()\n```\n\n\n```{python}\n# com um dicionário\n\ndf2g = df5.set_index('nuts3')\nmapping = {'11A': 'norte', '112': 'norte', '170': 'centro'}\ndf2g\nprint()\ndf2g.groupby(mapping).sum()\n```\n\n#### Pivot Tables\n\n```{python}\nimport seaborn as sns # importamos esta package para termos acesso a um dataset\n\ntitanic = sns.load_dataset('titanic')\n\ntitanic.head()\nprint()\ntitanic.describe()\n```\n\nPreparar manualmente a Pivot Table\n\n```{python}\ntitanic.groupby('sex')[['survived']].mean()\nprint()\ntitanic.groupby(['sex', 'class'], observed=True)['survived'].mean().unstack() # unstack para ter a pivot table\n```\n\n```{python}\nprint()\ntitanic.pivot_table(index='sex', columns='class',\n                    values='survived', aggfunc='mean', observed=True)\n```\n\nSyntax Pivot Table\n\n```{python}\ntitanic.pivot_table('survived', index='sex', columns='class', aggfunc='mean')\nprint()\nage_group = pd.cut(titanic['age'], [0, 18, 80])\nage_group\nprint()\n# multilevel pivot table\ntitanic.pivot_table('survived', index=['sex', age_group], \n                    columns='class', aggfunc='mean')\n```\n\nSummary Statistics na DataFrame\n\n```{python}\ntitanic.describe()\n```\n\ntodas estas funções estão disponíveis como fomos vendo nos exemplos anteriores\n\n```{python}\ntitanic.pivot_table('fare', index=['sex'], \n                    columns='class', aggfunc='mean')\n                    \nprint()\n\ntitanic.pivot_table('survived', index=['class'], aggfunc='count')\n```\n\n## Estatisticas Oficiais\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"PT_2012_Hosp.csv\"\n```\n\nler os dados:  \n```{python}\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nprint()\ndf_hosp.describe()\n\ndf_hosp = df_hosp.reset_index() # passar o index para uma coluna; podia ser feito com inplace = True\nprint()\ndf_hosp.head()\n```\n\n### Pre-processamento\n\n#### Exclusão de colunas:  \n```{python}\n# exclusão da coluna nordem\ndf_hosp = df_hosp.drop(columns=['NORDEM'])\ndf_hosp.head()\n```\n\n#### Variáveis com demasiadas categorias:  \n```{python}\ndf_hosp['NUTS2'].value_counts().sort_values()\nprint()\ndf_hosp['DTCC_COD'].value_counts()\nprint()\ndf_hosp['DTCC_COD'].value_counts(normalize=True) # resultados percentuais\n\n\n```\n\nAgregar todos os municipios com menos de dez por cento das ocorrências na amostra:  \n```{python}\n# verifica contagens para a coluna DTCC_COD\ndf_hosp['DTCC_COD_COUNT']= df_hosp.DTCC_COD.map(df_hosp.DTCC_COD.value_counts(normalize=True)) # Constroi a nova coluna\n\n# Correr sem criar a coluna primeiro\ndf_hosp.loc[df_hosp['DTCC_COD_COUNT'] < 0.1, 'DTCC_COD_NEW'] = 'outro' # Constroi a nova coluna\ndf_hosp.loc[df_hosp['DTCC_COD_COUNT'] >= 0.1, 'DTCC_COD_NEW'] = df_hosp['CC_DSG'] # atualiza os valores em falta\n\nprint()\ndf_hosp.loc[:,['DTCC_COD_NEW','DTCC_COD', 'CC_DSG']] # todas as linhas e as colunas selecionadas\n```\n\n```{python}\n# exclusão das colunas já tratadas\ndf_hosp = df_hosp.drop(columns=['DTCC_COD', 'CC_DSG','DTCC_COD_COUNT'])\ndf_hosp.head()\n```\n\n#### Missing data\n\nsem o scikit\n\nverificar onde temos dados em falta o método `isna()`    \n```{python}\n#Check missing data\ndf_hosp.isna().sum()\n```\n\npor haver linhas com todos os valores missing então há colunas que devem ser excluídos.\n\n```{python}\ndf_hosp[df_hosp.isna().any(axis=1)]\n```\n\n```{python}\n# dropna pode ser parametrizado para linhas ou colunas, e até thresholds\ndf_hosp1 = df_hosp.dropna()\ndf_hosp1\n```\n\n```{python}\n# vamos fazer uma cópia para testes\ndf1 = df_hosp1.copy()\ndf1\n```\n\nVamos introduzir um NaN num registo para experimentarmos outras formas de tratamento\n```{python}\ndf1.loc[0:3, :]\n\ndf1.loc[2,'C21001'] = np.nan\n\ndf1.loc[0:3, :]\n```\n\nvamos inserir o valor médio da C21001 (Médicos - Especialistas - Total) para a nuts 16, mas de uma forma genérica para qualquer que fosse a coluna numérica:    \n```{python}\n# passo 1 seleccionar a/as colunas onde há valores missing\ncols_missing = df1.isna().sum().index[df1.isna().sum().values >0].tolist()\nprint(cols_missing)\n\n# passo 2 seleccionar o conjunto de colunas da dataframe que são numéricas\ncols_numerical = df1.select_dtypes(include=['number']).columns.tolist()\nprint(cols_numerical)\n\n# passo 3 imputar a mediana na for missing data\ncols_numerical_missing = [ col for col in cols_missing if (col in cols_numerical)] # loop for condicional\ndf1[cols_numerical_missing] = df1[cols_numerical_missing].fillna(df1.groupby('NUTS2')[cols_numerical_missing].transform('mean'))\n\ndf1.loc[0:3, :]\n```\n\nquando não aplicamos o transform temos resultados diferentes:  \n```{python}\n# obtemos uma dataframe com 7 linhas e index na nuts2\ndf1.groupby('NUTS2')[cols_numerical_missing].mean()\nprint()\n\n# obtemos o mesmo nº de linhas que a dataframe tinha com a média calculada pelo group by\n# sem index\ndf1.groupby('NUTS2')[cols_numerical_missing].transform('mean')\nprint()\n\n# verificação das médias\ndf1.groupby('NUTS2')['C21001'].mean()\n```\n\nusando o `scikit`  \n\n```{python}\ndf1.loc[2,'C21001'] = np.nan\ndf1.loc[0:3, :]\n\n# o simple imputer não pode ser usado em colunas categóricas \n# com a estratégia de média, só em colunas numéricas\ndf1 = df1.drop(columns = ['DTCC_COD_NEW'])\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(df1)\n```\n\ne agora atua sobre a dataframe:  \n```{python}\nx = pd.DataFrame(imputer.transform(df1))\nx\n\n```\n\no método `SimpleImputer` do `scikit.impute` não suporta o `groupby` directamente mas é muito útil e suporta categorias quando a estratégia selecionada é '`most_frequent`' ou '`constant`' além disso pode ser usado indirectamente o `groupby` recorrendo a uma função lambda aplicada nas colunas dentro da função `transform`\n\n```{python}\nx.iloc[:4,:6]\n\ndf1.iloc[:4,:6]\n\ndf1['C21001'] = df1.groupby('NUTS2')['C21001'].transform(\n    lambda col: imputer.fit_transform(col.to_frame()).flatten(),)\nprint()\n\ndf1.iloc[:4,:6]\n\n```\n\nImputação de Variáveis Categóricas  \n\n```{python}\n# Seleciona todas as colunas categóricas e atribui-lhes \n# um novo nível criado 'missing'\ncols_categorical = df_hosp.select_dtypes(include=['object']).columns.tolist()\n\ndf_hosp[cols_categorical] = df_hosp[cols_categorical].fillna('missing')\nprint(cols_categorical)\n```\n\nexemplo em que usamos vários imputadores em simultâneo, seria possível usando o ColumnTransformer do módulo `sklearn.compose` e especificando as colunas a fectar é apresentado abaixo:  \n```{python}\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nA = [[np.nan,2,'Jose'],[4,np.nan,'Maria'],[7,8,'Joao'],[10,5,np.nan]]\n\ncol_trans = ColumnTransformer(\n[('imp0', SimpleImputer(strategy='constant', fill_value=1), [0]),\n ('imp1', SimpleImputer(strategy='mean'), [1]),\n ('imp2', SimpleImputer(strategy='constant', fill_value='desconhecido'), [2])],\nremainder='passthrough')\n\ncol_trans.fit_transform(A)\n```\n\nImputação com o vizinho mais próximo\n\n```{python}\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputer.fit_transform(X)\n```\n\n#### Variáveis Categóricas\n\n```{python}\n## Typecast da coluna para categoria em usando o pandas\ndf1['NUTS2'] = pd.Categorical(df1.NUTS2)\n\ndf1.dtypes\n\nprint()\n## Typecast da coluna para categoria em python\ndf_hosp11 = df_hosp1.copy()\ndf_hosp11['NUTS2']= df_hosp1.NUTS2.astype('category')\ndf_hosp11.dtypes\n```\n\nCriação de Variáveis Categóricas\n\ncriar colunas à custa de outras colunas directamente usando o método `map`:  \n```{python}\ndf_hosp2 = df1.copy()\ndf_hosp2\n\ndef label(value): # função para criar uma variável dummy\n    if value == 0:\n        return \"no\"\n    if value > 0:\n        return \"yes\"\n\ndf_hosp2['t_cirurgia'] = df_hosp2['C21071'].map(label)\ndf_hosp2.head()\n```\n\nVariáveis Dummy\n\n```{python}\n# Definimos y como o nosso target\nX = df_hosp2.drop(columns=['t_cirurgia'])\ny = df_hosp2['t_cirurgia'].values\n\n#Transforma as variáveis categoricas em dummies com drop da baseline\ndf_x = pd.get_dummies(X, drop_first = True)\n\ndf_x.head()\n```\n\nVariáveis Dummy com Scikit\n\nScikit tem vários encoders os mais comuns são o `OrdinalEncoder` e o `HotEncoder`.\n\n+ `OrdinalEncoder` é usado para transformar variáveis categóricas em numéricas.\n+ `OneHotEncoder` é usado para transformar variáveis categóricas em variáveis dummy.\n\n(criar encoder, fazer o fit e efectuar a transformação)\n\n```{python}\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')\n\n# Definimos X_sk como a coluna para criar dummies nos exemplos scikit\n# atenção df_hosp2[['NUTS2']] é uma dataframe, df_hosp2['NUTS2'] é uma serie\nX_sk = df_hosp2[['NUTS2']]\n\nprint(type(df_hosp2[['NUTS2']]))\nprint()\nencoder = encoder.fit(X_sk)\n\nX_encoded = pd.DataFrame(encoder.transform(X_sk).toarray()) # transforma a matriz em dataframe\nX_encoded.head()\nprint()\n\nX_encoded = X_encoded.rename(columns = {0 : \"Nut0\", 1 : \"Nut1\", 2 : \"Nut2\", 3 : \"Nut3\"\n                                       , 4 : \"Nut4\", 5 : \"Nut5\", 6 : \"Nut6\"}) # renomeia as colunas\nX_encoded\nprint()\n\n# merge com a dataframe dos hospitais\ndf_hosp2 = df_hosp2.join(X_encoded)\ndf_hosp2 = df_hosp2.drop(columns = 'NUTS2') # drop da coluna original\ndf_hosp2.head()\n```\n\n#### Variáveis Correlacionadas\n\nem modelos de regressão podemos ter problemas com variáveis correlacionadas (multicolinearidade)  \n```{python}\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = df_x.corr().abs()\ncor_matrix\nprint()\n\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\nprint(upper_tri)\nprint()\n\n```\n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# o heatmap é uma boa forma de visualizar as matrizes de correlações\nplt.figure(figsize = (20,20))\nsns.heatmap(cor_matrix)\n\nplt.show()\n```\n\n```{python}\n# seleciona para remover as colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\nprint(to_drop)\n```\n\n```{python}\ndf_hosp3 = df_x.drop(columns=to_drop, axis=1)\ndf_hosp3.head()\n```\n\n#### Class Imbalance\n\n```{python}\nfrom sklearn.linear_model import LogisticRegression\n\n# exemplo de declaração do regressor balanceando as classes\nLr = LogisticRegression(class_weight='balanced') # declarar uma regressão logística com classes balanceadas\n```\n\n#### Normalização e Standardização\n\n```{python}\nshape = (4, 2)\nshape\n\nimport numpy.random as rd\n\n# *shape faz unpack do tuplo shape\nrd.rand(*shape) # gerador de samples de uma distribuição uniforme\n```\n\n```{python}\n\n# random.RandomState.lognormal(mean=0.0, sigma=1.0, size=None) \n# gerador de samples de uma distribuição log-normal\nrd.rand(*shape) * rd.lognormal(0, 1, shape) \n\n```\n\n```{python}\nimport numpy.random as rd\nimport pandas as pd\nfrom sklearn.preprocessing import Normalizer\nimport seaborn as sns\n\nimport warnings # para suprimir warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nshape = (100, 2)\n# *shape faz unpack do tuplo shape\ndf = pd.DataFrame(rd.rand(*shape) * rd.lognormal(1, 0.4, shape)\n                  , columns=[\"weight\", \"age\"]) # gerar um dataframe com 100 linhas e 2 colunas\nndf = pd.DataFrame(Normalizer(norm=\"l2\").fit_transform(df),\n                   columns=[\"norm_weight\", \"norm_age\"]) # normalizar os dados\n\n# kernel density estimate (KDE) plot é um método para \n# visualizar a distribuição de observações num dataset\nsns.kdeplot(data=pd.concat([df, ndf], axis=1), fill=True, \n            common_norm=False, palette=\"crest\",alpha=.5, linewidth=1,)\n            \nplt.show()\n            \ndf\n```\n\n```{python}\nfor d in [df]:\n    sns.pairplot(d.reset_index(), hue=\"index\", diag_kind=None)\n    \nplt.show()\n```\n\n```{python}\n# repete o exemplo com shape(50,2) faz standardization\nimport numpy.random as rd\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\nshape = (50, 2)\n# *shape faz unpack do tuplo shape\ndf = pd.DataFrame(rd.rand(*shape) * rd.normal(10, 6, shape),\n                  columns=[\"weight\", \"age\"])\nndf = pd.DataFrame(StandardScaler().fit_transform(df),\n                   columns=[\"norm_weight\", \"norm_age\"])\n\n# kernel density estimate (KDE) plot é um método para \n# visualizar a distribuição de observações num dataset\nsns.kdeplot(data=pd.concat([df, ndf], axis=1), \n            fill=True, common_norm=False, palette=\"crest\",\n            alpha=.5, linewidth=1,)\n            \nplt.show()\n```\n\n```{python}\nfor d in [df]:\n    sns.pairplot(d.reset_index(), hue=\"index\", palette=\"crest\", diag_kind=None)\n\nplt.show()\n```\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Define a pre-processing pipeline\npipeline = Pipeline([\n('selector', VarianceThreshold()),\n('scaler', StandardScaler()),\n('KNN', KNeighborsClassifier())])\n```\n\n\n```{python}\n#| eval: false\n\n#Simple pipeline execution with default parameters\npipeline.fit(X, y)\nprint('Training set score: ' + str(pipeline.score(X,y)))\n\npipeline.predict(X)\n\ny\n\npipeline.predict_proba(X)\n```\n\n### Gravar os dados\n\n```{python}\n#| eval: false\n\ndf_prep = pd.concat([X, pd.DataFrame(y, columns=[\"t_cirurgia\"])], axis=1)\n\ndf_prep.head()\n\n# Exportação da dataframe\nfileout = 'df_prep.csv'\ndf_prep.to_csv(f\"{datadir}{fileout}\",  index=False)\n```\n\n## Introdução a Machine Learning\n\n### Regressão\n\n**QUESTÃO**:   \nCom este conjunto de dados que inclui o número de médicos e enfermeiros em várias especialidades será que consigo estimar o nº de total de Enfermeiros - Especialistas - Em Saúde Infantil e Pediátrica em cada hospital? \n\n+ Coluna **C31011**\n\n```{python}\n#| eval: true\n\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"df_prep.csv\"\n```\n\n```{python}\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nprint()\ndf_hosp.describe()\n```\n\n```{python}\ndf_hosp = df_hosp.reset_index() # passar o index para uma coluna; podia ser feito com inplace = True\n```\n\n```{python}\n# Definimos y como o nosso target\nX = df_hosp.drop(columns=['t_cirurgia'])\ny = df_hosp['t_cirurgia'].values\n```\n\n```{python}\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = X.corr().abs()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n```\n\n```{python}\n# seleciona para remover as colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\nprint(to_drop)\n```\n\n```{python}\ndf_hosp = df_hosp.drop(columns=to_drop, axis=1) # drop das colunas altamente correlacionadas\n```\n\n```{python}\ndf_hosp.head()\n```\n\nprimeiro modelo de regressão linear com uma variavél explicativa:  \n```{python}\nimport statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001',data = df_hosp).fit()\nprint(est.summary())\n```\n\n```{python}\n# Este é o R entre as 2 variáveis\nr = cor_matrix.loc['C31001','C31011']\nr\n```\n\n```{python}\n# E este é o r quadrado\nr2 = r**2\nr2\n```\n\nmodelos com duas variáveis explicativas:    \n```{python}\nimport statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001 + C21011',data = df_hosp).fit()\nprint(est.summary())\n```\n\n```{python}\n# import statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001 + C21361',data = df_hosp).fit()\nprint(est.summary())\n```\n\nmodelo com todas as variáveis explicativas:  \n```{python}\n# reorganiza as colunas para colocar a coluna target no fim\nlast_cols = ['C31011']\nfirst_cols = [col for col in df_hosp.columns if col not in last_cols]\n\ndf = df_hosp[first_cols+last_cols]\ndf.head()\n```\n\n```{python}\nstring_cols = ' + '.join(df.columns[:-1])\nest = smf.ols('C31011 ~ {}'.format(string_cols),data = df).fit()\nprint(est.summary())\n```\n\n::: {.callout-warning}\na variável C31001 permite construir Y directamente!\n:::\n\n```{python}\ndf = df.drop(columns='C31001', axis=1)\ndf.head()\n```\n\ne voltamos a fazer o modelo\n```{python}\nstring_cols = ' + '.join(df.columns[:-1])\nest = smf.ols('C31011 ~ {}'.format(string_cols),data = df).fit()\nprint(est.summary())\n```\n\n#### Regressão com Scikit\n\n```{python}\ndf_hosp.head()\n```\n\n```{python}\n# seleciona para remover as colunas ano e ordem\nto_drop = ['ANO','NORDEM']\ndf = df_hosp.drop(columns=to_drop, axis=1) # drop das colunas ano e ordem\n```\n\n```{python}\ndf1 = df.dropna() # drop das linhas com valores missing\n```\n\n```{python}\n# define a variável target e as features\nX = df1.drop(columns=['C31011'])\ny = df1['C31011'].values\n```\n\n```{python}\n## Typecast da coluna para categoria em pandas\nX['NUTS2'] = pd.Categorical(X.NUTS2)\n#X.dtypes\n\nX.shape\n```\n\n```{python}\n# cria variáveis dummy e faz drop da baseline\nX = pd.get_dummies(X, drop_first = True)\n\nX.shape\n```\n\n```{python}\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\n\nthresholder = VarianceThreshold(threshold=.2) # define o threshold de variância \nX = thresholder.fit_transform(X) # aplica o threshold para excluir variáveis com baixa variância\n\nX.shape\n```\n\n```{python}\nfrom sklearn.model_selection import train_test_split \n\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, test_size = 0.2 ,random_state = 2002)\nprint(X_train.shape)\nprint(X_test.shape)\n```\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train) # escalar os dados em pre-processamento\n\nscaler.mean_\nprint()\nscaler.scale_\n```\n\n```{python}\nX_scaled = scaler.transform(X_train)\n\nX_train\n\nprint()\n\nX_scaled\n```\n\n```{python}\n\nfrom sklearn.linear_model import LinearRegression \n\nlr = LinearRegression()\nlr.fit(X_scaled,y_train) # treina o modelo\n\nlr.coef_\nprint()\ny_pred = lr.predict(X_test) # faz a previsão mas não com os dados escalados\n```\n\n```{python}\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n```\n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred)\nplt.xlim(0, 10)\nplt.ylim(0, 1000)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\n::: {.callout-warning}\nfizemos o `scale` dos dados de train mas não dos dados de teste.\n:::\n\nVamos tentar de outra forma  \n```{python}\n# define a variável target e os predictors\nX_2nd = df1.drop(columns=['C31011'])\ny_2nd = df1['C31011'].values\n```\n\n```{python}\n## Typecast da coluna para categoria em pandas\nX_2nd['NUTS2'] = pd.Categorical(X_2nd.NUTS2)\n# cria variáveis dummy e faz drop da baseline\nX_2nd = pd.get_dummies(X_2nd, drop_first = True)\n```\n\n```{python}\n#Split data for machine learning\nX_2nd_train, X_2nd_test, y_2nd_train, y_2nd_test = train_test_split(X_2nd,  y_2nd, test_size = 0.2 ,random_state = 2002)\nprint(X_2nd_train.shape)\nprint(X_2nd_test.shape)\n```\n\n```{python}\nlr2 = LinearRegression()\nlr2.fit(X_2nd_train,y_2nd_train)\n```\n\n```{python}\nlr2.coef_\n```\n\n```{python}\ny_2nd_pred = lr2.predict(X_2nd_test)\n\n```\n\n```{python}\n# we choose the x axis as index, chossing year will give a discrete plot\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_2nd_test, y= y_2nd_pred)\nplt.xlim(-2, 20)\nplt.ylim(-2, 20)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n```\n\n```{python}\ny_2nd_test\n\ny_2nd_pred\n\nr2_score(y_2nd_test, y_2nd_pred)\n```\n\ntemos de excluir C31001!!\n```{python}\n  # define a variável target e os predictors\nX_3rd = df1.drop(columns=['C31011', 'C31001'])\ny_3rd = df1['C31011'].values\n```\n\n```{python}\n## Typecast da coluna para categoria em pandas\nX_3rd['NUTS2'] = pd.Categorical(X_3rd.NUTS2)\n# cria variáveis dummy e faz drop da baseline\nX_3rd = pd.get_dummies(X_3rd, drop_first = True)\n```\n\n```{python}\n#Split data for machine learning\nX_3rd_train, X_3rd_test, y_3rd_train, y_3rd_test = train_test_split(X_3rd,  y_3rd, test_size = 0.2 ,random_state = 2002)\nprint(X_3rd_train.shape)\nprint(X_3rd_test.shape)\n```\n\n```{python}\nlr3 = LinearRegression()\nlr3.fit(X_3rd_train,y_3rd_train)\nlr3.coef_\n```\n\n```{python}\ny_3rd_pred = lr3.predict(X_3rd_test)\n```\n```{python}\nr2_score(y_3rd_test, y_3rd_pred)\n```\n\n#### Avaliação dos modelos de regressão\n\n```{python}\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n```\n\n```{python}\nprint(\"                   MAE             MSE           RMSE          MedAE         R2\")\nprint(\"     1ª Tentativa {:.12f} {:.10f} {:.8f} {:.8f} {:.8f}\"\n        .format(mean_absolute_error(y_test, y_pred),\n                mean_squared_error(y_test,y_pred),\n                mean_squared_error(y_test,y_pred,squared=False), # dá a raiz quadrada do MSE\n                median_absolute_error(y_test,y_pred),\n                r2_score(y_test,y_pred)))\nprint(\"     2º Tentativa: {:.12f} {:.10f} {:.10f} {:.10f} {:.12f}\"\n        .format(mean_absolute_error(y_2nd_test, y_2nd_pred),\n                mean_squared_error(y_2nd_test,y_2nd_pred),\n                mean_squared_error(y_2nd_test,y_2nd_pred,squared=False),\n                median_absolute_error(y_2nd_test,y_2nd_pred),\n                r2_score(y_2nd_test,y_2nd_pred)))\nprint(\"     3º Tentativa: {:.11f} {:.8f} {:.8f} {:.8f} {:.8f}\"\n        .format(mean_absolute_error(y_3rd_test, y_3rd_pred),\n                mean_squared_error(y_3rd_test,y_3rd_pred),\n                mean_squared_error(y_3rd_test,y_3rd_pred,squared=False),\n                median_absolute_error(y_3rd_test,y_3rd_pred),\n                r2_score(y_3rd_test,y_3rd_pred)))\n```\n\n::: {.callout-warning}\no valor negativo de R2 indica que o modelo é pior que um modelo constante\n\n:::\n\n#### Gravar o modelo\n\n```{python}\n#| eval: false\n\nimport pickle\n\n# escolher o nome do ficheiro\nfilename = \"data\\linearRegression_SK.pickle\"\n\n# gravar o modelo\npickle.dump(lr3, open(filename, \"wb\"))\n```\n\n```{python}\n#| eval: false\n\n# fazer load do modelo\nloaded_model = pickle.load(open(filename, \"rb\"))\n```\n\n```{python}\n#| eval: false\n\n# ve rificar que conseguimos carregar o modelo gravado\nloaded_model.coef_\n\n```\n\n### Classificação\n\n**QUESTÃO**:  \nCom este conjunto de dados que inclui o número de médicos e enfermeiros em várias especialidades será que consigo estimar se essa unidade em particular tem serviço de cirurgia? \n\nColuna **t_cirurgia** criada a partir da coluna **C21071** que deve ser retirada depois de criada a etiqueta.\n\n#### Classification Trees \n\n```{python}\nimport numpy as np\nimport pandas as pd\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n```\n\n```{python}\ndf_hosp = df_hosp.reset_index() # \n\ndf = df_hosp.drop(columns='C21071', axis=1)  # drop da coluna C21071\n\ndf['t_cirurgia'] = df['t_cirurgia'].fillna(0) # preencher os missing values com 0\n\ndf['t_cirurgia'] = df['t_cirurgia'].replace({'yes': 1, 'no': 0}) # substituir os valores yes e no por 1 e 0\n\ndf.head()\n```\n\n```{python}\ndf1 = df.dropna() # drop das linhas com valores missing\n\ndf1.describe()\n```\n\n```{python}\n# conta o nº de diferentes valores na coluna\ndf1['t_cirurgia'].nunique()\n\nprint()\n# verifica se há nulos no dataframe\ndf1.isnull().any()\n\n```\n\n##### Information Gain - Entropia\n\n```{python}\n# função para calcular a entropia\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef compute_impurity(feature, impurity_criterion): # função para calcular a impureza de uma feature\n    \"\"\"\n    This function calculates impurity of a feature.\n    Supported impurity criteria: 'entropy', 'gini'\n    input: feature (this needs to be a Pandas series)\n    output: feature impurity\n    \"\"\"\n    probs = feature.value_counts(normalize=True)\n    \n    if impurity_criterion == 'entropy':\n        impurity = -1 * np.sum(np.log2(probs) * probs)\n    elif impurity_criterion == 'gini':\n        impurity = 1 - np.sum(np.square(probs))\n    else:\n        raise ValueError('Unknown impurity criterion')\n        \n    return(round(impurity, 3))\n\n\n# Exemplos\nprint('impurity using entropy:', compute_impurity(df1['t_cirurgia'], 'entropy'))\nprint('impurity using gini index:', compute_impurity(df1['t_cirurgia'], 'gini'))\n```\n\n```{python}\nfor level in df1['NUTS2'].unique(): # loop sobre os níveis da feature NUTS2\n    print('level name:', level)\n    df_feature_level = df1[df1['NUTS2']== level]\n    print('corresponding data partition:')\n    print(df_feature_level.head(5))\n    print('partition target feature impurity:', compute_impurity(df_feature_level['t_cirurgia'], 'entropy'))\n    print('partition weight:', str(len(df_feature_level)) + '/' + str(len(df1)))\n    print('====================')\n```\n\n```{python}\n# função para calcular o information gain\ndef comp_feature_information_gain(df, target, descriptive_feature, split_criterion): \n    \"\"\"\n    This function calculates information gain for splitting on \n    a particular descriptive feature for a given dataset\n    and a given impurity criteria.\n    Supported split criterion: 'entropy', 'gini'\n    \"\"\"\n    \n    print('target feature:', target)\n    print('descriptive_feature:', descriptive_feature)\n    print('split criterion:', split_criterion)\n            \n    target_entropy = compute_impurity(df[target], split_criterion)\n    print('the target entropy is', target_entropy)\n\n    # we define two lists below:\n    # entropy_list to store the entropy of each partition\n    # weight_list to store the relative number of observations in each partition\n    entropy_list = list()\n    weight_list = list()\n    \n    # loop over each level of the descriptive feature\n    # to partition the dataset with respect to that level\n    # and compute the entropy and the weight of the level's partition\n    for level in df[descriptive_feature].unique():\n        df_feature_level = df[df[descriptive_feature] == level]\n        entropy_level = compute_impurity(df_feature_level[target], split_criterion)\n        entropy_list.append(round(entropy_level, 3))\n        weight_level = len(df_feature_level) / len(df)\n        print('the level is {} and the weight is {}'.format(level, weight_level))\n        weight_list.append(round(weight_level, 3))\n\n    print('impurity of partitions:', entropy_list)\n    print('weights of partitions:', weight_list)\n\n    feature_remaining_impurity = np.sum(np.array(entropy_list) * np.array(weight_list))\n    print('remaining impurity:', feature_remaining_impurity)\n    \n    information_gain = target_entropy - feature_remaining_impurity\n    print('information gain:', information_gain)\n    \n    print('====================')\n\n    return(information_gain)\n```\n\nVamos ver que a variável do Número de Pessoal ao Serviço - Total **C10001** é bastante promissora para faser a divisão do espaço, e a variável do Número de Médicos - Total **C20001** é bastante melhor do que o Número de Médicos - Especialistas - Anatomia Patológica - Total **C21011**\n\n```{python}\n#| eval: false\n\nsplit_criterion = 'entropy' # escolher o critério de divisão\nfor feature in df1.drop(columns=['t_cirurgia','ANO','NORDEM']).columns:\n    feature_info_gain = comp_feature_information_gain(df1, 't_cirurgia', feature, split_criterion) # calcular o information gain\n```\n\n##### trees \n\n```{python}\n# define a variável target e os predictors\nX = df1.drop(columns=['t_cirurgia']) # dataframe sem a coluna target\ny = df1['t_cirurgia'].values # target em formato de array\n```\n\n```{python}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nclf = DecisionTreeClassifier(max_depth=3) # define a profundidade da árvore\n\nmodel = clf.fit(X,y) # treina o modelo\n```\n\n```{python}\n#from sklearn import tree\n\ntext_representation = tree.export_text(clf)\nprint(text_representation) # visualização do modelo\n\n```\n\n```{python}\n#| eval: false\n\n# gravar o log da Árvore\nwith open(\"data\\decision_tree.log\", \"w\") as fout: # \n    fout.write(text_representation)\n```\n\n```{python}\nimport matplotlib.pyplot as plt\n\n# class_names = True em vez da lista  faz print y(0) e y(1)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(clf, class_names= [\"No\",\"Yes\"], filled=True)\n\n```\n\n```{python}\n#| eval: false\n# gravar a imagem da tree\n\nfig.savefig(\"images/decision_tree.png\")\n```\n\n\n#### Classification com Scikit\n\n```{python}\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\nfrom sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder # For scaling variables\n\nfrom sklearn.model_selection import train_test_split,cross_val_score # For splitting data\nfrom sklearn.tree import DecisionTreeClassifier # For Decision Tree\n\n# For model evaluation\nfrom sklearn.metrics import accuracy_score,confusion_matrix \nfrom sklearn.metrics import roc_curve,roc_auc_score \n```\n\n```{python}\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, \n                                                    test_size = 0.2 ,\n                                                    random_state = 1984)\nprint(X_train.shape)\nprint(X_test.shape)\n```\n\n```{python}\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.preprocessing import StandardScaler #for scaling variables\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\n\n#Define a pipeline\npipeline = Pipeline([\n('scaler', StandardScaler()),\n('selector', VarianceThreshold()),\n('TREE', DecisionTreeClassifier())])\n```\n\n##### usando a Pipeline\n\n```{python}\nfrom sklearn import set_config # para configurar a visualização da pipeline\n\nset_config(display=\"diagram\")\npipeline\n```\n\n```{python}\n# execução da pipeline com parameteros de omissão\npipeline.fit(X_train, y_train)\nprint('Training set score: ' + str(pipeline.score(X_train,y_train)))\nprint('Test set score: ' + str(pipeline.score(X_test,y_test)))\n\nprint()\ny_pred = pipeline.predict(X_test)\ny_pred\n\nprint()\ny_proba = pipeline.predict_proba(X_test)\ny_proba\n```\n\n\n```{python}\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n```\n\n```{python}\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\nprint('Precision: %.3f' % precision_score(y_test, y_pred))\nprint('Recall: %.3f' % recall_score(y_test, y_pred))\nprint('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n```\n\n![](images/conf_matrix.png)\n\n```{python}\nfrom sklearn.metrics import roc_curve, balanced_accuracy_score\n# antes era plot_roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,\n                                       pipeline.predict_proba(X_test)[:,1],)\n\nRocCurveDisplay.from_estimator(pipeline, X_test, y_test)\n\nplt.show()\n```\n\n##### sem Pipeline\n\n```{python}\nclf2 = DecisionTreeClassifier(max_depth=2) \n\nmodel2 = clf2.fit(X_train, y_train)\n\ny_class = model2.predict(X_test)\ny_class\n\nprint()\ny_class_proba = model2.predict_proba(X_test)\ny_class_proba\n\nprint()\nmodel.predict_proba(X_test)[:, 1]\n\ntree.plot_tree(clf2, class_names= [\"No\",\"Yes\"], filled=True)\n```\n\npodemos mudar o threshold  \n```{python}\nthreshold = 0.7\ny_pred = (model2.predict_proba(X_test)[:, 1] > threshold)\nconfusion_matrix(y_test, y_pred)\n\nprint()\ncm_thr70 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr70,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n```\n\n```{python}\nfrom sklearn.metrics import roc_curve,RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,model2.predict_proba(X_test)[:,1],\n                                 drop_intermediate=False) # temos de fazer o unpack dos 3 resultados\n\nRocCurveDisplay.from_estimator(model,X_test,y_test)\n\nplt.show()\n```\n\n```{python}\nprint(fpr)\nprint(tpr)\nprint(thresholds)\n```\n\n##### gravar o modelo  \n```{python}\n#| eval: false\n\n# escolher o nome do ficheiro\nfilename = \"data\\Tree.pickle\"\n\n# gravar o modelo\npickle.dump(model2, open(filename, \"wb\"))\n```\n\n\n\n<br>\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["epub.css"],"output-file":"500-mod5.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["references.bib"],"theme":{"light":"flatly","dark":"solar"}},"extensions":{"book":{"multiFile":true}}},"epub":{"identifier":{"display-name":"ePub","target-format":"epub","base-format":"epub"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"epub","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":false,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"default-image-extension":"png","html-math-method":"mathml","to":"epub","toc":true,"css":["epub.css"],"output-file":"500-mod5.epub"},"language":{"toc-title-document":"Índice","toc-title-website":"Nesta página","related-formats-title":"Outros formatos","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Fonte","section-title-abstract":"Resumo","section-title-appendices":"Apêndices","section-title-footnotes":"Notas de rodapé","section-title-references":"Referências","section-title-reuse":"Reuso","section-title-copyright":"Direito autoral","section-title-citation":"Citação","appendix-attribution-cite-as":"Por favor, cite este trabalho como:","appendix-attribution-bibtex":"BibTeX","title-block-author-single":"Autor","title-block-author-plural":"Autores","title-block-affiliation-single":"Afiliação","title-block-affiliation-plural":"Afiliações","title-block-published":"Data de Publicação","title-block-modified":"Data de Modificação","callout-tip-title":"Dica","callout-note-title":"Nota","callout-warning-title":"Aviso","callout-important-title":"Importante","callout-caution-title":"Cuidado","code-summary":"Código","code-tools-menu-caption":"Código","code-tools-show-all-code":"Mostrar o código","code-tools-hide-all-code":"Esconder o código","code-tools-view-source":"Ver o código fonte","code-tools-source-code":"Código fonte","code-line":"Linha","code-lines":"Linhas","copy-button-tooltip":"Copiar para a área de transferência","copy-button-tooltip-success":"Copiada","repo-action-links-edit":"Editar essa página","repo-action-links-source":"Ver o código fonte","repo-action-links-issue":"Criar uma issue","back-to-top":"De volta ao topo","search-no-results-text":"Nenhum resultado","search-matching-documents-text":"documentos correspondentes","search-copy-link-title":"Copiar link para a busca","search-hide-matches-text":"Esconder correspondências adicionais","search-more-match-text":"mais correspondência neste documento","search-more-matches-text":"mais correspondências neste documento","search-clear-button-title":"Limpar","search-detached-cancel-button-title":"Cancelar","search-submit-button-title":"Enviar","search-label":"Procurar","toggle-section":"Alternar seção","toggle-sidebar":"Alternar barra lateral","toggle-dark-mode":"Alternar modo escuro","toggle-reader-mode":"Alternar modo de leitor","toggle-navigation":"Alternar de navegação","crossref-fig-title":"Figura","crossref-tbl-title":"Tabela","crossref-lst-title":"Listagem","crossref-thm-title":"Teorema","crossref-lem-title":"Lema","crossref-cor-title":"Corolário","crossref-prp-title":"Proposição","crossref-cnj-title":"Conjetura","crossref-def-title":"Definição","crossref-exm-title":"Exemplo","crossref-exr-title":"Exercício","crossref-ch-prefix":"Capítulo","crossref-apx-prefix":"Apêndice","crossref-sec-prefix":"Seção","crossref-eq-prefix":"Equação","crossref-lof-title":"Lista de Figuras","crossref-lot-title":"Lista de Tabelas","crossref-lol-title":"Lista de Listagens","environment-proof-title":"Comprovação","environment-remark-title":"Comentário","environment-solution-title":"Solução","listing-page-order-by":"Ordenar por","listing-page-order-by-default":"Pré-selecionado","listing-page-order-by-date-asc":"Mais velho","listing-page-order-by-date-desc":"O mais novo","listing-page-order-by-number-desc":"Decrescente","listing-page-order-by-number-asc":"Crescente","listing-page-field-date":"Data","listing-page-field-title":"Título","listing-page-field-description":"Descrição","listing-page-field-author":"Autor","listing-page-field-filename":"Nome do arquivo","listing-page-field-filemodified":"Arquivo modificado","listing-page-field-subtitle":"Subtítulo","listing-page-field-readingtime":"Tempo de leitura","listing-page-field-categories":"Categorias","listing-page-minutes-compact":"{0} minutos","listing-page-category-all":"Tudo","listing-page-no-matches":"Nenhum item correspondente"},"metadata":{"bibliography":["references.bib"],"lang":"pt","date":"2024-03-31"},"extensions":{"book":{"selfContainedOutput":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":true,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":"H","fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":{"text":"\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\areaset[0.50in]{4.5in}{8in}\n"},"include-before-body":{"text":"\\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n   showspaces = false,\n   showtabs = false,\n   breaksymbolleft={},\n   breaklines\n   % Note: setting commandchars=\\\\\\{\\} here will cause an error \n}  \n"},"output-file":"500-mod5.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrbook","classoption":["paper=6in:9in","pagesize=pdftex","headinclude=on","footinclude=on","12pt"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","epub","pdf"]}