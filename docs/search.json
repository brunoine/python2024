[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python 2024",
    "section": "",
    "text": "Programação em Python"
  },
  {
    "objectID": "index.html#formação-em-data-science",
    "href": "index.html#formação-em-data-science",
    "title": "Python 2024",
    "section": "Formação em Data Science",
    "text": "Formação em Data Science\n\n\n\nFormadores: Bartholomeus Schoenmakers, Luis Ferreira, Sónia Quaresma\nBasic de 2024-04-08 a 202404-12\nIntermediate de 2024-04-15 a 2024-04-22\nAdvanced de 2024-05-06 a 2024-05-14\nFormação presencial realizada nas instalações do INE - Porto.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nOs materiais de formação a utilizados estão disponiveis na cloud do INE:\nhttps://cloud2.ine.pt/index.php/s/qbLinNiMA7zpY9W\npassword: 2024.IntPorto\n\n\n\nUsing Python with the RStudio IDE\n\n\n\n\n\n\nshortcut\n\n\n\n\n\nPara inserir uma nova chunk de python, no RStudio:\nTools &gt; Modify Keyboard Shortcuts &gt; Insert Chunk Python &gt;\nCtrl+Alt+P"
  },
  {
    "objectID": "100-mod1.html#o-que-é-o-python",
    "href": "100-mod1.html#o-que-é-o-python",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.1 o que é o Python?",
    "text": "1.1 o que é o Python?\nlinguagem de programação:\n\ninterpretada e orientada a objectos (mas permite funções)\naprendizagem rápida e simples\ngratuita\n\nmodo interativo vs modo script\nIniciar -&gt; Anaconda prompt (se o Anaconda estiver configurado).\nmodo interativo vs modo script\ncriado um ficheiro.py, pode ser executado:\npython -u \"c:\\Users\\documents\\ficheiro.py\""
  },
  {
    "objectID": "100-mod1.html#funções-básicas",
    "href": "100-mod1.html#funções-básicas",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.2 Funções básicas",
    "text": "1.2 Funções básicas\n\n# imprimir na consola\nprint('olá mundo!')\n\nolá mundo!\n\n\n\n# docstring, funciona como comentário mas não é recomendado\n\n\"\"\"\npermite escrever texto \nem multiplas\nlinhas\n\"\"\"\n\n'\\npermite escrever texto \\nem multiplas\\nlinhas\\n'\n\n\nimprimir números e /ou cálculos:\n\nprint(3+6)\n\n9\n\n\npara concatenar texto formatado\n\nnumero = 4\nprint(f\"O número é: {numero*2}\")\n\nO número é: 8\n\n\nimprimir múltiplas linas\n\nprint(\"\"\"\nlinha1\nlinha2\n\"\"\")\n\n\nlinha1\nlinha2\n\n\n\nraw strings\n\n# forçar a imprimir exactamente o que está entre '' \nprint(r'tudo\\namora')\n\n# ou então podemos imprimir com a qubra de linha\nprint('tudo\\namora')\n\ntudo\\namora\ntudo\namora\n\n\nold string formating%\n\n# se x for string\nx = '15'\n\nprint(\"x como string = %s\" %(x))\n\nx como string = 15\n\n\n\ny = int(x)\nprint(\"x como integer = %d\" %(y))\n\nprint(\"x como float = %09.4f\" %(y))\n\nx como integer = 15\nx como float = 0015.0000\n\n\n\ninput(\"Escreve um input: \")\n\nfunção que avalia o seu argumento:\n\nnumero = eval('2')\n\n# em alternativa podemos especificar o tipo que pretendemos\nnumero = int('2')\n\nprint(f\"o dobro do numero é: {numero *2}\")\n\no dobro do numero é: 4\n\n\nem alternativa podemos especificar o tipo que pretendemos\n\ninteiro = int('2')\n\nprint(f\"o dobro do numero é: {inteiro *2}\")\n\no dobro do numero é: 4\n\n\nNo modo script podemos criar um ficheiro com o código que queremos executar, por exemplo o ficheiro mod1.py e excecutar na linha de comandos (Iniciar -&gt; Anaconda prompt):\nC:\\Users\\bruno.lima\\Documents\\Python\\python2024\\exercicios&gt;python mod1.py"
  },
  {
    "objectID": "100-mod1.html#aritmética",
    "href": "100-mod1.html#aritmética",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.3 Aritmética",
    "text": "1.3 Aritmética\n\n# quatro operações básicas\n\nprint(2+2, 5-2, 4*2, 10/5)\n\n# potencias\n4**3\n\n# calcula horas\nminutos = 70\nprint(f\"são {minutos//60} horas e {minutos-60} minutos\")\n\n4\n\n\n 3 8 2.0\nsão 1 horas e 10 minutos"
  },
  {
    "objectID": "100-mod1.html#definir-e-alterar-variáveis",
    "href": "100-mod1.html#definir-e-alterar-variáveis",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.4 Definir e alterar variáveis",
    "text": "1.4 Definir e alterar variáveis\n\n# por convenção devemos usar nome do formato case_snake\ncores_do_arco_iris = ['red','green','yellow','blue','orange','indigo', 'violet']\n\nnumero_de_tons = len(cores_do_arco_iris)\n\nnumero_de_tons\n\n7\n\n\ncasting\n\nx = int(3)\ny = str(3)\nz = float(3)"
  },
  {
    "objectID": "100-mod1.html#exercicios",
    "href": "100-mod1.html#exercicios",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.5 Exercicios",
    "text": "1.5 Exercicios\n\n# calcular o vlume de um cilindro dado o raio da base e a altura\n\nraio_base = 2\naltura = 10\n\narea_base = 3.14*raio_base**2\n\nvolume = area_base*float(altura)\n\nprint(f\"o volume é: {volume} m3\")\n\no volume é: 125.60000000000001 m3\n\n\nobtem os primeiros 4 divisores de um número N, separados por ‘++++’\n\nN = 12\nlista = []\n\nfor i in range(1, N+1):\n  if N%i == 0:\n    lista.append(i)\n    \nprint(*lista[:4], sep = '+++')\n\n1+++2+++3+++4\n\n\nobtem os primeiros 4 múltiplos de um número, separados por ‘++++’\n\nnumero=3\n\nprint(f'os múltiplos de {numero} são: {numero *1}', numero *2, numero *3, numero *4, sep = ' ++++ ', end = \" ^^\")\n\nos múltiplos de 3 são: 3 ++++ 6 ++++ 9 ++++ 12 ^^"
  },
  {
    "objectID": "100-mod1.html#tipos-de-dados",
    "href": "100-mod1.html#tipos-de-dados",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.6 Tipos de dados",
    "text": "1.6 Tipos de dados\n\nstr\nnumérico:\n\nint\nfloat\ncomplex\n\nbool\nsequencia:\n-list -tuple -range\ndict\nset / frozenset\nbinario:\n\nbytes\nbytearray\nmemoryview\n\n\n\n1.6.1 string\n\na = \"um conjunto de letras\"\n\ntipo=type(a)\nlen(a)\n\nprint(f\"o tipo da variável 'a' é {tipo}\")\n\nprint(f\"a primeira letra é %s e a ultima letra é %s\" % (a[0], a[-1]))\n\nprint(a[:2]) # todas as letras até à segunda posição\n\no tipo da variável 'a' é &lt;class 'str'&gt;\n\n\n\na primeira letra é u e a ultima letra é s\num\n\n\n\n\n1.6.2 int\n\ni1 = 12\ni2 = -1\n\nprint(type(i1), type(i2))\n\n&lt;class 'int'&gt; &lt;class 'int'&gt;\n\n\n\n\n1.6.3 float\n\nf2=-7.7e100\nf3=2E2\n\nprint(f2, f3)\n\n-7.7e+100 200.0\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA instalação dos packages deve ser feita através do terminal Bash com o comando: $ pip install pandas\n\n\n\nimport math\n\nprint(repr(math.pi))\nprint(format(math.pi,'.12g'))\nprint(format(math.pi,'.2f'))\n\n3.141592653589793\n3.14159265359\n3.14\n\n\narredondamento implicito\n\nprint(.1+.1+.1 == .3)\n\nprint(round(.1+.1+.1, 10) == round(.3, 10))\n\nFalse\nTrue\n\n\n\n\n1.6.4 complex(j=parte imaginária)\n\nc1 = 1j\nc2 = 3+5j\n\nprint(type(c1), type(c2))\n\n&lt;class 'complex'&gt; &lt;class 'complex'&gt;\n\n\n\n\n1.6.5 bool\n\nprint(type(True))\n\nprint(f\"o número zero é: {bool(0)}\")\nprint(f\"o número 45 é: {bool(45)}\")\nprint(f\"o nome 'INE' é: {bool('INE')}\")\nprint(f\"o vazio é: {bool('')}\")\n\n&lt;class 'bool'&gt;\no número zero é: False\no número 45 é: True\no nome 'INE' é: True\no vazio é: False\n\n\n\n\n1.6.6 sem tipo\nNoneType\n\n\n1.6.7 conversão entre tipos\n\n# conversão implicita\nx = 10\ny = 5\nz = x+y\n\nprint(type(z))\n\n&lt;class 'int'&gt;\n\n\n\n#conversão para inteiro\nprint(int(1))\nprint(int(2.8))\nprint(int(3))\n\n1\n2\n3\n\n\n\n# conversão para float\nprint(float('3.1'))\n\n3.1\n\n\n\n# conversão para bool\nprint(bool(1))\n\nTrue\n\n\n\n\n\n\n# conversão para string\na = str(2)\nb = str(3.0)\n\nprint(a,b)\n\n2 3.0\n\n\n\n#conversão com eval\na = eval('8**2')\nprint(a)\n\n64"
  },
  {
    "objectID": "100-mod1.html#organização-do-código",
    "href": "100-mod1.html#organização-do-código",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.7 Organização do código",
    "text": "1.7 Organização do código\n\n1.7.1 Programação modular\n\nsimplicidade\nfacilidade de manutenção\nreutilização\nâmbito\n\n\n\n1.7.2 Packages\npackages\n\nsubpackages\n\nmodule: funções, classes, variáveis, código, …\n\n\n\n\n\n1.7.3 Funções\n\nBuilt-in (funções standard do Python)\n\n\n\nUDF (funções definidas pelo utilizador)\nlambda (funções anónimas)\n\n\n\n1.7.4 UDF\ndef nome_da_funcao(parametros):\n  \"\"\" comentário com propósito da função\"\"\"\n  \n  # corpo da função (instruções e lógica)\n  \n  return informacao_a_retornar\nexemplos:\n\ndef soma(x,y):\n  \"\"\" AVISO \"\"\"\n  return(x+y)\n\nsoma(1,2)\n\n3\n\n\n\n\n1.7.5 exercícios\n\n# criar uma função que devolve um valor elevado a uma potencia\n\ndef pot(base, expo):\n  return(base**expo)\n\nprint(pot(5,2))\n\n25"
  },
  {
    "objectID": "100-mod1.html#controlo-da-execução",
    "href": "100-mod1.html#controlo-da-execução",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.8 Controlo da execução",
    "text": "1.8 Controlo da execução\n\n1.8.1 Controlo condicional\n\n\nchove = True\nif (chove):\n  print(\"chove mesmo!\")\nelse:\n  print(\"Faz sol!\")\n\nchove mesmo!\n\n\noperadores de comparação\n'&gt;', '&lt;', '==', '&gt;=', '&lt;=', '!=', 'is' ['not'], ['not'] 'in' \n\na = 1\nb = 2\nc = 3\n\nprint(f\"1-{a} &lt;{b} &lt;{c}\")\n\n1-1 &lt;2 &lt;3\n\n\n\n\n1.8.2 operador trenário\n\nidade = int('20')\ndecisao = 'já pode votar' if idade &gt;=18 else 'ainda não pode votar'\n\nprint(f\"com {idade} anos,  a decisão é: {decisao}\")\n\ncom 20 anos,  a decisão é: já pode votar\n\n\n\n# exemplo de menor legibilidade\nnr_mes = 3\n\nnome_mes = 'janeiro' if nr_mes == 1 else \\\n'fevereiro' if nr_mes == 2 else \\\n'março' if nr_mes == 3 else \\\n'outro'\n\nprint(nome_mes)\n\nmarço\n\n\n\n\n1.8.3 ciclos\n\n\n# definir função oráculo usado mais em baixo\ndef oraculo_mistico(pergunta):\n  ''' retorna uma resposta à pergunta feita\n  ao estilo do jogo \"Bola 8 mágica\"\n  '''\n  import random\n\n  respostas = [\n    \"Sim\", \"Não\", \"Claro\", \"Com certeza\", \"Arrisque\", \"Não conte com isso\", \"Provavelmente\",\n    \"É duvidoso\", \"Talvez\", \"Não tenho certeza\", \"Sem dúvida\", \"Absolutamente\",\n    \"É melhor não dizer agora\", \"Concentre-se e pergunte novamente\", \"Minhas fontes dizem não\",\n    \"As perspectivas não são boas\", \"Não é possível prever agora\", \"Reformule sua pergunta\",\n    \"Não posso responder a isso\", \"Pergunte novamente mais tarde\", \"Probabilidade zero\",\n    \"Está nas estrelas\", \"Muito provável\", \"Os astros são favoráveis\"\n  ]\n\n  return random.choice(respostas)\n\n\ncontinuar_jogo = True\n\nwhile(continuar_jogo):\n  pergunta = input(\"faz pergunta: \")\n  if(pergunta == 'sair'):\n    continuar_jogo = False\n  else:\n    reposta = oraculo_mistico(pergunta) # oraculo_mistico() é uma função ad hoc\n    print(f\"A resposta é: {resposta}\")\n\n\n\n1.8.4 for loop\n\nfor i in range(10):\n  print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n1.8.5 exercícios\nCom um for loop escreve os primeiros 10 multiplos de 7\n\nfor i in range(10):\n  print(i * 7)\n\n0\n7\n14\n21\n28\n35\n42\n49\n56\n63\n\n\nCom recurso a loops,repita uma palavra definida pelo utilizador o número de vezes que ele pretenda, removendo a primeira letra da palavra introduzida\n\npalavra='teste'\nn=5\n\nprint(palavra[1:]*n, sep=' + ')\n\nfor i in range(n):\n  print(palavra[1:])\n\nesteesteesteesteeste\neste\neste\neste\neste\neste\n\n\nSimule o jogo de pedra, papel e tesoura com o computador (papel ganha pedra que ganha tesoura que ganha papel):\n\nfrom random import sample\n\nlista = ['pedra','papel','tesoura']\n\njogador1 = sample(lista,1)[0]\njogador2 = sample(lista,1)[0]\n\nprint(jogador1, jogador2)\n\nganhador = 'empate' if jogador1 == jogador2 else \\\n'jogador1' if jogador1 == 'papel' and jogador2 == 'pedra' else \\\n'jogador1' if jogador1 == 'tesoura' and jogador2 == 'papel' else \\\n'jogador1' if jogador1 == 'pedra' and jogador2 == 'tesoura' else \\\n'jogador2'\n\nprint(f\"o vencedor é o {ganhador}\")\n\npedra pedra\no vencedor é o empate\n\n\nsolução alternativa:\n\nimport random\nfim_do_jogo = False\n\nwhile (not(fim_do_jogo)):\n  jogada_humana = int(input(\"escolha:\\n1 - pedra\\n2 - papel\\n3 - tesoura\\n0 - terminar o jogo\\n\"))\n  if (jogada_humana == 0):\n    print(\"fim.\\n\")\n    fim_do_jogo = True\n  else:\n    jogada_cpu = random.choice([1, 2, 3]) # porque ainda não conhecemos o randint()\n\n    jogada_cpu_texto = \"pedra\"\n    if (jogada_cpu == 2):\n      jogada_cpu_texto = \"papel\"\n    elif (jogada_cpu == 3):\n      jogada_cpu_texto = \"tesoura\"\n\n    if (jogada_cpu == jogada_humana):\n      print(\"empate\\n\")\n    else:\n      print(f\"o computador jogou '{jogada_cpu_texto}'.\")\n      if ((jogada_cpu == 1 and jogada_humana == 2) or (jogada_cpu == 2 and jogada_humana == 3) or (jogada_cpu == 3 and jogada_humana == 1)):\n        print(\"vitória humana!\\n\\n\")\n      else:\n        print(\"o computador venceu\\n\\n\")"
  },
  {
    "objectID": "100-mod1.html#funções-mais-usadas",
    "href": "100-mod1.html#funções-mais-usadas",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.9 Funções mais usadas",
    "text": "1.9 Funções mais usadas\n\n1.9.1 funções built-in\n\nprint(\"a lista de nomes que podemos usar:\")\ndir()\nlen(dir())\n\na lista de nomes que podemos usar:\n\n\n103\n\n\n\nimport random\nimport math\n\nlen(dir())\n\n106\n\n\nrange()\n\nfor i in range(1, 5, 2):\n  print(i)\n\n1\n3\n\n\nord()\n\nord('a')\n\n97\n\n\npow()\n\nprint(pow(4,2,5) == 4**2 %5)\n\nTrue\n\n\nsum()\nlen()\nmax()\n\nvar1 = 'teste'\nvar2 = 'palavra'\nvar3 = 'coisa'\nmax_val = max(var1, var2, var3, key = len)\n\nprint(max_val)\n\npalavra\n\n\n\n\n1.9.2 ler a partir de ficheiros\nficheiro = open(r\"c:\\path\\ficheiros.txt\", modo_de_acesso)\ntentativa de uniformizar:\n\nimport os\n\npath = os.path.join(os.sep, rooth_path + os.sep = 'directoria')\n\nler linha a linha:\n\nlinhas = [line.strip().split('',1)] for line in open(nome_do_ficheiro)\n\nopen(nome_do_ficheiro).read()\n\nopen(nome_do_ficheiro).readlines()\n\nescrever ficheiros\n\n# @title\nficheiro_escrita = g_path + 'texto_escrito.txt'\n\nwith open(ficheiro_escrita, 'w') as f:\n  f.write('Escrita a funcionar!')\n\nf.close()\n\n\n# @title\nficheiro_escrita = g_path + 'texto_escrito2.txt'\n\nlinhas = [\n    \"aaa\",\n    \"bbb\",\n    \"ccc\"\n]\n\nficheiro = open(ficheiro_escrita, 'w')\nficheiro.writelines(linhas)\nficheiro.close() # to change file access modes\n\n\n# @title\nficheiro_append = g_path + 'texto_append.txt'\n\nlinhas = [\n    \"uma linha\\n\",\n    \"duas linhas\\n\",\n    \"tantas linhas\\n\"\n]\n\nficheiro = open(ficheiro_append, 'a')\nficheiro.writelines(linhas)\nficheiro.close() # to change file access modes\n\n\n\n1.9.3 modulo maths\n\nimport math\n\ndir(math)\n\n['__doc__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'acos',\n 'acosh',\n 'asin',\n 'asinh',\n 'atan',\n 'atan2',\n 'atanh',\n 'cbrt',\n 'ceil',\n 'comb',\n 'copysign',\n 'cos',\n 'cosh',\n 'degrees',\n 'dist',\n 'e',\n 'erf',\n 'erfc',\n 'exp',\n 'exp2',\n 'expm1',\n 'fabs',\n 'factorial',\n 'floor',\n 'fmod',\n 'frexp',\n 'fsum',\n 'gamma',\n 'gcd',\n 'hypot',\n 'inf',\n 'isclose',\n 'isfinite',\n 'isinf',\n 'isnan',\n 'isqrt',\n 'lcm',\n 'ldexp',\n 'lgamma',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'modf',\n 'nan',\n 'nextafter',\n 'perm',\n 'pi',\n 'pow',\n 'prod',\n 'radians',\n 'remainder',\n 'sin',\n 'sinh',\n 'sqrt',\n 'sumprod',\n 'tan',\n 'tanh',\n 'tau',\n 'trunc',\n 'ulp']\n\n\n\n\n1.9.4 modulo statistics\n\nimport statistics\n\ndir(statistics)\n\nsample = [10,203,54,69,221,57,84,29,46,77]\n\n# o valor NaN (Not a Number) afecta o comportamento de muitas destas funções,\n# ou seja, convém remover os NaN das listas antes de invocar estas funções\nres = statistics.mean(sample)\nprint(\"Média: \", res)\n# fmean -&gt; mais rápido, converte todos os valores para float\n\nres = statistics.median(sample)\nprint(\"Mediana: \", res)\n#res = statistics.median_low(data)\n#res = statistics.median_high(data)\n\nres = statistics.stdev(sample)\nprint(\"Devsio padrão: \", res)\n# pstdev - toda a população\n\nres = statistics.mode(sample)\nprint(\"Moda: \", res)\n\nres = statistics.multimode(sample)\nprint(\"Modas (por ordem de aparecimento na lista): \", res)\n\nres = statistics.variance(sample)\nprint(\"Variância da amostra:\", res)\n# pvariance - toda a população\n\nMédia:  85\nMediana:  63.0\nDevsio padrão:  70.52816616233703\nModa:  10\nModas (por ordem de aparecimento na lista):  [10, 203, 54, 69, 221, 57, 84, 29, 46, 77]\nVariância da amostra: 4974.222222222223\n\n\n\n\n1.9.5 modulo random\n\nimport random\ndir(random)\n\nprint(f\"Um valor aleatório entre 0 e 1: {random.random()}\")\n\nUm valor aleatório entre 0 e 1: 0.8075252340096558"
  },
  {
    "objectID": "100-mod1.html#introdução-a-listas",
    "href": "100-mod1.html#introdução-a-listas",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.10 Introdução a listas",
    "text": "1.10 Introdução a listas\n\nlista = [5,7,9]\n\nsum(lista)\n#lista.sum()\n\n21\n\n\n\n# @title juntar listas\naves = [\"águia\", \"papagaio\", \"gaivota\"]\npeixes = [\"salmão\", \"tubarão\", \"carpa\"]\n\nanimais = aves + peixes\nprint(animais)\n\n['águia', 'papagaio', 'gaivota', 'salmão', 'tubarão', 'carpa']\n\n\n\nmamiferos = ['cao','gato','elefante']\n\nfor i in range(len(mamiferos)):\n  print(i, mamiferos[i])\n\n0 cao\n1 gato\n2 elefante\n\n\nacesso posicional\n\nfor i in reversed(lista):\n  print(i)\n\n9\n7\n5\n\n\nalterer listas append(), remove(), pop(),\n\nlista.append(11)\n\nlista.pop()\n\n11\n\n\n\ndel lista[2]\n\nprint(lista)\n\nprint(lista[1::2])\n\ninvertida = lista[::-1]\n\n[5, 7]\n[7]\n\n\nalterar listas\n\nlista = ['a','b',3,4]\n\nlista[2:] = 'r'\n\nprint(lista)\n\n['a', 'b', 'r']\n\n\nsem repetições - sample()\ncom repetições - choices()\nbaralhar - shuffle()\n\nprint(' '.join(['pequena', 'pausa']))\n\npequena pausa\n\n\n\n# @title remove o elemento na posição e devolve esse elemento\nlista = [2, 5, 3, 7]\nprint(\"lista:\",lista)\n\np = 1\nremovido = lista.pop(p) #\n\nprint(f'\\n{p+1}º elemento da lista: {removido}')\nprint(f'lista após remoção do {p+1}º elemento: {lista}')\n\nultimo = lista.pop()\nprint(f'lista após remoção do último elemento: {lista}')\n\nlista: [2, 5, 3, 7]\n\n2º elemento da lista: 5\nlista após remoção do 2º elemento: [2, 3, 7]\nlista após remoção do último elemento: [2, 3]\n\n\n\n# @title insere o elemento x na posição p da lista\nlista = [2, 3, 7]\nprint(\"lista:\",lista)\n\nx = 8\np = 2\nlista.insert(p, x)\n\nprint(f'\\napós inserção do {x} na {p+1}ª posição: {lista}')\n\nlista: [2, 3, 7]\n\napós inserção do 8 na 3ª posição: [2, 3, 8, 7]\n\n\nsegmentar listas (slicing) possiblidades de segmentação lista[ indice_pos_inicial : indice_pos_final : incremento_do_indice ]\n\n# @title\nlista = [1, 2, 3, 4, 5, 6]\n\npares = lista[1::2]\n\nprint(lista, pares, sep=\"\\n\")\n\n[1, 2, 3, 4, 5, 6]\n[2, 4, 6]\n\n\nslicing - alterando a lista\n\n# @title\nlista = [\"a\", \"b\", 3, 4]\n\nlista[2:] = [\"r\"]\n\nprint(lista)\n\n['a', 'b', 'r']\n\n\n\n1.10.1 exercicios\nver ficheiro mod1_ex_dia2_manha.py"
  },
  {
    "objectID": "100-mod1.html#strings-e-dicionários",
    "href": "100-mod1.html#strings-e-dicionários",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.11 Strings e dicionários",
    "text": "1.11 Strings e dicionários\nstring é uma sequencia de caracteres\nstr()\n\nstring = 'teste'\n\ndir(string)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\nunir - .join()\no dicionário é um array associativo: conjunto de chave / valor\ndict()\n\npaises_iso = {\n  'Portugal': 'PT',\n  'Espanha': 'ES',\n  'Franca': 'FR',\n  'Alemanha': 'DE',\n  'Brasil': 'BR',\n  'Argentina': 'AR'\n}\n\npaises_iso['Italia'] = 'IT'\n\nprint(paises_iso)\n\n{'Portugal': 'PT', 'Espanha': 'ES', 'Franca': 'FR', 'Alemanha': 'DE', 'Brasil': 'BR', 'Argentina': 'AR', 'Italia': 'IT'}\n\n\n\npaises_iso.get('Brasil')\n\n'BR'\n\n\n\n# for k, v in paises_iso:\n#   print(k,'{: }', sep = ': ')\n\nexercicio\n\nusers = {\n  '': 'pass1',\n  '': 'pass2',\n  '': 'pass3',\n  '': 'pass4'\n}"
  },
  {
    "objectID": "100-mod1.html#mais-código",
    "href": "100-mod1.html#mais-código",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.12 mais código",
    "text": "1.12 mais código\npodemos definir uma função com um input\n\ndef fib(n):\n  a, b = 0, 1\n  while a &lt; n:\n    print(a, end=' ')\n    a, b = b, a+b\n  print()\n  \nfib(1000)\n\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 \n\n\ne criar uma lista que queremos transformar e enumerar\n\n# python 3: list comprehensions\nfruits = ['Banana', 'Apple', 'Lime']\nloud_fruits = [fruit.upper() for fruit in fruits]\n\nprint(loud_fruits)\n\nlist(enumerate(fruits))\n\n['BANANA', 'APPLE', 'LIME']\n\n\n[(0, 'Banana'), (1, 'Apple'), (2, 'Lime')]\n\n\ne tentar um if else\n\nfechado = True\n\nif fechado:\n  print('porta fechada!')\nelse:\n  print('vamos lá!')\n\nporta fechada!\n\n\nOperadores lógicos\n\nTrue and False\n\nFalse\n\n\n\nTrue or False\n\nTrue\n\n\n\nnot True == False\n\nTrue"
  },
  {
    "objectID": "100-mod1.html#packages-1",
    "href": "100-mod1.html#packages-1",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.13 Packages",
    "text": "1.13 Packages\nFormas de importar\nimport &lt;modulo&gt;[, &lt;nome_modulo&gt;] from &lt;modulo&gt; import &lt;name(s)&gt; as &lt;alt_name&gt; import &lt;modulo&gt; as &lt;alt_name&gt;\npackages disponíveis:\n\n!pip freeze\n\nexemplo de importação de package\n\nimport humanize\n\nhumanize.i18n.activate(\"pt_PT\")\n\nprint(humanize.apnumber(4))\n\n\nimport random\n\nprint(random.random())\n\n0.1397826454606096"
  },
  {
    "objectID": "100-mod1.html#the-zen-of-python",
    "href": "100-mod1.html#the-zen-of-python",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.14 The Zen of Python",
    "text": "1.14 The Zen of Python\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\nLessons from the Zen of Python"
  },
  {
    "objectID": "200-mod2.html#estatística-descritiva-e-inferência",
    "href": "200-mod2.html#estatística-descritiva-e-inferência",
    "title": "2  Data Science (Basics)",
    "section": "2.1 Estatística Descritiva e Inferência",
    "text": "2.1 Estatística Descritiva e Inferência\n\\[D = \\{(\\vec{x}_1, y_1), ..., (\\vec{x}_n, y_n)\\} \\]\n\\((\\vec{x}, y) \\thicksim P\\)\ntipos de dados numericos: int, float\n\n# Creating integer variables\nx = 10\ny = -5\nz = 0\n\n\ntype(x)\n\nint\n\n\n\n# Creating float variables\na = 3.14\nb = -0.5\nc = 2.0\n\n# Using the variables\n(a + b) * c\n\n5.28\n\n\ntipo sequencia: list, tuple, range, numpy.array\n\n# Creating a list of integers\nmy_list = [1, 2, 3, 4, 5]\n\n# Creating a list of strings\nnames = ['Alice', 'Bob', 'Charlie']\n\n# Creating a mixed-type list\nmixed_list = [1, 'hello', 3.14, True]\n\n\n# Checking my_list\nmy_list\n\n[1, 2, 3, 4, 5]\n\n\nos elementos das listas são indexados\n\n# Accessing elements in the list my_list\nmy_list = [1, 2, 3, 4, 5]\n\n# Accessing the first element (index 0)\nfirst_element = my_list[0]\nprint(first_element)  \n\n# Accessing the third element (index 2)\nthird_element = my_list[2]\nprint(third_element)  \n\n# Accessing the last element\nlast_element = my_list[-1]\nprint(last_element)  \n\n# Accessing the second-to-last element\nsecond_to_last_element = my_list[-2]\nprint(second_to_last_element)  \n\n1\n3\n5\n4\n\n\nnum intervalo incluimos o primeiro index mas não o último\n\n# Slicing to get a subset of elements\nsubset = my_list[1:4] \nprint(subset)  \n\n[2, 3, 4]\n\n\no operador * permite repetir a lista\n\n# Using list repetition\nmy_list * 2\n\nmy_list + my_list\n\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\npara multiplicar cada elemento da lista:\n\n# Multiply every element of my_list by 2\nresult = [x * 2 for x in my_list]\nprint(result) \n\n[2, 4, 6, 8, 10]\n\n\nA função lambda em Python é uma função anónima:\n\nmy_list = [1, 2, 3, 4, 5]\nresult = list(map(lambda x: x * 2, my_list))\nprint(result)  \n\n[2, 4, 6, 8, 10]\n\n\npara alterar a lista original temos de fazer um ciclo\n\n# Update each element in the list by multiplying it by 2\nfor i in range(len(my_list)):\n    my_list[i] *= 2\n\nprint(my_list)  \n\n[2, 4, 6, 8, 10]\n\n\nordenação\n\n# Sort my_list in descending order\nmy_list.sort(reverse=True)\n\nprint(my_list) \n\n[10, 8, 6, 4, 2]\n\n\nconcatenar as listas usando o método extend\n\nmy_list = [1, 2, 3]\nanother_list = [4, 5, 6]\n\n# Concatenate another_list to my_list using the + operator\nmy_list += another_list\n\n# Concatenate another_list to my_list using the extend() method\nmy_list.extend(another_list)\n\nprint(my_list)  \n\n[1, 2, 3, 4, 5, 6, 4, 5, 6]\n\n\nou com append\n\n# Append each element from another_list to my_list using the append() method\nfor element in another_list:\n    my_list.append(element)\n\nprint(my_list)  \n\n[1, 2, 3, 4, 5, 6, 4, 5, 6, 4, 5, 6]\n\n\neliminar elementos da lista por index\n\nmy_list = [1, 2, 3, 4, 5]\n\n# Remove the element at index 2 (which is 3) from my_list\ndel my_list[2]\n\nprint(my_list)  \n\n[1, 2, 4, 5]\n\n\nOs tuplos são normalmente usados em vez de listas quando pretendemos que sejam imutáveis, por exemplo coordenadas, configurações, chaves de um dicionário…\n\n# Creating a tuple\nmy_tuple = (1, 2, 3, 4, 5)\nprint(my_tuple) \n\ncoordinates = {(0, 0): 'origin', (1, 1): 'diagonal'}\ncoordinates\n\n(1, 2, 3, 4, 5)\n\n\n\n\n\n{(0, 0): 'origin', (1, 1): 'diagonal'}\n\n\nAs funções podem retornar múltiplos valores na forma de tuplo, permitindo um código conciso e eficiente. A descompactação de tuplos (tuple unpacking) é frequentemente usada para extrair os valores.\n\n# Tuple unpacking\ndef get_coordinates():\n    return 10, 20\n\nx, y = get_coordinates()\nprint(\"x:\", x)\nprint(\"y:\", y)\n\nx: 10\ny: 20\n\n\nranges\n\n# Create a range of eggs\neggs = range(10)\neggs\n\nrange(0, 10)\n\n\n\n# print all the elements in the range\nfor egg in eggs:\n    print(egg, end=\" \")\nprint()\n\ntype(eggs)\n\nlen(eggs)\n\nsum(eggs)\n\n0\n\n\n 1 2 3 4 5 6 7 8 9 \n\n\n45\n\n\ntipo texto (strings): str\ntipo booleano: bool\n\ndef is_even(number):\n    \"\"\"\n    Check if the given number is even.\n    \"\"\"\n    return number % 2 == 0\n\n# Test the function\nprint(is_even(4))  \nprint(is_even(7))  \n\nnum = 18\nif is_even(num):\n    print(f\"{num} is even.\")\nelse:\n    print(f\"{num} is odd.\")\n\nTrue\nFalse\n18 is even.\n\n\ntipo categorico: pandas.Categorical\ndados temporais: datetime, panda.Series, pandas.DataFrame\ndistribuições estatisticas: scipy.stats\nArrays: numpy.array\n\n2.1.1 Dados quantitativos\n\nalturas = [1.65,1.73,1.78,1.67,1.82,1.76,1.75,1.74,1.75,1.67,1.67,1.69]\nlen(alturas)\ntype(alturas)\nalturas.sort(reverse=True)\nmedia_alturas = sum(alturas)/len(alturas)\nmedia_alturas\n\n1.7233333333333334\n\n\nrepresentação gráfica com um scatterplot\n\npesos = [92, 87, 102, 78, 87, 76, 69, 62, 63, 58, 61, 62]\n\n# importing the required module\nimport matplotlib.pyplot as plt\n  \n# x axis values\nx = alturas\n# corresponding y axis values\ny = pesos\n  \n# plotting points as a scatter plot\n# s - point size, alpha - opacity\nplt.scatter(x, y, color= \"green\", marker= \"*\", s=40)\n\n# naming the x axis\nplt.xlabel('Altura em m')\n# naming the y axis\nplt.ylabel('Peso em Kg')\n  \n# giving a title to my graph\nplt.title('Alturas e Pesos dos Formandos')\n  \n# function to show the plot\nplt.show()\n\n?plt.scatter\n\n\n\n\n\n\n2.1.2 Dados quantitativos\n\nestado_civil = [\"solteiro\", \"casado\", \"solteiro\", \"divorciado\", \"solteiro\", \"solteiro\",\n               \"casado\", \"solteiro\", \"casado\", \"solteiro\", \"divorciado\", \"solteiro\"]\n              \nsolteiro_count = estado_civil.count(\"solteiro\")\n\ncasado_count = estado_civil.count(\"casado\")\n\ndivorciado_count = estado_civil.count(\"divorciado\")\ndivorciado_count\n\n2\n\n\nrepresentação gráfica (barras)\n\n# heights of bars\nheight = [solteiro_count, casado_count, divorciado_count]\n  \n# labels for bars\ntick_label = ['solteiro', 'casado', 'divorciado']\n  \n# plotting a bar chart\nplt.bar(tick_label, height, width = 0.8, color = 'blue')\n  \n# naming the x-axis\nplt.xlabel('Estado Civil')\n# naming the y-axis\nplt.ylabel('Nº de Indivíduos')\n# plot title\nplt.title('Frequências dos Estados Civis')\n  \n# function to show the plot\nplt.show()\n\n\n\n\nA partir de um dicionário\n\nalturas_dict = { \"Teresa\": 165, \"Maria\": 169, \"Joao\": 178, \"Carlos\": 187,\n                \"Vasco\": 182, \"Joana\": 162, \"Sofia\": 165, \"Pedro\": 177,\n                \"Afonso\": 175, \"Miguel\": 177, \"Ana\": 163, \"Margarida\": 162}\n\n# converter para uma lista para depois fazerf o gráfico                \nalturas_list = list(alturas_dict.values())\nalturas_list\n\n[165, 169, 178, 187, 182, 162, 165, 177, 175, 177, 163, 162]\n\n\nagora um histograma\n\nplt.hist(alturas_list, 4)\n\nplt.show()\n\n\n\n\nestatisticas\n\n# Calcula a média usando a fórmula\nn = len(alturas_list)\nmed_alturas_list= sum(alturas_list)/n\n\nprint(\"Median: {0}\".format(med_alturas_list))\n\nMedian: 171.83333333333334\n\n\n\n# Calcula a soma dos desvios quadrados\nss_alturas_list = sum((x - med_alturas_list)**2 for x in alturas_list)\nprint(ss_alturas_list)\n\n807.6666666666666\n\n\n\n# Variância amostral corrigida (da população) com ddof = 0\n# Variância amostral não corrigida com ddof = 1\nddof = 0\nvar_alturas_list = ss_alturas_list/(n-ddof)\nprint(var_alturas_list)\n\n# Desvio Padrão (corrigido - para alterar voltar ao passo anterior da variância)\ndp_alturas_list = var_alturas_list**0.5\nprint(dp_alturas_list)\n\n67.30555555555556\n8.203996316159312\n\n\nusando uma pckage\n\nimport statistics as st\n\nprint(\"The mean is:\", st.mean(alturas_list))\nprint(\"The mode is:\", st.mode(alturas_list))\nprint(\"The median is:\", st.median(alturas_list))\nprint(\"The sample variance is:\", st.variance(alturas_list))\nprint(\"The population variance is:\",st.pvariance(alturas_list))\nprint(\"The sample standard deviation is:\",st.stdev(alturas_list))\nprint(\"The population standard deviation is:\",st.pstdev(alturas_list))\n\nprint(\"The median is:\", st.median(alturas_list))\n# N = 4 devolve os 3 quartis superiores - percentis 25 e 75\nprint(\"The first three quartiles are:\", st.quantiles(alturas_list, n = 4))\n\nprint(\"The median is:\", st.median(alturas_list))\n# N = 10 devolve  - percentil 10, 20, ... , 90\nprint(\"The percentiles are:\", st.quantiles(alturas_list, n = 10))\n\nThe mean is: 171.83333333333334\nThe mode is: 165\nThe median is: 172.0\nThe sample variance is: 73.42424242424242\nThe population variance is: 67.30555555555556\nThe sample standard deviation is: 8.568794689117158\nThe population standard deviation is: 8.203996316159312\nThe median is: 172.0\nThe first three quartiles are: [163.5, 172.0, 177.75]\nThe median is: 172.0\nThe percentiles are: [162.0, 162.6, 164.8, 165.8, 172.0, 176.6, 177.1, 179.6, 185.5]\n\n\n\n\n2.1.3 Numpy\n\nimport numpy as np\n\no vector (array) é o objecto principal no NumPy\ncriar array a partir de lista\n\nalturas_list\n\narray = np.array(alturas_list)\narray\n\narray([165, 169, 178, 187, 182, 162, 165, 177, 175, 177, 163, 162])\n\n\nOs elementos dos array têm de ser todos do mesmo tipo.\narray 2D e 3D\n\narray_2D = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\narray_2D\n\nint_list = [[[1,2,4,4,5], [5,7,7,9,3], [3,2,5,3,6], [6,8,9,5,1]],\n             [[8,9,3,4,3], [4,6,4,3,3], [2,6,3,6,6], [8,5,7,2,1]],\n             [[1,2,7,9,5], [4,8,7,7,3], [8,5,7,6,7], [2,4,4,5,4]]]\narray_3D = np.array(int_list)\narray_3D\n\narray([[[1, 2, 4, 4, 5],\n        [5, 7, 7, 9, 3],\n        [3, 2, 5, 3, 6],\n        [6, 8, 9, 5, 1]],\n\n       [[8, 9, 3, 4, 3],\n        [4, 6, 4, 3, 3],\n        [2, 6, 3, 6, 6],\n        [8, 5, 7, 2, 1]],\n\n       [[1, 2, 7, 9, 5],\n        [4, 8, 7, 7, 3],\n        [8, 5, 7, 6, 7],\n        [2, 4, 4, 5, 4]]])\n\n\nCriar um array a partir do zero\n\n# Criar um array de inteiros de tamanho 10 (length-10) preenchido a zeros\nnp.zeros(10, dtype=int)\n\n# Criar um array de 3x5 (3 linhas e 5 colunas) com dados do tipo floating-point preenchido a 1s\nnp.ones((3, 5), dtype=float)\n# criar arrau 3D\nnp.ones((2, 4,6), dtype=float)\n\n# Criar um array de 3x5 (3 linhas e 5 colunas) com 3.14\nnp.full((3, 5), 3.14)\n\n# Criar um array de -3 a 4 com espaçamento igual entre os seus elementos\n# atenção que o valor inicial está incluido mas o de stop não\nnp.arange(-3,4)\n\n# Criar um array preenchido com uma sequência de 0 até 20 saltando de 2 em 2\n# quando se passa um terceiro argumento é interpretado como o espaçamento\nnp.arange(0, 20, 2)\n\n# Criar um array de 5 valores igualmente espaçados entre 0 e 1\nnp.linspace(0, 1, 5)\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\nos array são iteráveis\ntodos os iteradores são iteráveis (o contrário não é válido)\n\nclass SquaresIterator:\n    def __init__(self, n):\n        self.n = n\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.current &gt;= self.n:\n            raise StopIteration\n        result = self.current ** 2\n        self.current += 1\n        return result\n\n# Criar o iterador\niterator = SquaresIterator(5)\n\n# Usar o iterador para mostrar os valores\nwhile True:\n    try:\n        num = next(iterator)\n        print(num)\n    except StopIteration:\n        break\n\n0\n1\n4\n9\n16\n\n\nfunção gerador\n\n# Definir o gerador\ndef squares(n):\n    current = 0\n    while current &lt; n:\n        yield current ** 2\n        current += 1\n\n# Criar o iterador\nsquares_generator = squares(5)\n\n# Usar o iterador para imprimir os valores\nfor num in squares_generator:\n    print(num)\n\n0\n1\n4\n9\n16\n\n\nfazer um contador regressivo\n\n# Definir o gerador\ndef countdown(n):\n  while n &gt;= 0:\n    yield n \n    n -= 1\n\n# Criar o iterador instanciado com o valor 3\ncountdown_generator = countdown(3)\n\n# Usar o iterador para imprimir os valores\nfor minutos in countdown_generator:\n    print(minutos)\n\n3\n2\n1\n0\n\n\ncriar amostras pseudo/aleatorias&gt;\n\n# Criar um array de 3x5 de valores aleatórios entre 0 e 1\n# a partir de uma distribuição uniforme contínua.\n# cada número gerado tem a mesma probabilidade de ocorrer dentro do intervalo\nnp.random.random((3, 5))\n\n# Criar um array de 3x3 de números aleatórios com uma distribuição normal\n# com média 0 e desvio padrão 1\nnp.random.normal(0, 1, (3, 3))\n\n# Criar um array de 3x3 de números aleatórios no intervalo de [0, 10[\n# notem que 0 pertece ao intervalo mas 10 não...\nnp.random.randint(0, 10, (3, 3))\n\narray([[5, 3, 0],\n       [7, 6, 9],\n       [4, 9, 4]])\n\n\ndefinir semente\n\n# Define a semente\nnp.random.seed(42)\n\n# Gera uma matriz 3x3 de números aleatórios duma distribuição normal\nnormal_array = np.random.normal(loc=0, scale=1, size=(3, 3))\n\nprint(normal_array)\n\n# Criar uma matriz identidade de 5x5\nnp.eye(5)\n\n[[ 0.49671415 -0.1382643   0.64768854]\n [ 1.52302986 -0.23415337 -0.23413696]\n [ 1.57921282  0.76743473 -0.46947439]]\n\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\nexercicios\n\n# Proposta de Exercicio\n# Converta a lista de sudoku num array e mostre no écran o tipo da nova variável\n# sudoku_array para demonstrar que o código funcionou correctamente\nsudoku_list = [\n  [0, 0, 4, 3, 0, 0, 2, 0, 9], [0, 0, 5, 0, 0, 9, 0, 0, 1], [0, 7, 0, 0, 6, 0, 0, 4, 3],\n  [0, 0, 6, 0, 0, 2, 0, 8, 7], [1, 9, 0, 0, 0, 7, 4, 0, 0], [0, 5, 0, 0, 8, 3, 0, 0, 0],\n  [6, 0, 0, 0, 0, 0, 1, 0, 5], [0, 0, 3, 5, 0, 8, 6, 9, 0], [0, 4, 2, 9, 1, 0, 3, 0, 0]\n               ]\n               \nprint(type(sudoku_list))\n\n&lt;class 'list'&gt;\n\n\npassar para array\n\nsudoku_array = np.array(sudoku_list)\nprint(type(sudoku_array))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nCriar um array de zeros com 4 colunas e 10 linhas\n\n# Proposta de Exercicio\n# mostrar no écran o array para demonstrar que o código funcionou correctamente\nzero_array = np.zeros((10,4))\nzero_array\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\n# criação do doubling array que será mostrado no eixo do y\ndoubling_array = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n\n# criação do one_to_ten array que será mostrado no eixo do x\none_to_ten =np.arange(1,11)\n\n# código de display do plot que terá de ser adaptado\nplt.scatter(one_to_ten, doubling_array)\nplt.show()\n\n\n\n\n\n\n2.1.4 Atributos dos arrays NumPy\nobjecto do tipo gerador para numeros aleatorios\n\nrng = np.random.default_rng(seed=2012) \ntype(rng)\n\nnumpy.random._generator.Generator\n\n\ncria array com valores até 3 com 6 elementos\n\nx1 = rng.integers(3, size = 6)  \nx1\n\narray([2, 0, 1, 0, 0, 1], dtype=int64)\n\n\n\nrng = np.random.default_rng(seed=2012)  # seed for reproducibility\n\nx2 = rng.integers(10, size=(3, 4))  # two-dimensional array\nprint('x2 tem {0} dimensões e uma shape {1} o seu conteúdo é '.format(x2.ndim, x2.shape))\nprint('{0}'.format(x2))\n\nx2 tem 2 dimensões e uma shape (3, 4) o seu conteúdo é \n[[8 2 4 0]\n [1 3 4 2]\n [4 8 7 7]]\n\n\nmultidimensoes\n\nrng = np.random.default_rng(seed=2012)  # seed for reproducibility\n\nx5 = rng.integers(10, size=(3, 4, 2, 5, 9))  # two-dimensional array\nprint('x5 tem {0} dimensões e uma shape {1} o seu conteúdo é '.format(x5.ndim, x5.shape))\n\n#print('{0}'.format(x5))\n\nx5 tem 5 dimensões e uma shape (3, 4, 2, 5, 9) o seu conteúdo é \n\n\nmudar a shape dos arrays NumPy\n\nx1=np.arange(1,6)\nprint(x1)\nprint(x1.shape)\n\n[1 2 3 4 5]\n(5,)\n\n\nflatten(), reshape()\n\narray = np.array([[1,2], [3,4], [5,6]])\n\nflat_array = array.flatten()\n\ndiff_array = flat_array.reshape(2,3)\n\nprint(array)\n\n[[1 2]\n [3 4]\n [5 6]]\n\n\n\nprint(flat_array.shape, flat_array, flat_array.size)\n\nprint(diff_array.shape, diff_array, diff_array.size, diff_array.dtype, diff_array.ndim)\n\n(6,) [1 2 3 4 5 6] 6\n(2, 3) [[1 2 3]\n [4 5 6]] 6 int32 2\n\n\n\nnp.array([1.32, 5.78, 175.55]).dtype\n\nnp.array([\"Python\", \"para\", \"Estatísticas\",\"Oficiais\", \"otorrinolaringologista\"]).dtype\n\nboolean_array = np.array([[True, False], [False, False]], dtype=np.bool_)\nprint(boolean_array.dtype)\nboolean_array.astype(np.int32)\n\nbool\n\n\narray([[1, 0],\n       [0, 0]])\n\n\n\nnp.array([True, 42, 42.42, \"Hitchikers' Guide to the Galaxy\"]).dtype\n\ndtype('&lt;U32')\n\n\nexercicios\n\nsudoku_game = np.array([[0, 0, 4, 3, 0, 0, 2, 0, 9],\n       [0, 0, 5, 0, 0, 9, 0, 0, 1],\n       [0, 7, 0, 0, 6, 0, 0, 4, 3],\n       [0, 0, 6, 0, 0, 2, 0, 8, 7],\n       [1, 9, 0, 0, 0, 7, 4, 0, 0],\n       [0, 5, 0, 0, 8, 3, 0, 0, 0],\n       [6, 0, 0, 0, 0, 0, 1, 0, 5],\n       [0, 0, 3, 5, 0, 8, 6, 9, 0],\n       [0, 4, 2, 9, 1, 0, 3, 0, 0]])\n\nsudoku_solution = np.array([[8, 6, 4, 3, 7, 1, 2, 5, 9],\n       [3, 2, 5, 8, 4, 9, 7, 6, 1],\n       [9, 7, 1, 2, 6, 5, 8, 4, 3],\n       [4, 3, 6, 1, 9, 2, 5, 8, 7],\n       [1, 9, 8, 6, 5, 7, 4, 3, 2],\n       [2, 5, 7, 4, 8, 3, 9, 1, 6],\n       [6, 8, 9, 7, 3, 4, 1, 2, 5],\n       [7, 1, 3, 5, 2, 8, 6, 9, 4],\n       [5, 4, 2, 9, 1, 6, 3, 7, 8]])\n\n\n# Usando os arrays já definidos sudoku_game e sudoku_solution \n# crie um array 3D com o jogo e a solução\ngame_and_solution = np.array([sudoku_game, sudoku_solution])\nprint(game_and_solution)\ngame_and_solution.shape\n\n[[[0 0 4 3 0 0 2 0 9]\n  [0 0 5 0 0 9 0 0 1]\n  [0 7 0 0 6 0 0 4 3]\n  [0 0 6 0 0 2 0 8 7]\n  [1 9 0 0 0 7 4 0 0]\n  [0 5 0 0 8 3 0 0 0]\n  [6 0 0 0 0 0 1 0 5]\n  [0 0 3 5 0 8 6 9 0]\n  [0 4 2 9 1 0 3 0 0]]\n\n [[8 6 4 3 7 1 2 5 9]\n  [3 2 5 8 4 9 7 6 1]\n  [9 7 1 2 6 5 8 4 3]\n  [4 3 6 1 9 2 5 8 7]\n  [1 9 8 6 5 7 4 3 2]\n  [2 5 7 4 8 3 9 1 6]\n  [6 8 9 7 3 4 1 2 5]\n  [7 1 3 5 2 8 6 9 4]\n  [5 4 2 9 1 6 3 7 8]]]\n\n\n(2, 9, 9)\n\n\n\nnew_sudoku_game = np.array([[0, 0, 4, 3, 0, 0, 2, 0, 9],\n       [0, 0, 5, 0, 0, 9, 0, 0, 1],\n       [0, 7, 0, 0, 6, 0, 0, 4, 3],\n       [0, 0, 6, 0, 0, 2, 0, 8, 7],\n       [1, 9, 0, 0, 0, 7, 4, 0, 0],\n       [0, 5, 0, 0, 8, 3, 0, 0, 0],\n       [6, 0, 0, 0, 0, 0, 1, 0, 5],\n       [0, 0, 3, 5, 0, 8, 6, 9, 0],\n       [0, 4, 2, 9, 1, 0, 3, 0, 0]])\n\nnew_sudoku_solution = np.array([[8, 6, 4, 3, 7, 1, 2, 5, 9],\n       [3, 2, 5, 8, 4, 9, 7, 6, 1],\n       [9, 7, 1, 2, 6, 5, 8, 4, 3],\n       [4, 3, 6, 1, 9, 2, 5, 8, 7],\n       [1, 9, 8, 6, 5, 7, 4, 3, 2],\n       [2, 5, 7, 4, 8, 3, 9, 1, 6],\n       [6, 8, 9, 7, 3, 4, 1, 2, 5],\n       [7, 1, 3, 5, 2, 8, 6, 9, 4],\n       [5, 4, 2, 9, 1, 6, 3, 7, 8]])\n\n\n# Usando os novos já definidos new_sudoku_game e new_sudoku_solution\n# crie um novo array 3D com o novo jogo e a nova solução.\nnew_game_and_solution = np.array([new_sudoku_game, new_sudoku_solution])\n\n# Depois usando o arrays 3D do exercicio anterior e o agora criado \n# agrupe-os num novo array 4D\ngames_and_solutions = np.array([game_and_solution, new_game_and_solution])\n\n# Verifique a sua shape\ngames_and_solutions.shape\n\n(2, 2, 9, 9)\n\n\n\nprint(games_and_solutions)\n\n[[[[0 0 4 3 0 0 2 0 9]\n   [0 0 5 0 0 9 0 0 1]\n   [0 7 0 0 6 0 0 4 3]\n   [0 0 6 0 0 2 0 8 7]\n   [1 9 0 0 0 7 4 0 0]\n   [0 5 0 0 8 3 0 0 0]\n   [6 0 0 0 0 0 1 0 5]\n   [0 0 3 5 0 8 6 9 0]\n   [0 4 2 9 1 0 3 0 0]]\n\n  [[8 6 4 3 7 1 2 5 9]\n   [3 2 5 8 4 9 7 6 1]\n   [9 7 1 2 6 5 8 4 3]\n   [4 3 6 1 9 2 5 8 7]\n   [1 9 8 6 5 7 4 3 2]\n   [2 5 7 4 8 3 9 1 6]\n   [6 8 9 7 3 4 1 2 5]\n   [7 1 3 5 2 8 6 9 4]\n   [5 4 2 9 1 6 3 7 8]]]\n\n\n [[[0 0 4 3 0 0 2 0 9]\n   [0 0 5 0 0 9 0 0 1]\n   [0 7 0 0 6 0 0 4 3]\n   [0 0 6 0 0 2 0 8 7]\n   [1 9 0 0 0 7 4 0 0]\n   [0 5 0 0 8 3 0 0 0]\n   [6 0 0 0 0 0 1 0 5]\n   [0 0 3 5 0 8 6 9 0]\n   [0 4 2 9 1 0 3 0 0]]\n\n  [[8 6 4 3 7 1 2 5 9]\n   [3 2 5 8 4 9 7 6 1]\n   [9 7 1 2 6 5 8 4 3]\n   [4 3 6 1 9 2 5 8 7]\n   [1 9 8 6 5 7 4 3 2]\n   [2 5 7 4 8 3 9 1 6]\n   [6 8 9 7 3 4 1 2 5]\n   [7 1 3 5 2 8 6 9 4]\n   [5 4 2 9 1 6 3 7 8]]]]\n\n\n\n\n2.1.5 Aceder aos elementos\n\nx1\n\n# Aceder ao primeiro elemento \n# (começa em zero)\nprint(\"O primeiro elemento é {0} e o segundo {1}\" .format(x1[0], x1[1]))\n\n# Aceder ao ante-penultimo elemento\nx1[-3]\n\n# Aceder ao 1º elemento da 2ª linha\n# (linha e coluna começam em zero)\nx2\nx2[2,3]\n\nO primeiro elemento é 1 e o segundo 2\n\n\n7\n\n\nslicing arrays\n\nx1=np.arange(0,10)\n\nx1\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nx1[:3]\n\nx1[3:]\n\nx1[1:6:2]\n\narray([1, 3, 5])\n\n\nmodificar células\n\nx1[0] = 11\nx1\n\nx2[0,2] = 5\nx2\n\nx2[-1,0] = 6\nx2\n\narray([[8, 2, 5, 0],\n       [1, 3, 4, 2],\n       [6, 8, 7, 7]], dtype=int64)\n\n\nSubarrays Multidimensionais\n\nprint(x2)\n# fatia das 2 primeiras linhas\n# as 3 primeiras colunas\nx2[:2, :3] \n\nx2[0:2, 0:2]\n\nx2[::-1, ::-1]\n\n# fatia a primeira coluna de x2\nx2[:, 0] \n\n# igual a x2[0, :]\nx2[0, 0:4]\n\n[[8 2 5 0]\n [1 3 4 2]\n [6 8 7 7]]\n\n\narray([8, 2, 5, 0], dtype=int64)\n\n\nCriar Cópias de Subarrays\n\n# x2_sub é uma view e não uma cópia!!!\nx2_sub = x2[:2, :2]\nprint(x2_sub)\n\nx2_sub[0, 0] = 99\nprint(x2_sub)\nprint(x2)\n\nx2_sub_copy = x2[:2, :2].copy()\nprint(x2_sub_copy)\n\nx2_sub_copy[0, 0] = 42\nprint(x2_sub_copy)\nprint(x2)\n\n[[8 2]\n [1 3]]\n[[99  2]\n [ 1  3]]\n[[99  2  5  0]\n [ 1  3  4  2]\n [ 6  8  7  7]]\n[[99  2]\n [ 1  3]]\n[[42  2]\n [ 1  3]]\n[[99  2  5  0]\n [ 1  3  4  2]\n [ 6  8  7  7]]\n\n\nReshape do Array\n\n# criar uma grid de 3 por 3\ngrid = np.arange(1, 10).reshape(3, 3)\nprint(grid)\n\ngrid.reshape((1, 9))\n\ngrid.reshape((9,1))\n\n# criar um vector em linha através do reshape\nx = np.array([1, 2, 3])\nx\n\n# criar um vector em linha através do newaxis\nx[np.newaxis, :] \n\n# criar um vector em coluna através do reshape\nx.reshape((3, 1))\n\n# criar um vector em coluna através do newaxis\nx[: ,np.newaxis] \n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\narray([[1],\n       [2],\n       [3]])\n\n\nConcatenação de Arrays\n\nx = np.array([1, 2, 3])\ny = np.array([3, 2, 1])\nnp.concatenate([x, y])\n\n# concatenar mais do que 2 arrays de cada vez\nz = np.array([99, 99, 99])\nprint(np.concatenate([x, y, z]))\n\n[ 1  2  3  3  2  1 99 99 99]\n\n\n\ngrid = np.array([[1, 2, 3],\n                 [4, 5, 6]])\n# concatenar ao longo do eixo 1\nnp.concatenate([grid, grid])\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [1, 2, 3],\n       [4, 5, 6]])\n\n\n\n# concatenar ao longo do 2º eixo\n# (indice dos eixos começa em zero)\nnp.concatenate([grid, grid], axis=1)\n\n# stack vertical de arrays\nnp.vstack([x, grid])\n\n# stack horizontal de arrays\ny = np.array([[99],\n              [99]])\nnp.hstack([grid, y])\n\nx_exp0 = np.expand_dims(x, axis=0)\nprint(x)\nprint(x_exp0)\n\n[1 2 3]\n[[1 2 3]]\n\n\n\n# Definindo duas matrizes bidimensionais\nx = np.array([[1, 2],\n              [3, 4]])\n\ny = np.array([[5, 6],\n              [7, 8]])\n\n# Adicionando uma dimensão extra às matrizes\nx_expandido = np.expand_dims(x, axis=2)\ny_expandido = np.expand_dims(y, axis=2)\n\n# Concatenando as matrizes ao longo da terceira dimensão usando np.dstack()\nresultado = np.dstack((x_expandido, y_expandido))\n\nprint(x)\nprint(x_expandido)\nprint(resultado)\n\nx_exp1 = np.expand_dims(x, axis=1)\nx_exp1\n\nx_exp3 = np.expand_dims(x, axis=2)\nx_exp3\n\nx_exp2 = np.expand_dims(x, axis=2)\nx_exp2\n\n[[1 2]\n [3 4]]\n[[[1]\n  [2]]\n\n [[3]\n  [4]]]\n[[[1 5]\n  [2 6]]\n\n [[3 7]\n  [4 8]]]\n\n\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]])\n\n\nSplit de Arrays\n\nx = [1, 2, 3, 99, 99, 3, 2, 1]\nx1, x2, x3 = np.split(x, [3, 5])\nprint(x1, x2, x3)\n\ngrid = np.arange(16).reshape((4, 4))\ngrid\n\nupper, lower = np.vsplit(grid, [2])\nprint(upper)\nprint(lower)\n\nleft, right = np.hsplit(grid, [2])\nprint(left)\nprint(right)\n\n[1 2 3]\n\n\n [99 99] [3 2 1]\n[[0 1 2 3]\n [4 5 6 7]]\n[[ 8  9 10 11]\n [12 13 14 15]]\n[[ 0  1]\n [ 4  5]\n [ 8  9]\n [12 13]]\n[[ 2  3]\n [ 6  7]\n [10 11]\n [14 15]]\n\n\nAritmética de Arrays\nos operadores aritméticos são universal functions (Ufuncs)\n\nx = np.arange(4)\nprint(\"x      =\", x)\nprint(\"x + 5  =\", x + 5)\nprint(\"x - 5  =\", x - 5)\nprint(\"x * 2  =\", x * 2)\nprint(\"x / 2  =\", x / 2)\nprint(\"x // 2 =\", x // 2)  # divisão inteira\n\nprint(\"-x     = \", -x)     # - negação\nprint(\"x ** 2 = \", x ** 2) # ** quadrado\nprint(\"x ** 3 = \", x ** 3) # ** cubo\nprint(\"x % 2  = \", x % 2)  # % resto da divisão\n\nprint(\"e^x =\", np.exp(x))       # exponencial de base e\nprint(\"2^x =\", np.exp2(x))      # exponencial de base 2\nprint(\"3^x =\", np.power(3., x)) # exponencial de base 3\n\n# as operacções inversas das exponenciais, os logaritmos\n# também estão disponíveis\nx = [1, 2, 4, 10]\nprint(\"x        =\", x)\nprint(\"ln(x)    =\", np.log(x))\nprint(\"log2(x)  =\", np.log2(x))\nprint(\"log10(x) =\", np.log10(x))\n\nx      = [0 1 2 3]\nx + 5  = [5 6 7 8]\nx - 5  = [-5 -4 -3 -2]\nx * 2  = [0 2 4 6]\nx / 2  = [0.  0.5 1.  1.5]\nx // 2 = [0 0 1 1]\n-x     =  [ 0 -1 -2 -3]\nx ** 2 =  [0 1 4 9]\nx ** 3 =  [ 0  1  8 27]\nx % 2  =  [0 1 0 1]\ne^x = [ 1.          2.71828183  7.3890561  20.08553692]\n2^x = [1. 2. 4. 8.]\n3^x = [ 1.  3.  9. 27.]\nx        = [1, 2, 4, 10]\nln(x)    = [0.         0.69314718 1.38629436 2.30258509]\nlog2(x)  = [0.         1.         2.         3.32192809]\nlog10(x) = [0.         0.30103    0.60205999 1.        ]\n\n\nProdutos Externos\n\nx = np.arange(1, 6)\nx\n\nnp.multiply.outer(x, x)\n\narray([[ 1,  2,  3,  4,  5],\n       [ 2,  4,  6,  8, 10],\n       [ 3,  6,  9, 12, 15],\n       [ 4,  8, 12, 16, 20],\n       [ 5, 10, 15, 20, 25]])\n\n\nAgregações\n\nx = np.arange(1, 6)\nx\n\nnp.add.reduce(x)\n\nnp.multiply.reduce(x)\n\nnp.add.accumulate(x)\n\nnp.multiply.accumulate(x)\n\nrng = np.random.default_rng()\nbig_array = rng.random(1000000)\n\n# %timeit sum(big_array)\n# %timeit np.sum(big_array)\n\nnp.min(big_array), np.max(big_array)\n\n(1.2949751597712833e-06, 0.9999994694498117)\n\n\nEstatísticas Básicas\n\n# Alturas dos primeiros ministros portugueses\nalturas = np.array([169, 170, 159, 173, 173, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183, 182,\n                   178, 173, 174, 173, 176, 164, 170, 173, 182, 180, 183, 178, 182, 174, 175, 179,\n                   174, 173, 162, 173, 171, 165, 164, 168, 175, 165, 181, 172])\n                   \nprint(\"Média das Alturas:       \", np.mean(alturas))\nprint(\"Desvio Padrão:           \", np.std(alturas))\nprint(\"Altura Minima:    \", np.min(alturas))\nprint(\"Altura Máxima:    \", np.max(alturas))\n\nprint(\"Percentil 25:   \", np.percentile(alturas, 25))\nprint(\"Mediana:            \", np.median(alturas))\nprint(\"Percentil 75:   \", np.percentile(alturas, 75))\n\n#%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.hist(alturas,6)\nplt.title('Distribuição das Alturas dos Primeiro Ministros')\nplt.xlabel('Altura (cm)')\nplt.ylabel('Número');\n\nimport scipy.stats as st\n\n# criar um intervalo de confiança a 95% para a altura média da população\n# usando a distribuição normal\nst.norm.interval(confidence=0.95, loc=np.mean(alturas), scale=st.sem(alturas))\n\n#%pwd\n\nMédia das Alturas:        173.75\nDesvio Padrão:            6.049511625667885\nAltura Minima:     159\nAltura Máxima:     185\nPercentil 25:    170.75\nMediana:             173.0\nPercentil 75:    178.0\n\n\n(171.94185115248527, 175.55814884751473)\n\n\n\n\n\n\n# Linux\n# datadir = \"../../../../Datasets/Hospital/\"\n\n# Windows\n# datadir = \"..\\\\..\\\\..\\\\..\\\\Datasets\\\\Hospital\\\\\"\ndatadir =\"data\\\\\"\nfilename = \"D_Internamento_1.csv\"\n\n\nimport pandas as pd \n\ndf_int = pd.read_csv(f\"{datadir}{filename}\", skiprows=2)\ndf_int.head()\n\n\n\n\n\n\n\n\nAno\nNORDEM\nD010001\nD010002\nD010003\nD010004\nD010005\nD010006\nD010007\nD010008\n...\nD022097\nD022098\nD022101\nD022102\nD022103\nD022104\nD022105\nD022106\nD022107\nD022108\n\n\n\n\n0\n2012\n130\n5149.0\n4471.0\n533.0\n18.0\n2079.0\n2079.0\n127.0\n72894.0\n...\n62\n21104\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n2012\n75\n1086.0\n963.0\n85.0\n0.0\n0.0\n0.0\n38.0\n14864.0\n...\n19\n6454\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n2012\n128\n231.0\n205.0\n10.0\n0.0\n0.0\n0.0\n16.0\n6186.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n2012\n111\n421.0\n284.0\n101.0\n0.0\n0.0\n0.0\n36.0\n14813.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n2012\n14\n3737.0\n3257.0\n4.0\n272.0\n3012.0\n3012.0\n204.0\n74172.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 226 columns\n\n\n\n\nprint(df_int.D010002.min())\nprint(df_int.D010002.max())\n\nprint(df_int.D010004.min())\nprint(df_int.D010004.max())\n\n0.0\n42051.0\n0.0\n311.0\n\n\n\ndef plot_hist(x, p=5):\n # Plot the distribution and mark the mean\n plt.hist(x, alpha=.5)\n plt.axvline(x.mean())\n # 95% confidence interval \n plt.axvline(np.percentile(x, p/2.), color='red', linewidth=3)\n plt.axvline(np.percentile(x, 100-p/2.), color='red', linewidth=3)\n \ndef plot_dists(a, b, nbins, a_label='com_p', b_label='sem_p', p=5):\n # Create a single sequence of bins to be shared across both\n # distribution plots for visualization consistency.\n combined = pd.concat([a, b])\n breaks = np.linspace(\n combined.min(), \n combined.max(), \n num=nbins+1)\n plt.subplot(2, 1, 1)\n plot_hist(a)\n plt.title(a_label)\n \n plt.subplot(2, 1, 2)\n plot_hist(b)\n plt.title(b_label)\n \n plt.tight_layout()\n \nplot_dists(df_int.D010002, df_int.D010004, 20, a_label='Com Parecer', b_label='Sem parecer')\nplt.show()\n\n\n\n\n\n\n2.1.6 Introdução às bases de dados e modelos\nestabelecer ligação a base de dados\n\n# importar package \nimport cx_Oracle # cx_Oracle to access Oracle database\n\n# criar conexão\n# host = c21oradev01.int.ine.pt\n# port = 1521\n# service =FORMACAO\ndsn_tns = cx_Oracle.makedsn('c21oradev01.int.ine.pt', '1521', \n                            service_name='FORMACAO') \n\npedir user e password\n\n# importar package em vez do package todo\n# trazer só o método getpass\nfrom getpass import getpass # para ler a password sem a mostrar\n\nmy_user = \"BRUNO.LIMA\"\nmy_password = \"*******\"\n\ncriar ligação\n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\nabrir cursor\n\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\nconstruir query sql\n\nmy_sql = \"\"\"\nSELECT ano, nordem, nuts2, dtcc_cod, ent_cod\nFROM BDIFRM.TD_HOSP_10\n\"\"\"\n\nexecutar\n\nc.execute(my_sql)\n\nguardar dados\n\n# guardar os dados numa estrutura Python Pandas\nimport pandas as pd\n\ndf = pd.DataFrame(c.fetchall())\n\nfechar cursor\n\nc.close()\n\nfechar conexão à base de dados\n\nconn.close()\n\nexplorar os dados\n\ndf.head()\n\n# atribuir os nomes das colunas\nnomes_col = [\"ano\", \"nordem\", \"nuts2\", \"dtcc_cod\", \"ent_cod\"]\ndf.columns = nomes_col\n\nguardar num ficheiro .csv\n\ndf.head()\n\ndf.to_csv('data/tsee_2023.csv', index=False)\n\nexercicio\nvoltar a criar ligação\n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\nvoltar a criar cursor\n\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\ncontar nº de registos\n\nmy_sql = \"\"\"\nselect count(1) from\nBDIFRM.TD_HOSP_10\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\ncontar distritos:\n\nmy_sql= \"\"\"\nselect dtcc_cod, count(1) from BDIFRM.TD_HOSP_10\ngroup by dtcc_cod\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\ndistritos com 6:\n\nmy_sql= \"\"\"\nselect * from BDIFRM.TD_HOSP_10\nwhere dtcc_cod like '%6%'\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\nNUTS2 11 ou 17 de 2012:\n\nmy_sql= \"\"\"\nselect * from BDIFRM.TD_HOSP_10\nwhere nuts2 in ('11', '17') AND ano = '2012'\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\ncontar pessoal ao serviço em tabela de hospitais\n\n# query\nmy_sql= \"\"\"\nselect ano, sum(c10001), sum(c10002), sum(c10003)\nfrom BDIFRM.TD_RECHUM1_10\nwhere ano = '2012'\ngroup by ano\n\"\"\"\n# executa\nc.execute(my_sql)\n# faz fetch\ndf = pd.DataFrame(c.fetchall())\n# mostra resultado\ndf\n\nexemplos com joins e grupos\n\nmy_sql = \"\"\"\nselect t.ano, t.nuts2, t.dtcc_cod, m.cc_dsg, \nsum(r.C21041) cardio_total, sum(r.C21042) cardio_homnes, sum(r.C21043) cardio_mulheres \nfrom BDIFRM.TD_HOSP_10 h\nleft join BDIFRM.TD_NUM_10 m \non h.dtcc_cod = m.dtcc_cod\nleft join BDIFRM.REC_HUM1_10 r\non h.nordem = r.nordem\n--using (dtcc_cod)\nwhere nuts2 like '%1'\ngroup by t.ano, t.nuts2, t.dtcc_cod, m.cc_dsg\n\"\"\"\n# executa\nc.execute(my_sql)\n# faz fetch\ndf = pd.DataFrame(c.fetchall())\n# mostra resultado\ndf\n\nfechar cursor e conexção\n\nc.close()\n\nconn.close()"
  },
  {
    "objectID": "200-mod2.html#series-e-dataframes",
    "href": "200-mod2.html#series-e-dataframes",
    "title": "2  Data Science (Basics)",
    "section": "2.2 Series e Dataframes",
    "text": "2.2 Series e Dataframes\n\n2.2.1 Series\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0])\ndata\n\n0    0.25\n1    0.50\n2    0.75\n3    1.00\ndtype: float64\n\n\n\ndata.values\n\narray([0.25, 0.5 , 0.75, 1.  ])\n\n\n\ndata.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\naceder aos elementos series\n\n# acesso ao 2º elemento\n# lembrar que começa em 0\ndata[1]\n# acesso a 2 elementos no meio da Series \n# tal como no Numpy o último não está contido\ndata[1:3]\n# acesso a todos os elementos a partir do 3º\ndata[2:]\n\n2    0.75\n3    1.00\ndtype: float64\n\n\nIndexes nas series\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n\na    0.25\nb    0.50\nc    0.75\nd    1.00\ndtype: float64\n\n\n\npopulation_dict = {'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233,\n                   'Porto': 231834, 'Cascais': 214239, 'Loures': 201349,\n                   'Braga': 193324, 'Almada': 177943}\npopulation = pd.Series(population_dict)\npopulation\n\nLisboa               544325\nSintra               385989\nVila Nova de Gaia    304233\nPorto                231834\nCascais              214239\nLoures               201349\nBraga                193324\nAlmada               177943\ndtype: int64\n\n\ncom indexes explicitos o último elemento está contido no slice\n\nserie = pd.Series({2:'a', 1:'b', 3:'c'})\n\nserie[1:3]\n\n1    b\n3    c\ndtype: object\n\n\nconstruir series\n\npd.Series([2, 4, 6])\n\npd.Series(10, index=[10, 20, 30])\n\npd.Series({2:'a', 1:'b', 3:'c'})\n\npd.Series({'a':1.2, 'b':1.5, 'c':1.7})\n\na    1.2\nb    1.5\nc    1.7\ndtype: float64\n\n\n\n\n2.2.2 DataFrames\n\narea_dict = {'Lisboa': 100.1,'Sintra': 23.8, 'Vila Nova de Gaia': 56.3,\n                   'Porto': 41.4, 'Cascais': 97.1, 'Loures': 11.8,\n                   'Braga': 41, 'Almada': 14.7}\narea = pd.Series(area_dict)\narea\n\nLisboa               100.1\nSintra                23.8\nVila Nova de Gaia     56.3\nPorto                 41.4\nCascais               97.1\nLoures                11.8\nBraga                 41.0\nAlmada                14.7\ndtype: float64\n\n\n\npopulation\n\nLisboa               544325\nSintra               385989\nVila Nova de Gaia    304233\nPorto                231834\nCascais              214239\nLoures               201349\nBraga                193324\nAlmada               177943\ndtype: int64\n\n\njuntar numa dataFrame\n\ncities = pd.DataFrame({'population': population,\n                       'area': area})\ncities\n\n\n\n\n\n\n\n\npopulation\narea\n\n\n\n\nLisboa\n544325\n100.1\n\n\nSintra\n385989\n23.8\n\n\nVila Nova de Gaia\n304233\n56.3\n\n\nPorto\n231834\n41.4\n\n\nCascais\n214239\n97.1\n\n\nLoures\n201349\n11.8\n\n\nBraga\n193324\n41.0\n\n\nAlmada\n177943\n14.7\n\n\n\n\n\n\n\n\n# atributo index \ncities.index\n\n# atributo columns \ncities.columns\n\nIndex(['population', 'area'], dtype='object')\n\n\ncriar dataFrame a partir de uma serie\n\n# a partir de um único objecto Series\npd.DataFrame(population, columns=['population'])\n\n\n\n\n\n\n\n\npopulation\n\n\n\n\nLisboa\n544325\n\n\nSintra\n385989\n\n\nVila Nova de Gaia\n304233\n\n\nPorto\n231834\n\n\nCascais\n214239\n\n\nLoures\n201349\n\n\nBraga\n193324\n\n\nAlmada\n177943\n\n\n\n\n\n\n\ncom um array\n\n# a partir de um array Numpy 2D\npd.DataFrame(np.random.rand(3, 2),\n             columns=['col1', 'col2'],\n             index=['a', 'b', 'c'])\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\na\n0.181825\n0.183405\n\n\nb\n0.304242\n0.524756\n\n\nc\n0.431945\n0.291229\n\n\n\n\n\n\n\ncom um dicionario\n\n# a partir de uma lista de dicionarios\ndata = [{'simples': i, 'dobro': 2 * i, 'triplo': 3 * i}\n        for i in range(6)]\npd.DataFrame(data)\n\n\n\n\n\n\n\n\nsimples\ndobro\ntriplo\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n2\n3\n\n\n2\n2\n4\n6\n\n\n3\n3\n6\n9\n\n\n4\n4\n8\n12\n\n\n5\n5\n10\n15\n\n\n\n\n\n\n\n\n# se algumas chaves do dicionário estiverem em falta\n# vão ser preenchidas com o valor NaN\npd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1.0\n2\nNaN\n\n\n1\nNaN\n3\n4.0\n\n\n\n\n\n\n\n\n# a partir de um dicionário\npd.DataFrame({'population': population,\n              'area': area})\n\n\n\n\n\n\n\n\npopulation\narea\n\n\n\n\nLisboa\n544325\n100.1\n\n\nSintra\n385989\n23.8\n\n\nVila Nova de Gaia\n304233\n56.3\n\n\nPorto\n231834\n41.4\n\n\nCascais\n214239\n97.1\n\n\nLoures\n201349\n11.8\n\n\nBraga\n193324\n41.0\n\n\nAlmada\n177943\n14.7\n\n\n\n\n\n\n\n\n\n2.2.3 Pandas index\nindex como array imutável\n\nind = pd.Index([2, 3, 5, 7, 11])\nind\n\n# funciona e acede-se como um array\nind[1]\n\n# podem-se obter slices\nind[::2]\n\n# tem muitos atributos iguais\nprint(ind.size, ind.shape, ind.ndim, ind.dtype)\n\n5 (5,) 1 int64\n\n\npor ser imutável\n\n# mas é imutável, i.e. não pode ser alterado\n# pelos meios habituais, por isso isto não funciona\nind[1] = 0\n\nO Index também pode ser visto como um set ordenado\n\nindA = pd.Index([1, 3, 5, 7, 9])\nindB = pd.Index([2, 3, 5, 7, 11])\n\n# interseção de conjuntos\nindA.intersection(indB)\n\n# união de conjuntos\nindA.union(indB)\n\n# diferença entre conjuntos\nindA.symmetric_difference(indB)\n\nIndex([1, 2, 9, 11], dtype='int64')\n\n\n\n\n2.2.4 Seleção de dados\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n\ndata['b']\n\n# verifica se tem esta chave (key)\n'a' in data\n\n# acede a todas as chaves (keys)\ndata.keys()\n\n# acede aos pares\ndata.items()\n\n# acede a todos os valores (items)\nlist(data.items())\n\n[('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)]\n\n\n\n# expande a series acrescentando um elemento\ndata['e'] = 1.25\ndata\n\n# altera a série mapeando a key b para 0.48 em vez de 0.5\ndata['b'] = 0.48\ndata\n\n# slicing com index explicito\ndata['a':'c']\n\n# slicing com index implicito\ndata[0:2]\n\n# masking \ndata[(data &gt; 0.3) & (data &lt; 0.8)]\n\n# fancy indexing\ndata[['a', 'e']]\n\na    0.25\ne    1.25\ndtype: float64\n\n\n\n\n2.2.5 Indexers: loc (explícito) e iloc (implícito)\n\ndata = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\ndata\n\n# indice explicito \ndata.loc[1]\n\n# indice explicito \ndata.loc[1:3]\n\n# indice implicito\ndata.iloc[1]\n\n# indice implicito \ndata.iloc[1:3]\n\n3    b\n5    c\ndtype: object\n\n\n\n\n2.2.6 DataFrames\n\npop = pd.Series({'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233,\n                   'Porto': 231834, 'Cascais': 214239, 'Loures': 201349,\n                   'Braga': 193324, 'Almada': 177943})\n\narea = pd.Series({'Lisboa': 100.1,'Sintra': 23.8, 'Vila Nova de Gaia': 56.3,\n                   'Porto': 41.4, 'Cascais': 97.1, 'Loures': 11.8,\n                   'Braga': 41, 'Almada': 14.7})\n\ndata = pd.DataFrame({'area':area, 'pop':pop})\ndata\n\n\n\n\n\n\n\n\narea\npop\n\n\n\n\nLisboa\n100.1\n544325\n\n\nSintra\n23.8\n385989\n\n\nVila Nova de Gaia\n56.3\n304233\n\n\nPorto\n41.4\n231834\n\n\nCascais\n97.1\n214239\n\n\nLoures\n11.8\n201349\n\n\nBraga\n41.0\n193324\n\n\nAlmada\n14.7\n177943\n\n\n\n\n\n\n\ntransformar os dados\n\n# criar coluna\ndata['dens'] = data['pop'] / data['area'] \ndata\n\n# ver os dados como num array 2D\ndata.values\n\n# transposicao para trocar linhas com colunas\ndata.T\n\n\n\n\n\n\n\n\nLisboa\nSintra\nVila Nova de Gaia\nPorto\nCascais\nLoures\nBraga\nAlmada\n\n\n\n\narea\n100.100000\n23.80000\n56.300000\n41.400000\n97.100000\n11.800000\n41.000000\n14.700000\n\n\npop\n544325.000000\n385989.00000\n304233.000000\n231834.000000\n214239.000000\n201349.000000\n193324.000000\n177943.000000\n\n\ndens\n5437.812188\n16218.02521\n5403.783304\n5599.855072\n2206.374871\n17063.474576\n4715.219512\n12104.965986\n\n\n\n\n\n\n\naceder aos dados\n\n# aceder a linha\ndata.values[0]\n\n# aceder a coluna \ndata['area']\n\n# aceder usando os indices implicitos inteiros\n# as primeiras 3 linhas, 0, 1 e 2\n# as primeiras 2 colunas 0 e 1\ndata.iloc[:3, :2]\n\n# aceder àos mesmos dados que anteriormente\n# agora usando os indices explicitos \ndata.loc[:'Vila Nova de Gaia', :'pop']\n\ndata.loc[data.dens &gt; 10000, ['pop', 'dens']]\n\ndata.loc[data['dens'] &gt; 10000, ['pop', 'dens']]\n\ndata.iloc[0, 2] = 5000\ndata\n\n\n\n\n\n\n\n\narea\npop\ndens\n\n\n\n\nLisboa\n100.1\n544325\n5000.000000\n\n\nSintra\n23.8\n385989\n16218.025210\n\n\nVila Nova de Gaia\n56.3\n304233\n5403.783304\n\n\nPorto\n41.4\n231834\n5599.855072\n\n\nCascais\n97.1\n214239\n2206.374871\n\n\nLoures\n11.8\n201349\n17063.474576\n\n\nBraga\n41.0\n193324\n4715.219512\n\n\nAlmada\n14.7\n177943\n12104.965986\n\n\n\n\n\n\n\n\n\n2.2.7 Operações no Pandas\nnp.random.default_rng(42) cria uma instância da classe gerador do módulo numpy.random module. Este gerador baseado no algoritmo PCG64 é um gerador de números pseudo-random que na realidade é determinado pelo valor inicial da seed, neste caso 42.\n\n# criação de Series com números aleatórios \n# entre 0 e 10 (exclusive) e 4 linhas\n# a seed está fixa a 42\nrng = np.random.default_rng(42)\nser = pd.Series(rng.integers(0, 10, 4))\nser\n\n# a função unária preserva os indices\nnp.exp(ser)\n\n# criação de dataFrame com números aleatórios entre\n# 0 e 10 (exclusive) e 3 linhas e 4 colunas\ndf = pd.DataFrame(rng.integers(0, 10, (3, 4)),\n                  columns=['A', 'B', 'C', 'D'])\ndf\n\n# a função unária preserva os indices\nnp.sin(df * np.pi / 4)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1.224647e-16\n-2.449294e-16\n0.000000\n-1.000000\n\n\n1\n1.000000e+00\n0.000000e+00\n-0.707107\n0.707107\n\n\n2\n-7.071068e-01\n-7.071068e-01\n-0.707107\n-0.707107\n\n\n\n\n\n\n\nNas operações binárias como a soma e a multiplicação o Pandas alinha os indices ao passar os objectos para as ufunc. Nos items para os quais não há uma entrada é colocado o valor NaN, “Not a Number” que é como o Pandas marca valores em falta (missing data)\n\npop_u = pd.Series({'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233}, name='pop')\n\narea_u = pd.Series({'Sintra': 23.8, 'Vila Nova de Gaia': 56.3, 'Porto': 41.4}, name='area')\n\npop_u / area_u\n\narea.index.union(pop.index)\n\nIndex(['Lisboa', 'Sintra', 'Vila Nova de Gaia', 'Porto', 'Cascais', 'Loures',\n       'Braga', 'Almada'],\n      dtype='object')\n\n\n\n# se os indices forem numéricos ficam ordenados\nA = pd.Series([2, 4, 6], index=[0, 1, 2])\nB = pd.Series([1, 3, 5], index=[1, 2, 3])\nA + B\n\n# existem várias hipóteses para lidar com missing values\n# adiciona as series mas sunstitui nan por zero\nA.add(B, fill_value=0)\n\nA = pd.DataFrame(rng.integers(0, 20, (2, 2)),\n                 columns=['a', 'b'])\nA\n\nB = pd.DataFrame(rng.integers(0, 10, (3, 3)),\n                 columns=['b', 'a', 'c'])\nB\n\n# exemplo de alinhamento de indices nas DataFrames\nA + B\n\nA.values.mean()\n\n# outra forma de lidar com missing values\nA.add(B, fill_value=A.values.mean())\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n13.00\n7.00\n10.25\n\n\n1\n23.00\n18.00\n15.25\n\n\n2\n17.25\n13.25\n14.25\n\n\n\n\n\n\n\nUfuncs: Operações entre DataFrames e Series\n\nA = rng.integers(10, size=(3, 4))\nA\n\n# subtrai a todas as linhas a primeira\nA - A[0]\n\n# igual ao exemplo anterior mas usando indices explicitos\ndf = pd.DataFrame(A, columns=['Q', 'R', 'S', 'T'])\ndf - df.iloc[0]\n\n# Subtrair uma coluna em vez de uma linha\n# não esquecer de indicar axis = 0\ndf.subtract(df['R'], axis=0)\n\n# vai buscar a linha de indice 2 e as colunas (todas) com step 2\nprint(df.head())\nmeialinha = df.iloc[1, ::2]\nmeialinha\n\n# alinha os indices antes da operação\n# por isso só vai subtrair nas colunas Q e S\ndf - meialinha\n\n   Q  R  S  T\n0  4  4  2  0\n1  5  8  0  8\n2  8  2  6  1\n\n\n\n\n\n\n\n\n\nQ\nR\nS\nT\n\n\n\n\n0\n-1.0\nNaN\n2.0\nNaN\n\n\n1\n0.0\nNaN\n0.0\nNaN\n\n\n2\n3.0\nNaN\n6.0\nNaN\n\n\n\n\n\n\n\n\n\n2.2.8 Missing values\nO tipo None do Python também pode ser usado para marcar missing values, mas não suporta operações aritméticas. Assim o uso do nan é mais vantajoso.\n\nvals1 = np.array([1, None, 2, 3])\nvals1\n\narray([1, None, 2, 3], dtype=object)\n\n\nnão conseguimos sumar\n\nvals1.sum()\n\ncom nan não dá erro\n\n# criação de um array com nan a marcar missing values\nvals2 = np.array([1, np.nan, 3, 4]) \nvals2\n\nprint(1 + np.nan)\nprint(0 * np.nan)\nprint(vals2.sum(), vals2.min(), vals2.max())\n\nnan\nnan\nnan nan nan\n\n\nO Pandas converte None em nan\n\npd.Series([1, np.nan, 2, None])\n\n0    1.0\n1    NaN\n2    2.0\n3    NaN\ndtype: float64\n\n\nTodos os tipos começados por maiuscula como Int32 podem ser nullable e por isso receber NaN, None ou NA\n\npd.Series([1, np.nan, 2, None, pd.NA], dtype='Int32')\n\n0       1\n1    &lt;NA&gt;\n2       2\n3    &lt;NA&gt;\n4    &lt;NA&gt;\ndtype: Int32\n\n\n\ndata = pd.Series([1, np.nan, 'hello', None])\ndata\n\ndata.isnull()\n\ndata.isna()\n\ndata.notnull()\n\ndata[data.notnull()]\n\ndata.dropna()\n\n0        1\n2    hello\ndtype: object\n\n\n\ndf = pd.DataFrame([[1,      np.nan, 2],\n                   [2,      3,      5],\n                   [np.nan, 4,      6]])\ndf\n\n# remove registos com missing values\ndf.dropna()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n1\n2.0\n3.0\n5\n\n\n\n\n\n\n\n\ndf = pd.DataFrame([[1,      np.nan, 2],\n                   [2,      3,      5],\n                   [np.nan, 4,      6]])\ndf\n\n# remove colunas indicando axis = 1\n# também se pode indicar axis = columns' em vez de 1\ndf.dropna(axis = 1)\n\n\n\n\n\n\n\n\n2\n\n\n\n\n0\n2\n\n\n1\n5\n\n\n2\n6\n\n\n\n\n\n\n\n\ndf.loc[:,3] = np.nan\ndf\n\n# excluir aopenas quando todos são nulos\ndf.dropna(axis='columns', how='all')\n\ndf.fillna(0)\n# podemos fazer forward fill\ndf.ffill()\n# backward fill\ndf.bfill()\n# Ou amobos\ndf.bfill().ffill()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n3.0\n2\nNaN\n\n\n1\n2.0\n3.0\n5\nNaN\n\n\n2\n2.0\n4.0\n6\nNaN\n\n\n\n\n\n\n\nestas experiências não alteram mesmo a DataFrame se não usarmos o parâmetro inplace\n\ndf.dropna(axis='columns', how='all', inplace = True)\n\ndf.ffill(inplace = True)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1.0\nNaN\n2\n\n\n1\n2.0\n3.0\n5\n\n\n2\n2.0\n4.0\n6\n\n\n\n\n\n\n\n\ndf.isna()\n\n# proposta testa por partes a expressão\n# estamos a examinar só a 2ª coluna\ndf.iloc[:,1].isna().sum()\n\ndf.iloc[:,1].fillna(df.iloc[:,1].mean())\n\ndf.head()\n\ndf.describe()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       3 non-null      float64\n 1   1       2 non-null      float64\n 2   2       3 non-null      int64  \ndtypes: float64(2), int64(1)\nmemory usage: 204.0 bytes\n\n\nquando estamos a examinar uma coluna tb pode ser útil saber quantos valores unicos tem\n\n# porque é que usamos o len\n# e não o .sum()\nlen(df.iloc[:,0].unique())\n\nsum(df.iloc[:,0].unique())\n\ndf.iloc[:,0].unique() \n\nvalor, contador = np.unique(df.iloc[:,2], return_counts = True)\nprint(valor)\nprint(contador)\n\nfor valor, contador in zip(valor, contador):\n    print(f\"{valor} aparece {contador} vezes\")\n\n[2 5 6]\n[1 1 1]\n2 aparece 1 vezes\n5 aparece 1 vezes\n6 aparece 1 vezes\n\n\n\n# a função zip transforma 2 iteráveis num único iterável\n# em que cada elemento é um par\ncities = ['Elvas', 'Evora', 'Estremoz']\npop = [21750, 81127, 12750]\n \nnew_dict = {cities: pop for cities, pop in zip(cities, pop)}\nprint(new_dict)\n\n{'Elvas': 21750, 'Evora': 81127, 'Estremoz': 12750}\n\n\n\n\n2.2.9 exemplos extra formação\n\nimport pandas as pd\nimport altair as alt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\npenguins = pd.read_csv(\"https://pos.it/palmer-penguins-github-csv\")\n\n\npenguins.groupby(\"species\").size().reset_index(name = \"count\")\n\n\n\n\n\n\n\n\nspecies\ncount\n\n\n\n\n0\nAdelie\n152\n\n\n1\nChinstrap\n68\n\n\n2\nGentoo\n124\n\n\n\n\n\n\n\n\ncolors = [\"#FF8C00\", \"#A020F0\", \"#008B8B\"]\nsns.set_palette(colors, n_colors = 3)\n\n\npenguins[\"bill_ratio\"] = (\n   penguins[\"bill_length_mm\"] / penguins[\"bill_depth_mm\"] \n)\nsns.displot(penguins, \n            x = \"bill_ratio\", \n            hue = \"species\", \n            kind = \"kde\", fill = True, aspect = 2, height = 3)\nplt.show()\n\n\n\n\n\ndef collatz(num):\n    if num % 2 == 0:\n        return num // 2\n    else:\n        return 3 * num + 1\n\nnumber = 5\n\nwhile number != 1:\n    number = collatz(int(number))\n    print(number)\n\n16\n8\n4\n2\n1"
  },
  {
    "objectID": "300-mod3.html#jupyter-e-markdown",
    "href": "300-mod3.html#jupyter-e-markdown",
    "title": "3  Jupyter e Visualizações (Basics)",
    "section": "3.1 Jupyter e Markdown",
    "text": "3.1 Jupyter e Markdown\nUm JUPYTER Notebook é composto de uma combinação de células de código e documentação.\nAbrir através do prompt: Anaconda prompt -&gt; jupyter notebook\nFicheiros do tipo .ipynb\n\nShortcuts \nThe Zen of Python, by Tim Peters\n\n\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n3.1.1 Data Exploration and Visualization (Basics)\n\n3.1.1.1 Importar Dados Externos para Pandas Dataframe\nExemplo .csv Exemplo .xlsx\n\n# Obter dados a partir do Link Direto ao dados\n# Importar Bibliotecas\n# No ambiente formação devem-se utilizar os ficheiros locais (poderá dar erro \"http error 404\")\n\nimport pandas as pd\nimport requests \n# Link direto para ficheiro CSV: Despesa com Medicamentos nos Hospitais do SNS\nficheiro = r'https://dados.gov.pt/pt/datasets/r/8803343f-6e1b-47de-87ac-26432adb45f0'\n\n# Importar num Pandas Dataframe\ndf = pd.read_csv(ficheiro, sep=';')\n\n# Informação\n# print(df.head(5))\n# print(df.info())\n# print(df.describe())\n\n\n# Import Packages openxyl para poder trabalhar com XLSX\nimport pandas as pd\nimport openpyxl\nimport numpy as np\n\n# Link Direto para ficheiro XLSX\nficheiro = r'https://dados.gov.pt/s/resources/classificacao-etaria-de-teatro/20240331-230417/classificacaoetariateatro.xlsx'\n# Importar num DataFrame\n# Argumentos uteis: skiprows=[0,1], usecols='A', nrows=2, header=None, sheet_name='Sheet1'\n\n# Mostrar primeiras 3 linhas e ultimas 3 linhas (head e tail em conjunto)\n# Utilizar iloc e numpy\n# print (df.iloc[np.r_[0:3, -3:0]])  # head e tail\n# print(df.info())\n# print(df.describe())\n\n\n\n3.1.1.2 Importar tabela da BD\n\n# Obter Password e Utilizador para Ligacao SQL\nfrom getpass import getpass # para ler a password sem a mostrar\nmy_user = '\"BRUNO.LIMA\"[BDIFRM]' \nmy_password = '*******'\n\n\n# Ler Dados da BD\n# criar conexão\nimport cx_Oracle \nimport pandas as pd\nhost = 'c21oradev01.int.ine.pt'\nport = '1521'\nservice = 'FORMACAO'\ndsn_tns = cx_Oracle.makedsn(host, port, service_name=service) \n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\n# Cursor:\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\nLer Diferentes Views para Pandas DF\n\n# Dados por Municipio:\n# SQL String\nmy_sql = \"\"\"\nselect *\nfrom V_BGRI2021_DTMN_PT \n\"\"\"\n# Executar o cursor c com a string como parâmetro\nc.execute(my_sql)\n\n# Ober Nomes Colunas: (c.description devolva listagem dos atributos, nome atributo é 1º elemento - x[0])\n# No Modulo Intermédio devem discutir mais este tipo de metodo para criar Listagem\nnames = [x[0] for x in c.description]\n#print(names)\n\n# Input tabela dentro DataFrame, atribuir nomes colunas\ndf_mn_c2021 = pd.DataFrame(c.fetchall(), columns = names)\n\n# Dados por NUTS3:\n\n# SQL String\nmy_sql = \"\"\"\nselect *\nfrom V_BGRI2021_N3_PT \n\"\"\"\n\n# Executar o cursor c com a string como parâmetro\nc.execute(my_sql)\n# Criar Nomes colunas\nnames = [ x[0] for x in c.description]\ndf_n3_c2021 = pd.DataFrame(c.fetchall(), columns = names)\n\nler a partir de .xlsx\n\n# Solução de recurso caso existem problemas da BD \nimport pandas as pd\ndf_n3_c2021 = pd.read_excel(r'data\\N3_C2021.xlsx')\ndf_mn_c2021 = pd.read_excel(r'data\\DTMN_C2021.xlsx')\n# Mostrar informação inicial:\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n\n\n# Criar DF a partir de Ficheiros EXCEL\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\DTMN_C2021.xlsx\"\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\N3_C2021.xlsx\"\n# Import Packages openxyl para poder trabalhar com XLSX\nimport pandas as pd\nimport numpy as np\n\n# Link Direto para ficheiro XLSX\nficheiro_dtmn = r'data\\DTMN_C2021.xlsx'\nficheiro_n3 = r'data\\N3_C2021.xlsx'\n# Importar num DataFrame\ndf_n3_c2021 = pd.read_excel(ficheiro_n3)\nficheiro_dtmn = pd.read_excel(ficheiro_dtmn) \n# Argumentos uteis: skiprows=[0,1], usecols='A', nrows=2, header=None\n\n# Mostrar primeiras 3 linhas e ultimas 3 linhas (head e tail em conjunto)\n# Utilizar iloc e numpy\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n# print(df_mn_c2021.info())\n\n\n\n3.1.1.3 importar dados da DGS\nImportar DGS (Certificados de Obitos por Dia)\n\n# Ver referencia: https://dados.gov.pt/pt/datasets/registos-de-certificados-de-obito/\n# Atributos: Ano, Mês, Dia do Mês, Dia da Semana, Nº Certificados de Óbito Diários\n\nimport pandas as pd\n\n# Link DGS (Certificados de Obitos por Dia)  \nficheiro = r'http://dados.gov.pt/pt/datasets/r/dde8a843-d6a8-4a3f-82ff-b0e9c6743a3a'\n# Ler ficheiro do computador:\n#ficheiro = r'data\\evolucao-diaria-de-certificados-de-obito.csv'\n\n# Importar em DataFrame\ndf_obitos = pd.read_csv(ficheiro, sep=';')\n\n# Mostrar informação df\n# print(df_obitos.head(5))\n# print(df_obitos.info())\n# print(df_obitos.describe())"
  },
  {
    "objectID": "300-mod3.html#gráficos-em-matplotlib-e-seaborn",
    "href": "300-mod3.html#gráficos-em-matplotlib-e-seaborn",
    "title": "3  Jupyter e Visualizações (Basics)",
    "section": "3.2 Gráficos em MATPLOTLIB e SEABORN",
    "text": "3.2 Gráficos em MATPLOTLIB e SEABORN\nobter dados\n\n# Solução de recurso caso existem problemas da BD \nimport pandas as pd\n\ndf_n3_c2021 = pd.read_excel(r'data\\N3_C2021.xlsx')\ndf_mn_c2021 = pd.read_excel(r'data\\DTMN_C2021.xlsx')\n# Mostrar informação inicial:\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n\n\n# Criar DF a partir de Ficheiros EXCEL\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\DTMN_C2021.xlsx\"\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\N3_C2021.xlsx\"\n# Import Packages openxyl para poder trabalhar com XLSX\nimport pandas as pd\nimport numpy as np\n\n# Link Direto para ficheiro XLSX\nficheiro_dtmn = r'data\\DTMN_C2021.xlsx'\nficheiro_n3 = r'data\\N3_C2021.xlsx'\n# Importar num DataFrame\ndf_n3_c2021 = pd.read_excel(ficheiro_n3)\nficheiro_dtmn = pd.read_excel(ficheiro_dtmn) \n# Argumentos uteis: skiprows=[0,1], usecols='A', nrows=2, header=None\n\n# Mostrar primeiras 3 linhas e ultimas 3 linhas (head e tail em conjunto)\n# Utilizar iloc e numpy\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n# print(df_mn_c2021.info())\n\n\n# Importar DGS (Certificados de Obitos por Dia)\n# Ver referencia: https://dados.gov.pt/pt/datasets/registos-de-certificados-de-obito/\n# Atributos: Ano, Mês, Dia do Mês, Dia da Semana, Nº Certificados de Óbito Diários\n\nimport pandas as pd\n\n# Link DGS (Certificados de Obitos por Dia)  \n#ficheiro = r'http://dados.gov.pt/pt/datasets/r/dde8a843-d6a8-4a3f-82ff-b0e9c6743a3a'\n# Ler ficheiro do computador:\nficheiro = r'data\\evolucao-diaria-de-certificados-de-obito.csv'\n\n# Importar em DataFrame\ndf_obitos = pd.read_csv(ficheiro, sep=';')\n\n# Mostrar informação df\n# print(df_obitos.head(5))\n# print(df_obitos.info())\n# print(df_obitos.describe())\n\n\n\n3.2.1 Gráfico de Dispersão (Scatterplot) em MATPLOTLIB\n\n# Importar pyplot do matplotlib \nfrom matplotlib import pyplot as plt\n\n\n# Scatterplot que conssiste de 2 plots\n# 1º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade superior a 65 \n# 2º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade inferior a 14  \n\n\n# Marker: circle ('o'), point ('.'), diamond('d'), or square ('s')\n# s - tamamnho\n# label - rótulo a colocar\n# alpha - transparência\n# Plot 1\nplt.scatter(df_mn_c2021.N_INDIVIDUOS, df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS,\n                color = 'Red', \n                marker='.', \n                s=12,\n                label = '65 ou mais', \n                alpha = 0.5)\n# Plot 2\nplt.scatter(df_mn_c2021.N_INDIVIDUOS, df_mn_c2021.N_INDIVIDUOS_0_14,\n                color = 'Green', \n                marker='s', \n                s=12,\n                label = '15 ou menos', \n                alpha = 0.5) \n# Definir Labels x e y áxis\nplt.xlabel('Nº Individuos')\nplt.ylabel('Nº Individuos por grupo')\n\n# Adicionar GRID\nplt.grid(True)\n\n# Definir Título\nplt.title('Relação Individuos por grupo idade ao total \\n por municipio',size=12,fontweight=\"bold\")\n\n# Mostrar Legenda\nplt.legend()\nplt.show()\n\n\n\n\n\n\n3.2.2 Gráfico Relação Grupos Etários com Tamanho Municipio (valores relativos)\n\n# É possivel dividir as variáveis!\n# Permita tirar conclusões dos dados\nplt.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS,\n               color = 'Red', s=12, marker='.', label = '65 ou mais', alpha = 0.5)\n# 2º plot com \nplt.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_0_14/df_mn_c2021.N_INDIVIDUOS,\n               color = 'Green', s=12, marker='s', label = '15 ou menos', alpha = 0.5) # \n#'''\n# Labels x e y áxis\nplt.xlabel('Nº Individuos (por 1000)')\nplt.ylabel('Rácio nº Individuos')\n# Adicionar GRID\nplt.grid(True)\nplt.title('Relação Individuos por grupo idade ao total \\n por municipio',size=12,fontweight=\"bold\")\n# Mostrar Legenda\nplt.legend()\nplt.style.use('fivethirtyeight')\n#plt.title\nplt.show()\n\n\n\n\nCriar a mesma Figura utilizando os objectos fig e ax\n\n# Import matplotlib\nfrom matplotlib import pyplot as plt\nfig,ax = plt.subplots()\n\n# Scatterplot que conssite de 2 plots\n# 1º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade superior a 65 \n# 2º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade inferior a 14  \n\nax.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS,\n            color = 'Red', \n           s=12, \n           marker='.', \n           label = '65 ou mais', \n           alpha = 0.5)\n# 2º plot com \nax.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_0_14/df_mn_c2021.N_INDIVIDUOS,\n               color = 'Green', \n               s=12, \n               marker='s', \n               label = '15 ou menos', \n               alpha = 0.5) # \n\n\n\n# Definir Labels x e y áxis\nax.set_xlabel('Nº Individuos')\nax.set_ylabel('Nº Individuos por grupo')\n\n# Adicionar GRID\nax.grid(True)\n\n# Definir Título\nax.set_title('Relação Individuos por grupo idade ao total \\n por municipio',size=12,fontweight=\"bold\")\n\n\n# Mostrar Legenda\nplt.legend()\nplt.show()\n\n\n\n\n\n\n3.2.3 ScatterPlot em SEABORN\n\n# Grafico Scatterplot SEABORN - Inicial\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Criar Gráfico\nsns.scatterplot(x=df_mn_c2021.N_INDIVIDUOS, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS, \n                data = df_mn_c2021\n               )\nplt.show()\n\n\n\n\nSeaborn adicionar tamanho\n\n# Grafico Scatterplot SEABORN - Incluir cor\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.set_palette('Accent')\n\n# Criar Gráfico\n# size - variavel para tamanho de cada ponto\nsns.scatterplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                hue = \"NUTS2\",\n                hue_order = ['11','15','16','17','18','20','30'],\n                # Relação entre edificios até 1980 no total de edificios\n                size = (df_mn_c2021.N_EDIFICIOS_CONSTR_ANTES_1945 + \n                        df_mn_c2021.N_EDIFICIOS_CONSTR_1946_1980) /\n                    df_mn_c2021.N_EDIFICIOS_CLASSICOS\n               )\nplt.show()\n\n\n\n\n\n\n3.2.4 plotnine\n\nfrom plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap\n\n(\n    ggplot(df_mn_c2021, aes(x=df_mn_c2021.N_INDIVIDUOS, \n    y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS))\n    + geom_point()\n    + stat_smooth(method=\"lm\")\n)\n\n\n\n\n\n\n3.2.5 SEABORN RELPLOT\nSEABORN REPLOT Inicial\n\nsns.set_palette('Accent')\n\n# Criar Gráfico\nsns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                kind = \"scatter\",\n                col = \"NUTS2\",\n                col_order = ['11','15','16','17','18','20','30'],\n                # Numero colunas\n                col_wrap = 3,\n                size = (df_mn_c2021.N_EDIFICIOS_CONSTR_ANTES_1945 + \n                        df_mn_c2021.N_EDIFICIOS_CONSTR_1946_1980) /\n                    df_mn_c2021.N_EDIFICIOS_CLASSICOS,\n                # Tamanho minimo e máximo\n                sizes=(10, 150),\n                alpha = 0.7\n               )\nplt.show()\n\n\n\n\n\n# Outro Exemplo com Municipios com menos de 50000 e mais que 50000\nsns.set_style('whitegrid')\n\n# Criar Nova Coluna para indicar tamanho do municipio (2 classes)\ndf_mn_c2021[\"Tipo Municipio\"] = np.where(df_mn_c2021.N_INDIVIDUOS&lt;50000, 'Menos 50.000', 'Mais 50.000')\n\n# Mostra população com mais de 65 anos por tipo de municipio\nsns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS,\n                data = df_mn_c2021,\n           kind=\"scatter\",\n           col=df_mn_c2021[\"Tipo Municipio\"]           \n           )\nplt.show()\n\n\n\n\nMesmo Figura Utilizando MATPLOTLIB\n\n# Create subplots\nfig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n\n# Mudar o array de 2 dimensõe para listagem, facilitando a definição do subplot\naxes = axes.flatten()  \n\n# Define NUTS2 order\nnut2_order = ['11', '15', '16', '17', '18', '20', '30']\n\n\n# Percorrer cada NUTS2 e criar um SUBPLOT\nfor i, nut2 in enumerate(nut2_order[:7]):\n    \n    # Definir ax para subplot (0...n)\n    ax = axes[i]\n    # Seleção dos dados NUTS2\n    df_nut2 = df_mn_c2021[df_mn_c2021['NUTS2'] == nut2]\n    # Criar SUBPLOT\n    ax.scatter(\n        df_nut2['N_INDIVIDUOS'] / 1000,\n        df_nut2['N_INDIVIDUOS_65_OU_MAIS'] / df_nut2['N_INDIVIDUOS'],\n        s=(df_nut2['N_EDIFICIOS_CONSTR_ANTES_1945'] +\n           df_nut2['N_EDIFICIOS_CONSTR_1946_1980']) / df_nut2['N_EDIFICIOS_CLASSICOS'] * 1000,  \n        alpha=0.7,\n        label=nut2\n    )\n    ax.set_xlabel('N_INDIVIDUOS (thousands)')\n    ax.set_ylabel('N_INDIVIDUOS_65_OU_MAIS / N_INDIVIDUOS')\n    ax.set_title(f'NUTS2: {nut2}')\n    ax.legend()\n\nplt.tight_layout(pad=1.5)\nplt.show()\n\n\n\n\n\n\n3.2.6 SeaBorn CATPLOTs\nGráfico de contagens (CountPlot)\n\n# Criar COUNTPLOT - Contagens de registos por ano\nsns.set_palette('Accent')\n\n# Ordenar no Notebook de demonstração\ncategory_order = sorted(df_obitos['Ano'].unique(),reverse=True)\n\n# Criar Gráfico\nsns.catplot(x=\"Ano\", # x ou y\n            data = df_obitos,\n            kind = \"count\",\n            order = category_order\n            )\nplt.show()\n\n\n\n\nGráfico de barras (Bar plot)\n\n# Criar Barplot - Contagens de nº de óbitos diarios por ano\n\nsns.set_palette('Accent')\n\n# Gráfico com nº de óbitos diários por ano\nsns.catplot(x=\"Ano\",\n            y = 'Nº Certificados de Óbito Diários',\n            data = df_obitos,\n            kind = \"bar\",\n            # Error bar mostra Interval de confiança (ci) \n            errorbar = 'ci' # None\\'ci\\sd\\se\\pi\\metodo definido'\n            )\nplt.show()\n\n\n\n\nBox plot\n\n# Criar BOXPLOT - Distribuição dos ceritficados Óbito Diários\n\nsns.set_palette('tab10')\n# Mudar o aspecto do output\n#sns.set_context('notebook')\n\n# Gráfico mostra a distribuição de nº de óbitos diários\nsns.catplot(x=\"Dia da Semana\", # Dia da Semana\n            y = 'Nº Certificados de Óbito Diários',\n            data = df_obitos,\n            kind = \"box\",\n            whis = [5,95] # 2.0\n            # sym = '' # - controlar visualização outliers\n            )\nplt.show()\n\n\n\n\n\n\n3.2.7 Criar plots com ou sem RELPLOT\n\n# Mostrar Percentagem pop65 no total\n\nsns.set_palette('Accent')\n\n# Nova Coluna com Ano e Mes\n# Criar Nova Coluna para indicar tamanho do municipio\n# Converter para String e Juntar\ndf_obitos[\"ANO_MES\"] = df_obitos['Ano'].astype(str) + df_obitos['Mês'].astype(str)\n\n# Criar Gráfico nº de óbitos por ano e mes\ng1 = sns.relplot(x='ANO_MES',\n                y= 'Nº Certificados de Óbito Diários', \n                data = df_obitos,\n                kind = 'line',\n                color = 'green',\n                errorbar = 'ci' # Mudar de ci para sd\n                )\n\nplt.show()\n\n# Grafico com Lineplot com seleção dos anos 2015 até 2016\n\ndf_obitos2015_2016 = df_obitos[df_obitos['Ano'].isin([2015, 2016, 2017])]\ng2 = sns.lineplot(x='ANO_MES',\n                  y= 'Nº Certificados de Óbito Diários', \n                  data = df_obitos2015_2016,\n                  color = 'red',\n                  errorbar = 'sd' # Mudar de ci para sd\n                  )\n\n# Get the x-axis tick positions and labels\nxtick_positions = g2.get_xticks()\nxtick_labels = df_obitos2015_2016['ANO_MES'].iloc[xtick_positions].tolist()\n\n# Set x-axis tick labels to show only every 12th label\nfiltered_xtick_labels = [label if i % 12 == 0 else '' for i, label in enumerate(xtick_labels)]\ng2.set_xticks(xtick_positions)\ng2.set_xticklabels(filtered_xtick_labels, rotation=45)  # You can adjust rotation as needed\n    \ng1    \nplt.show()\n\n\n\n\n\n\n\n\n\n3.2.8 Histograma (Histogram)\n\n# histograma que mostra a distribuição do rácio de população com mais de 65 anos\n\nsns.set_palette('Accent')\n\n# Alternativa Order List\nhue_order_list = sorted(df_mn_c2021['NUTS2'].unique())\n\n# Criar Gráfico com sns.histplot\nsns.histplot(x=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n            data = df_mn_c2021,\n             bins = 30,\n             #binwidth = 0.02, - Alternativa definir tamanho de cada bin\n            hue = \"NUTS2\",\n            hue_order = ['11','15','16','17','18','20','30']\n               )\nplt.show()\n\n\n\n\n\n\n3.2.9 Customização dos gráficos em SEABORN\n\n# Exemplo diferenças na visualização com estes 3 paramentros\n\n# Categorical Color Brewer palettes: 'tab10' (default matplotlib palette), 'Dark2', 'Pastel1', 'Set2', 'Paired'\n# Seaborn has six variations of matplotlib’s palette, called: deep, muted, pastel, bright, dark, and colorblind\nsns.set_palette('colorblind')\n# 'paper',  'notebook', 'talk', 'poster'\nsns.set_context('paper')\n# Atenção - estilo continua valido para o resto do codigo\n# darkgrid, whitegrid, dark, white, ticks\nsns.set_style('darkgrid')\n# Mudar o aspecto do output\n\n\n# Gráfico mostra a distribuição de nº de óbitos diários\nsns.catplot(x=\"Dia da Semana\", # Dia da Semana\n            y = 'Nº Certificados de Óbito Diários',\n            data = df_obitos,\n            kind = \"box\",\n            whis = [5,95]# 2.0\n            #sym = '' # - controlar mostrar outliers\n            )\nplt.show()\n\n\n\n\n\n# Demonstração diferentes STYLES\n\n# darkgrid, whitegrid, dark, white, ticks\nfor estilo in ['darkgrid', 'whitegrid', 'dark', 'white', 'ticks']:\n    sns.set_style(estilo)\n    sns.set_palette('Accent')\n\n    # Criar Gráfico\n    sns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                    y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                    data = df_mn_c2021,\n                    kind = \"scatter\",\n                    hue = 'NUTS2')\n    print(estilo.upper())\n    plt.show()\n\n#plt.show()\n\nDARKGRID\n\n\n\n\n\nWHITEGRID\n\n\n\n\n\nDARK\n\n\n\n\n\nWHITE\n\n\n\n\n\nTICKS\n\n\n\n\n\ntipos de plots em Seaborn\n\n# Diferentes tipos de plots em Seaborn\n# \ng1 = sns.scatterplot(x=df_mn_c2021.N_INDIVIDUOS,y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS, data = df_mn_c2021)\ng2 = sns.relplot(x=df_mn_c2021.N_INDIVIDUOS,y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS, data = df_mn_c2021,kind='scatter')\nprint(type(g1))\nprint(type(g2))\n\n&lt;class 'matplotlib.axes._axes.Axes'&gt;\n&lt;class 'seaborn.axisgrid.FacetGrid'&gt;\n\n\n\n\n\n\n\n\nAdicionar Titulos\n\n# Definições globais:\nsns.set_style('ticks')\nsns.set_palette('colorblind')\nsns.set_context('notebook')\n\n# # Criar Gráfico\ng = sns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                kind = \"scatter\",\n                hue = 'NUTS2',\n               col='NUTS1')\n\n# Definir Titulo e font do titulo\ng.fig.suptitle('Relacao população 65+ no total de população',\n               y = 1.05,\n              fontdict={'size': 20, 'color': 'black','name': 'Arial'})\n\ng.set_titles(\"NUTS1 {col_name}\")\n\nplt.show()\n\n\n\n\nMudar Rotulos dos Eixos\n\ng = sns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                kind = \"scatter\",\n                hue = 'NUTS2',\n               col='NUTS1')\n\ng.fig.suptitle('Relacao população 65+ no total de população',\n               y = 1.05,\n              fontdict={'color': 'black','name': 'Arial'})\n\ng.set_titles(\"NUTS1 {col_name}\")\n\n# Rotulos Eixos\n# .set - permite definir atributos para cada eixo de um FacetGrid\ng.set(xlabel=\"População por municipio (em 1000)\",\n     ylabel=\"Rácio nº Individuos superior a 65 anos\")\n\nplt.show()\n\n'''\n# Gravar Ficheiro:\noutputfile = r'c:\\temp\\omeuplot.png'\ng.savefig(outputfile, format='png')\n\n'''\n\n&lt;&gt;:21: SyntaxWarning: invalid escape sequence '\\o'\n&lt;&gt;:21: SyntaxWarning: invalid escape sequence '\\o'\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8728\\235803542.py:21: SyntaxWarning: invalid escape sequence '\\o'\n\n\n\n\n\n\"\\n# Gravar Ficheiro:\\noutputfile = r'c:\\temp\\\\omeuplot.png'\\ng.savefig(outputfile, format='png')\\n\\n\""
  },
  {
    "objectID": "300-mod3.html#exercícios",
    "href": "300-mod3.html#exercícios",
    "title": "3  Jupyter e Visualizações (Basics)",
    "section": "3.3 Exercícios",
    "text": "3.3 Exercícios\nLer dados\n\nimport pandas as pd\nimport requests \n# Link direto para ficheiro CSV: Despesa com Medicamentos nos Hospitais do SNS\nficheiro = r'data\\utentes-atendidos-nos-centros-de-saude-no-ambito-da-soep.csv'\n\n# Importar num Pandas Dataframe\ndf = pd.read_csv(ficheiro, sep=';')\n\ndf.head\ndf.info\ndf.describe\n\n&lt;bound method NDFrame.describe of        Período      ARS                                    ACES  \\\n0      2019-07    Norte                            ULS Nordeste   \n1      2019-07    Norte  Trás-os-Montes - Alto Tâmega e Barroso   \n2      2019-07    Norte           Douro I - Marão e Douro Norte   \n3      2019-07    Norte           Douro I - Marão e Douro Norte   \n4      2019-07    Norte                    Douro II - Douro Sul   \n...        ...      ...                                     ...   \n27347  2023-09  Algarve                     Algarve I - Central   \n27348  2023-09  Algarve                 Algarve II - Barlavento   \n27349  2023-09  Algarve                 Algarve II - Barlavento   \n27350  2023-09  Algarve                  AlgarveIII - Sotavento   \n27351  2023-09  Algarve                  AlgarveIII - Sotavento   \n\n      Localização Geográfica       Sexo Faixa Etária  Nº Utentes  \\\n0      41.8069684,-6.7587977  Masculino       65 e +          57   \n1       41.741781,-7.4731648   Feminino        50-64          30   \n2      41.2968711,-7.7483727  Masculino       65 e +          54   \n3      41.2968711,-7.7483727   Feminino        35-49          42   \n4      41.0953745,-7.8123805  Masculino          &lt;20           9   \n...                      ...        ...          ...         ...   \n27347  37.0274264,-7.9395984   Feminino        50-64          84   \n27348  37.1387554,-8.5445093  Masculino        20-34          24   \n27349  37.1387554,-8.5445093  Masculino       65 e +          77   \n27350   37.383008,-7.7293275  Masculino          &lt;20          23   \n27351   37.383008,-7.7293275   Feminino        20-34          25   \n\n                                                      ID  \n0                   2019-7/65 e +/Masculino/ULS Nordeste  \n1      2019-7/50-64/Feminino/Trás-os-Montes - Alto Tâ...  \n2      2019-7/65 e +/Masculino/Douro I - Marão e Dour...  \n3      2019-7/35-49/Feminino/Douro I - Marão e Douro ...  \n4              2019-7/&lt;20/Masculino/Douro II - Douro Sul  \n...                                                  ...  \n27347          2023-9/50-64/Feminino/Algarve I - Central  \n27348     2023-9/20-34/Masculino/Algarve II - Barlavento  \n27349    2023-9/65 e +/Masculino/Algarve II - Barlavento  \n27350        2023-9/&lt;20/Masculino/AlgarveIII - Sotavento  \n27351       2023-9/20-34/Feminino/AlgarveIII - Sotavento  \n\n[27352 rows x 8 columns]&gt;\n\n\nBOXPLOT com a distribuíção por ARS dos diferentes nº de utentes por Sexo\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Gráfico mostra a distribuição de nº de óbitos diários\nsns.catplot(x=\"ARS\", # Dia da Semana\n            y = 'Nº Utentes',\n            data = df,\n            kind = \"box\",\n            hue = \"Sexo\",\n            whis = [5,95] # 2.0\n            # sym = '' # - controlar visualização outliers\n            )\nplt.show()\n\n\n\n\nMostrar num LINEPLOT o nº de utentos por Ano por ARS\n\n# Criar novo variável ano no Dataframe:\n# Converter \"Período\" para formato datetime\ndf['Período'] = pd.to_datetime(df['Período'], format='%Y-%m')\n\n# Extrair o ano e criar nova coluna ANO\ndf['ANO'] = df['Período'].dt.year\n\ncol_order= sorted(df['ARS'].unique(),reverse=False)\n\nsns.relplot(x=df.ANO, \n                y=df['Nº Utentes'], \n                data = df,\n                kind = \"line\",\n                col = \"ARS\",\n                color='red',\n                col_wrap = 3, \n                errorbar = 'ci') \nplt.show()\n\n\n\n\nDesafio utilizar o package CUTECHARTS para nº de observações por ARS\n\nimport pandas as pd\nimport cutecharts.charts as ctc\n\ndf2 = df['ARS'].value_counts().to_frame(name=\"count\")\n\n# pie chart \npie = ctc.Pie('ARS', # title\n              width='720px',height='720px')\n\n# set the chart options\npie.set_options(labels=list(df2.index), # ARS names as labels\n                inner_radius=0)\n\n# label to be shown on graph\npie.add_series(list(df2['count'])) \n\n# display the charts\npie.render_notebook()"
  },
  {
    "objectID": "400-mod4.html#conda",
    "href": "400-mod4.html#conda",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.1 CONDA",
    "text": "4.1 CONDA\nMotor para gestão de packages, dependencias e enviroments para qualquer linguagem.\n\nabrir o terminal: Anaconda prompt\n\nC:\\users\\bruno.lima&gt;conda -V\nC:\\users\\bruno.lima&gt;conda info\nC:\\users\\bruno.lima&gt;conda update -n base -c defaults conda (atualiza conda no env base)"
  },
  {
    "objectID": "400-mod4.html#ambientes-virtuais",
    "href": "400-mod4.html#ambientes-virtuais",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.2 Ambientes virtuais",
    "text": "4.2 Ambientes virtuais\nEspaços isolados e independentes que têm código e as dependências de um projecto.\nFerramentas de criação de ambientes virtuais:\nvenv, virtualenv, pipenv, conda, poetry\n\n4.2.1 Exercícios\ncriar ambiente virtual ‘exercicio_env’:\n&gt; conda create --name exercicio_env\ncriar ambiente dentro duma subdirectoria:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda create --prefix ./envs\nlistar os ambientes existentes:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda env list\napagar o ambiente ‘exercicio_env’:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda remove -n exercicio_env --all\ninstalar o pip:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda install pip\nexportar o ambiente activo para um ficheiro enviroment.ymal:\n(base) C:\\Users\\bruno.lima\\Documents&gt;conda env export &gt; enviroment.yml\nexportar o ambiente activo sem as dependencias para um ficheiro from_history.yml:\n(base) C:\\Users\\bruno.lima\\Documents&gt;conda env export --from-history &gt; from_history.yml\ncriar um spec-file.txt com os detalhes do ambiente activo:\n(base) C:\\Users\\bruno.lima\\Documents&gt;conda list --explicit &gt; spec-file.txt\nQuando iniciamos um novo projecto é recomendável iniciar um novo ambiente:\n(base) C:\\Users\\bruno.lima\\Documents\\projecto_novo&gt;conda create -p ./conda numpy\npara sair de um ambiente:\n(teste) C:\\Users\\bruno.lima\\Documents\\projecto_novo&gt;conda deactivate"
  },
  {
    "objectID": "400-mod4.html#instalar-packages",
    "href": "400-mod4.html#instalar-packages",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.3 Instalar packages",
    "text": "4.3 Instalar packages\n\n\n4.3.1 Exercícios\ncriar ambiente e activá-lo na nossa pasta de projecto:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda create --prefix ./envs\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda activate ./envs\ninstalar package pip:\n(C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj\\envs) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda install pip\nprocurar versoes numpy entre 1.19 e 1.23:\n(C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj\\envs) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda search \"numpy &gt;=1.19, &lt;=1.23\"\ninstalar package python-twitter-v2:\n(C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj\\envs) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;pip install python-twitter-v2"
  },
  {
    "objectID": "400-mod4.html#ides",
    "href": "400-mod4.html#ides",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.4 IDEs",
    "text": "4.4 IDEs\n\n4.4.1 Jupyter notebook\npode usar-se a partir do ANACONDA\nCom o miniconda temos de activar um ambiente primeiro e depois instalar:\nconda install ipykernel conda install jupyter\n\n\n4.4.2 VsCode\nInstalar extensões:\n\nPython\nJupyter\n\nEm settings (Ctrl + ,), pesquisar CONDA e definir o path: C:\\ProgramData\\anaconda3\\Scripts\\connda.exe\npesquisar format e activar formatação automáticaao gravar\nactivar verificação de tipos typecheck -&gt; escolher basic\npodemos também defenir terminal integrated default como Command Prompt\nsettings -&gt; themes -&gt; color theme (Ctrl K Ctrl T)\nmais atalhos:\n\n\n\n4.4.3 Cloud\nGoogle Colab"
  },
  {
    "objectID": "400-mod4.html#classes-e-módulos",
    "href": "400-mod4.html#classes-e-módulos",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.5 Classes e Módulos",
    "text": "4.5 Classes e Módulos\n\n\n4.5.1 Módulos\n\nfrom modules import conversor\n\nconversor.metros_em_milhas(100)\n\n0.06213727366498068"
  },
  {
    "objectID": "400-mod4.html#programação-orientada-a-objectos-oop",
    "href": "400-mod4.html#programação-orientada-a-objectos-oop",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.6 Programação Orientada a Objectos (OOP)",
    "text": "4.6 Programação Orientada a Objectos (OOP)\n\n4.6.1 Classes\nclasses simples\n\nclass Gato:\n    \"\"\"Modelo de um gato\"\"\"\n\n    def mia(self):\n        print(\"Miau...\")\n\n\ntom = Gato()\n\ntom.mia()\n\ntom.__doc__\n\nMiau...\n\n\n'Modelo de um gato'\n\n\n\n4.6.1.1 Instanciação\n\nclass Pessoa:\n\n    # método constutor\n    # executado sempre que se cria um obejcto do tipo Pessoa\n    def __init__(self, nome_da_pessoa):\n        self.nome = nome_da_pessoa  # cada instância tem a seu atributo (property)\n\n\nesta_pessoa = Pessoa(\"Fernando\")\n\nesta_pessoa.nome\n\n'Fernando'\n\n\napagar instancia\n\npessoa_errada = Pessoa(\"Errada\")\nprint(pessoa_errada.nome)\nprint(pessoa_errada)\n\ndel pessoa_errada\n#print(pessoa_errada.nome)\n#print(pessoa_errada)\n\nErrada\n&lt;__main__.Pessoa object at 0x000001B770805430&gt;\n\n\natributos de classe\n\nclass Funcionario:\n\n    funcionarios_count =0\n\n    def __init__(self, numero_de_funcionario):\n        self.numero_de_funcionario = numero_de_funcionario\n        type(self).funcionarios_count += 1\n\n\nfor num_func in range(1, 10):\n   Funcionario(num_func)\n\nprint(f\"Foram criados {Funcionario.funcionarios_count} funcionários.\")\n\nForam criados 9 funcionários.\n\n\n\n\n4.6.1.2 Atributos Property (Getter e Setter)\n\nclass PessoaReal:\n    def __init__(self, nome_de_entrada):\n        self.nome = nome_de_entrada\n\n    @property\n    def nome(self):\n        return f\"Sua alteza {self._nome_privado}\"\n\n    @nome.setter\n    def nome(self, nome_de_entrada2):\n        if (len(nome_de_entrada2) == 0):\n            raise ValueError(\"O nome não pode ser vazio\")\n        self._nome_privado = nome_de_entrada2\n\n\numa_pessoa = PessoaReal(\"Artur\")\n\nprint(uma_pessoa.nome)\n\numa_pessoa.nome = \"Clara\"\nprint(uma_pessoa.nome)\n\nSua alteza Artur\nSua alteza Clara\n\n\n\n\n\n4.6.2 exercícios\nDefine uma class carro com os atributos: - marca - deposito - nivel\ne com os métodos: - andar (Km) - abastecer (litros)\n\nclass Carro:\n    def __init__(self, marca, deposito, nivel):\n        self.marca = marca\n        self.deposito = deposito\n        self.nivel = nivel\n\n    def andar(self, km):\n        self.nivel -= km * 7 / 100\n        if self.nivel &lt; 0:\n            self.nivel = 0\n        print(f'Andou {km} km')\n        \n        \n    # recebe o numero de litros a abastecer; so pode abastecer ate ao maximo do deposito\n    def abastecer(self, litros):\n        self.nivel += litros\n        if self.nivel &gt; self.deposito:\n            self.nivel = self.deposito\n        print(f'Abasteceu {self.nivel} litros')\n\n    def __str__(self):\n        return f'{self.marca} - {self.nivel:.2f}'\n\n\ncarro1 = Carro('Fiat', 50, 20)\n\ncarro2 = Carro('Renault', 60, 30)\n\nprint(carro1)\n\ncarro1.abastecer(100)\ncarro1.andar(10)\ncarro1.nivel\n\nFiat - 20.00\nAbasteceu 50 litros\nAndou 10 km\n\n\n49.3"
  },
  {
    "objectID": "400-mod4.html#listas-bidimensionais",
    "href": "400-mod4.html#listas-bidimensionais",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.7 Listas bidimensionais",
    "text": "4.7 Listas bidimensionais\ndefinição de uma lista bidimensional\n\nlista_bi = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\nlista_bi[1][2]\n\n6\n\n\ndois loops percorrendo a lista bidimensional usando iteradores\n\nfor linha in lista_bi:\n    for coluna in linha:\n        print(coluna, end = '')\n\n123456789\n\n\nAdicionar ou remover\n\nlista = [[1,2,3],[4,5,6]]\nlista.append([7,8,9])\nlista\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\ndel lista[1]\nlista\n\n[[1, 2, 3], [7, 8, 9]]\n\n\n\nlista = [[1,2,3],[4,5,6]]\nlista.extend([['a','b']])\nlista\n\n[[1, 2, 3], [4, 5, 6], ['a', 'b']]\n\n\n\nlista[:2]\n\n[[1, 2, 3], [4, 5, 6]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ndel lista[1][2]\nlista\n\n[[1, 2, 3], [4, 5], [7, 8, 9]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 66,6,6], [7, 8, 9]]\nlista[1].remove(6)\nlista\n\n[[1, 2, 3], [4, 5, 66, 6], [7, 8, 9]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nlista[1].pop(2)\nlista\n\n[[1, 2, 3], [4, 5], [7, 8, 9]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nlista.insert(1,['a','b','c'])\nlista\n\n[[1, 2, 3], ['a', 'b', 'c'], [4, 5, 6], [7, 8, 9]]\n\n\n\n4.7.1 exercicio\n\n# criar uma lista bidimensional com colunas A a J e linhas 1 a 10\nlista_bi = [[f'{chr(65 + col)}{linha}' for col in range(10)] for linha in range(1, 11)]\n\nlista_bi\n\n[['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1', 'I1', 'J1'],\n ['A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2', 'I2', 'J2'],\n ['A3', 'B3', 'C3', 'D3', 'E3', 'F3', 'G3', 'H3', 'I3', 'J3'],\n ['A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4', 'I4', 'J4'],\n ['A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5', 'H5', 'I5', 'J5'],\n ['A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6', 'I6', 'J6'],\n ['A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7', 'I7', 'J7'],\n ['A8', 'B8', 'C8', 'D8', 'E8', 'F8', 'G8', 'H8', 'I8', 'J8'],\n ['A9', 'B9', 'C9', 'D9', 'E9', 'F9', 'G9', 'H9', 'I9', 'J9'],\n ['A10', 'B10', 'C10', 'D10', 'E10', 'F10', 'G10', 'H10', 'I10', 'J10']]\n\n\n\n# solução alternativa\nlista_bi2 = [[0 for coluna in range(10)] for linha in range(10)]\n\nfor linha in range(10):\n    for coluna in range(10):\n        lista_bi2[linha][coluna] = f'{chr(65 + coluna)}{linha + 1}'\n        \nlista_bi2\n\n[['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1', 'I1', 'J1'],\n ['A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2', 'I2', 'J2'],\n ['A3', 'B3', 'C3', 'D3', 'E3', 'F3', 'G3', 'H3', 'I3', 'J3'],\n ['A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4', 'I4', 'J4'],\n ['A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5', 'H5', 'I5', 'J5'],\n ['A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6', 'I6', 'J6'],\n ['A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7', 'I7', 'J7'],\n ['A8', 'B8', 'C8', 'D8', 'E8', 'F8', 'G8', 'H8', 'I8', 'J8'],\n ['A9', 'B9', 'C9', 'D9', 'E9', 'F9', 'G9', 'H9', 'I9', 'J9'],\n ['A10', 'B10', 'C10', 'D10', 'E10', 'F10', 'G10', 'H10', 'I10', 'J10']]"
  },
  {
    "objectID": "400-mod4.html#list-comprehension",
    "href": "400-mod4.html#list-comprehension",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.8 List Comprehension",
    "text": "4.8 List Comprehension\n\n\nnum = [1, 2, 3, 4, 5]\n\ndobro = [i*2 for i in num]\n\nprint(\"\"\"- a expressão a aplicar é: i*2\n- a sequência de entrada é a lista num:\"\"\",num,\"\"\"\n- a variável que representa o elemento é: i\n- o resultado é:\"\"\",\n      dobro, sep='\\n')\n\n- a expressão a aplicar é: i*2\n- a sequência de entrada é a lista num:\n[1, 2, 3, 4, 5]\n\n- a variável que representa o elemento é: i\n- o resultado é:\n[2, 4, 6, 8, 10]\n\n\n\nlista  =  [x ** 2  for x in range (1, 11)   if  x % 2 == 1]\n\nprint(\"\"\"- a expressão a aplicar é: x ** 2\n- a sequência de entrada é: range (1, 11)\n- a variável é: x\n- a condição predicado é: if x % 2 == 1\"\"\",\n      lista, sep='\\n\\n')\n\n- a expressão a aplicar é: x ** 2\n- a sequência de entrada é: range (1, 11)\n- a variável é: x\n- a condição predicado é: if x % 2 == 1\n\n[1, 9, 25, 49, 81]\n\n\n\n# outra forma de fazer uma lista bidimensional\nlista_bi1 = []\nfor valor_linha in range(10, 40, 10):\n  linha = []\n  for valor_col in range(1,5):\n    linha.append(valor_linha + valor_col)\n  lista_bi1.append(linha)\nlista_bi1\n\n[[11, 12, 13, 14], [21, 22, 23, 24], [31, 32, 33, 34]]\n\n\n\nlista_bi2 = [[linha+col for col in range(1,5)] for linha in range(10, 40, 10)]\nlista_bi2\n\n[[11, 12, 13, 14], [21, 22, 23, 24], [31, 32, 33, 34]]\n\n\n\nlista_bi3 = [[ (linha+1)*10 + (col+1) for col in range(4)] for linha in range(3)]\nlista_bi3\n\n[[11, 12, 13, 14], [21, 22, 23, 24], [31, 32, 33, 34]]\n\n\n\n# matriz toda a zeros com n linhas e m colunas\nn = 3\nm = 4\nmatriz = [[0 for _ in range(m)] for _ in range(n)]\nmatriz\n\n[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n\n\n\n# matriz toda a zeros com n linhas e m colunas\nn = 3\nm = 4\nmatriz = [[0] * m for _ in range(n)]\nmatriz\n\n[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n\n\n\n3*[4*[0]]\n\n[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n\n\n\n# @title exemplo\nnumeros = [\"um\", \"dois\", \"três\", \"quatro\"]\n\niniciais = [numero[0] for numero in numeros]\n\nprint( numeros, iniciais, sep='\\n')\n\n['um', 'dois', 'três', 'quatro']\n['u', 'd', 't', 'q']\n\n\n\n# @title exemplo\n\nnaturais_5 = [1, 2, 3, 4, 5]\n\nimpares_5 = [i for i in naturais_5 if i%2!=0]\n\nprint(naturais_5, impares_5, sep = '\\n')\n\n[1, 2, 3, 4, 5]\n[1, 3, 5]\n\n\n\n# @title exemplo\nprint ([x.lower() for x in [\"A\",\"B\",\"C\"]], sep=\"\\n\")\n\n['a', 'b', 'c']"
  },
  {
    "objectID": "400-mod4.html#dictionary-comprehension",
    "href": "400-mod4.html#dictionary-comprehension",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.9 Dictionary Comprehension",
    "text": "4.9 Dictionary Comprehension\n\n\nextenso = [\"Um\", \"Dois\", \"Três\"]\nromanos = ['I', 'II', 'III']\n\nnumeros = {numero_por_extenso:numero_romano for (numero_por_extenso, numero_romano) in zip(extenso, romanos)}\nprint(numeros)\n\n{'Um': 'I', 'Dois': 'II', 'Três': 'III'}\n\n\n\n# trocando k,v por v,k\nnumeros = {numero_romano:numero_por_extenso for (numero_por_extenso, numero_romano) in zip(extenso, romanos)}\n\nprint(numeros)\n\n{'I': 'Um', 'II': 'Dois', 'III': 'Três'}\n\n\n\nquadrados = {x: x**2 for x in range(1, 6)}\nprint(quadrados)\n\n{1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n\nusando funções na expressão\n\nfrom math import sqrt\n\ndef is_prime(numero):\n    if numero == 1:\n        return \"Não primo\"\n    for i in range(2, int(sqrt(numero))+1):\n        if numero % i == 0:\n            return \"Não primo\"\n    return \"Primo\"\n\nnumeros_candidatos = [21, 43, 53, 87, 99, 101]\n\nmarca_primos = {candidato:is_prime(candidato) for candidato in numeros_candidatos}\nprint(marca_primos)\n\n{21: 'Não primo', 43: 'Primo', 53: 'Primo', 87: 'Não primo', 99: 'Não primo', 101: 'Primo'}\n\n\n\n4.9.1 exercício\n\n# considerando o dicionario\ndicionario = {'hidrogenio': 'H', 'oxigenio': 'O', 'sodio': 's'}\n\n# usa uma compreehnsion list para obter o dicionario invertido\ndicionario_invertido = {v:k for k,v in dicionario.items()}\n\nprint(dicionario_invertido)\n\n{'H': 'hidrogenio', 'O': 'oxigenio', 's': 'sodio'}"
  },
  {
    "objectID": "500-mod5.html#exploração-com-pandas",
    "href": "500-mod5.html#exploração-com-pandas",
    "title": "5  Data Science (Intermediate)",
    "section": "5.1 Exploração com Pandas",
    "text": "5.1 Exploração com Pandas\n\n# usamos por convenção np para Numpy\n# usamos por convenção pd para Pandas\nimport numpy as np\nimport pandas as pd\n\n\n5.1.1 Series\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0])\ndata\n\n0    0.25\n1    0.50\n2    0.75\n3    1.00\ndtype: float64\n\n\n\ndata.values\n\ndata.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\n\ndel data[2]\n\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n\na    0.25\nb    0.50\nc    0.75\nd    1.00\ndtype: float64\n\n\n\npopulation_dict = {'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233,\n                   'Porto': 231834, 'Cascais': 214239, 'Loures': 201349,\n                   'Braga': 193324, 'Almada': 177943}\n                   \npopulation = pd.Series(population_dict)\npopulation\n\nLisboa               544325\nSintra               385989\nVila Nova de Gaia    304233\nPorto                231834\nCascais              214239\nLoures               201349\nBraga                193324\nAlmada               177943\ndtype: int64\n\n\n\npopulation['Braga']\n\npopulation['Braga'] = 201000\n\npopulation['Braga']\n\n201000\n\n\n\n\n5.1.2 DataFrames\n\narea_dict = {'Lisboa': 100.1,'Sintra': 23.8, 'Vila Nova de Gaia': 56.3,\n                   'Porto': 41.4, 'Cascais': 97.1, 'Loures': 11.8,\n                   'Braga': 41, 'Almada': 14.7}\narea = pd.Series(area_dict)\narea\n\nLisboa               100.1\nSintra                23.8\nVila Nova de Gaia     56.3\nPorto                 41.4\nCascais               97.1\nLoures                11.8\nBraga                 41.0\nAlmada                14.7\ndtype: float64\n\n\ncriar uma fataframe a partir de series:\n\ncities = pd.DataFrame({'population': population,\n                       'area': area})\ncities\n\n\n\n\n\n\n\n\npopulation\narea\n\n\n\n\nLisboa\n544325\n100.1\n\n\nSintra\n385989\n23.8\n\n\nVila Nova de Gaia\n304233\n56.3\n\n\nPorto\n231834\n41.4\n\n\nCascais\n214239\n97.1\n\n\nLoures\n201349\n11.8\n\n\nBraga\n201000\n41.0\n\n\nAlmada\n177943\n14.7\n\n\n\n\n\n\n\n\n# reset index com o nome cidade\ncities.reset_index()\n\ncities\n\n\n\n5.1.3 Index\n\nindA = pd.Index([1, 3, 5, 7, 9])\nindA\n\nindB = pd.Index([2, 3, 5, 7, 11])\nindA.intersection(indB)\n\nIndex([3, 5, 7], dtype='int64')\n\n\na intersecção é muito útil para descobrirmos registos com a mesma identificação em vários conjuntos\n\naerod_dict = {'Lisboa': 3, 'Porto': 4, 'Cascais': 1, 'Braga': 7, 'Viseu': 2}\naerod = pd.Series(aerod_dict)\naerod.index\n\naerod.index.intersection(cities.index)\n\nIndex(['Lisboa', 'Porto', 'Cascais', 'Braga'], dtype='object')\n\n\n\n\n5.1.4 Reorganizar as DataFrames\nPara juntar duas dataframes podemos usar os métodos: + Concatenate (pd.concat()) + Append (df.append): As of pandas 2.0, append (previously deprecated) was removed + Merge (pd.merge())\nPor conveniência vamos definir uma função para criar dataframes\n\ndef make_df(cols, ind):\n    \"\"\"Quickly make a DataFrame\"\"\"\n    data = {c: [str(c) + str(i) for i in ind]\n            for c in cols}\n    return pd.DataFrame(data, ind)\n  \n# exemplo de DataFrame\nmake_df('ABC', range(3))\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\nA0\nB0\nC0\n\n\n1\nA1\nB1\nC1\n\n\n2\nA2\nB2\nC2\n\n\n\n\n\n\n\n\n5.1.4.1 Método concatenate\n\ndf1 = make_df('AB', [1, 2])\ndf2 = make_df('AB', np.arange(3,5))\n\ndf1\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n3\nA3\nB3\n\n\n4\nA4\nB4\n\n\n\n\n\n\n\n\npd.concat([df1, df2])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\nA1\nB1\n\n\n2\nA2\nB2\n\n\n3\nA3\nB3\n\n\n4\nA4\nB4\n\n\n\n\n\n\n\n\ndf3 = make_df('AB', range(2))\ndf4 = make_df('CD', range(2))\n\ndf3 \ndf4\n\npd.concat([df3, df4], axis='columns')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nA0\nB0\nC0\nD0\n\n\n1\nA1\nB1\nC1\nD1\n\n\n\n\n\n\n\nDuplicação de indexes\n(Uma diferença importante entre np.concatenate e pd.concat é que a concatenação do Pandas preserva os índices, mesmo que o resultado tenha índices duplicados.)\n\nx = make_df('AB', [0, 1])\ny = make_df('AB', [2, 3])\ny.index = x.index  # fazer o match dos indices\n\nx\ny\n\npd.concat([x, y])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n0\nA2\nB2\n\n\n1\nA3\nB3\n\n\n\n\n\n\n\n\n# Tratar índices repetidos como um erro, fazendo apenas a verificação\ntry:\n    pd.concat([x, y], verify_integrity=True)\nexcept ValueError as e:\n    print(\"ValueError:\", e)\n\nValueError: Indexes have overlapping values: Index([0, 1], dtype='int64')\n\n\n\n# Ignorando o index das dataframes de origem, e refazendo na nova dataframe o index\nx\ny\n\npd.concat([x, y], ignore_index=True)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n2\nA2\nB2\n\n\n3\nA3\nB3\n\n\n\n\n\n\n\n\n# Adicionando chaves MultiIndex para especificar um rótulo para as fontes de dados\nx\ny\n\npd.concat([x, y], keys=['x', 'y'])\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\nx\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\ny\n0\nA2\nB2\n\n\n1\nA3\nB3\n\n\n\n\n\n\n\n\nteste = pd.concat([x, y], keys=['x', 'y'])\n\nteste.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nA\nB\n\n\n\n\n0\nx\n0\nA0\nB0\n\n\n1\nx\n1\nA1\nB1\n\n\n2\ny\n0\nA2\nB2\n\n\n3\ny\n1\nA3\nB3\n\n\n\n\n\n\n\nConcatenação com joins\n\ndf5 = make_df('ABC', [1, 2])\ndf6 = make_df('BCD', [3, 4])\n\ndf5\ndf6\npd.concat([df5, df6])\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n1\nA1\nB1\nC1\nNaN\n\n\n2\nA2\nB2\nC2\nNaN\n\n\n3\nNaN\nB3\nC3\nD3\n\n\n4\nNaN\nB4\nC4\nD4\n\n\n\n\n\n\n\npara juntar fazendo a união das colunas de entrada usamos join=‘outer’, que é o valor por omissão\npara juntar fazendo a interseção das colunas de entrada usamos join=‘inner’\n\npd.concat([df5, df6], join='inner')\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n1\nB1\nC1\n\n\n2\nB2\nC2\n\n\n3\nB3\nC3\n\n\n4\nB4\nC4\n\n\n\n\n\n\n\nse quisermos preservar todas as colunas de uma das dataframes devemos fazer reindex das colunas a preservar na outra dataframe axis=1\n\npd.concat([df6, df5.reindex(df6.columns, axis=1)])\n\n\n\n\n\n\n\n\nB\nC\nD\n\n\n\n\n3\nB3\nC3\nD3\n\n\n4\nB4\nC4\nD4\n\n\n1\nB1\nC1\nNaN\n\n\n2\nB2\nC2\nNaN\n\n\n\n\n\n\n\n\n\n5.1.4.2 Método merge\n\ndf1 = pd.DataFrame({'cidade': [ 'Braga','Lisboa','Sintra', 'Vila Nova de Gaia','Porto', 'Cascais', \n                               'Loures', 'Almada'],\n                    'populacao': [ 193324, 544325, 385989, 304233, 231834, 214239, 201349, 177943]})\ndf2 = pd.DataFrame({'cidade': ['Lisboa','Sintra', 'Vila Nova de Gaia',\n                   'Porto', 'Cascais', 'Loures', 'Braga', 'Almada'],\n                    'area': [ 100.1, 23.8, 56.3, 41.4, 97.1, 11.8, 41,  14.7],\n                   'nuts3': [ '170', '170', '11A', '11A', '170', '170', '112', '170']})\ndf1\ndf2\n\n\n\n\n\n\n\n\ncidade\narea\nnuts3\n\n\n\n\n0\nLisboa\n100.1\n170\n\n\n1\nSintra\n23.8\n170\n\n\n2\nVila Nova de Gaia\n56.3\n11A\n\n\n3\nPorto\n41.4\n11A\n\n\n4\nCascais\n97.1\n170\n\n\n5\nLoures\n11.8\n170\n\n\n6\nBraga\n41.0\n112\n\n\n7\nAlmada\n14.7\n170\n\n\n\n\n\n\n\none-to-one join\n\ndf3 = pd.merge(df1, df2)\n\ndf3\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\n\n\n\n\n0\nBraga\n193324\n41.0\n112\n\n\n1\nLisboa\n544325\n100.1\n170\n\n\n2\nSintra\n385989\n23.8\n170\n\n\n3\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\n4\nPorto\n231834\n41.4\n11A\n\n\n5\nCascais\n214239\n97.1\n170\n\n\n6\nLoures\n201349\n11.8\n170\n\n\n7\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\none-to-many join\n\ndf4 = pd.DataFrame({'nuts3': [ '112', '11A', '170'],\n                   'nuts3_dsg': ['Cávado', 'Área Met. Porto', 'Área Met. Lisboa']})\n                   \ndf5 = pd.merge(df3, df4)\n\ndf4\ndf5\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\n\n\n0\nBraga\n193324\n41.0\n112\nCávado\n\n\n1\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\n\n\n2\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\n\n\n3\nVila Nova de Gaia\n304233\n56.3\n11A\nÁrea Met. Porto\n\n\n4\nPorto\n231834\n41.4\n11A\nÁrea Met. Porto\n\n\n5\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\n\n\n6\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\n\n\n7\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\nmany-to-many join\n\ndf6 = pd.DataFrame({'nuts3': [ '112', '112','11A', '170'],\n                   'class': ['Urbano', 'Rural','Urbano', 'Urbano']})\ndf6\npd.merge(df5, df6)\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\nnuts3_dsg\nclass\n\n\n\n\n0\nBraga\n193324\n41.0\n112\nCávado\nUrbano\n\n\n1\nBraga\n193324\n41.0\n112\nCávado\nRural\n\n\n2\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\nUrbano\n\n\n3\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\nUrbano\n\n\n4\nVila Nova de Gaia\n304233\n56.3\n11A\nÁrea Met. Porto\nUrbano\n\n\n5\nPorto\n231834\n41.4\n11A\nÁrea Met. Porto\nUrbano\n\n\n6\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\nUrbano\n\n\n7\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\nUrbano\n\n\n8\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\nUrbano\n\n\n\n\n\n\n\nmerge key\npodemos indicar a chave para ligar, o primeiro exemplo é equivalente a display(‘df1’, ‘df2’, “pd.merge(df1, df2, on=‘cidade’)”)\nmas nem sempre as colunas por onde queremos fazer o join têm o mesmo nome, nesse caso podemos usar o left_on e o right_on\n\ndf1a = pd.DataFrame({'cidade': ['Lisboa','Sintra', 'Vila Nova de Gaia','Porto', 'Cascais', \n                               'Loures', 'Braga', 'Almada'],\n                    'populacao': [544325, 385989, 304233, 231834, 214239, 201349, 193324, 177943]})\ndf2a = pd.DataFrame({'cidade+100khab': ['Lisboa','Sintra', 'Vila Nova de Gaia',\n                   'Porto', 'Cascais', 'Loures', 'Braga', 'Almada'],\n                    'area': [ 100.1, 23.8, 56.3, 41.4, 97.1, 11.8, 41,  14.7],\n                   'nuts3': [ '170', '170', '11A', '11A', '170', '170', '112', '170']})\n\ndf1a\nprint()\ndf2a\nprint()\npd.merge(df1a, df2a, left_on=\"cidade\", right_on=\"cidade+100khab\")\n\n\n\n\n\n\n\n\n\n\n\n\ncidade\npopulacao\ncidade+100khab\narea\nnuts3\n\n\n\n\n0\nLisboa\n544325\nLisboa\n100.1\n170\n\n\n1\nSintra\n385989\nSintra\n23.8\n170\n\n\n2\nVila Nova de Gaia\n304233\nVila Nova de Gaia\n56.3\n11A\n\n\n3\nPorto\n231834\nPorto\n41.4\n11A\n\n\n4\nCascais\n214239\nCascais\n97.1\n170\n\n\n5\nLoures\n201349\nLoures\n11.8\n170\n\n\n6\nBraga\n193324\nBraga\n41.0\n112\n\n\n7\nAlmada\n177943\nAlmada\n14.7\n170\n\n\n\n\n\n\n\npodemos fazer drop da coluna repetida\n\npd.merge(df1a, df2a, left_on=\"cidade\", right_on=\"cidade+100khab\").drop('cidade+100khab', axis=1)\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\n\n\n\n\n0\nLisboa\n544325\n100.1\n170\n\n\n1\nSintra\n385989\n23.8\n170\n\n\n2\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\n3\nPorto\n231834\n41.4\n11A\n\n\n4\nCascais\n214239\n97.1\n170\n\n\n5\nLoures\n201349\n11.8\n170\n\n\n6\nBraga\n193324\n41.0\n112\n\n\n7\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\nLeft_index e Right_index Keywords\n\ndf1i = df1.set_index('cidade')\ndf2i = df2.set_index('cidade')\n\ndf1i\ndf2i\n\n\n\n\n\n\n\n\narea\nnuts3\n\n\ncidade\n\n\n\n\n\n\nLisboa\n100.1\n170\n\n\nSintra\n23.8\n170\n\n\nVila Nova de Gaia\n56.3\n11A\n\n\nPorto\n41.4\n11A\n\n\nCascais\n97.1\n170\n\n\nLoures\n11.8\n170\n\n\nBraga\n41.0\n112\n\n\nAlmada\n14.7\n170\n\n\n\n\n\n\n\n\npd.merge(df1i, df2i, left_index=True, right_index=True)\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\n\n\ncidade\n\n\n\n\n\n\n\nBraga\n193324\n41.0\n112\n\n\nLisboa\n544325\n100.1\n170\n\n\nSintra\n385989\n23.8\n170\n\n\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\nPorto\n231834\n41.4\n11A\n\n\nCascais\n214239\n97.1\n170\n\n\nLoures\n201349\n11.8\n170\n\n\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\nquando temos os indices dos dois lados podemos usar apenas o join\n\n# método antigo\ndf1i.join(df2i)\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\n\n\ncidade\n\n\n\n\n\n\n\nBraga\n193324\n41.0\n112\n\n\nLisboa\n544325\n100.1\n170\n\n\nSintra\n385989\n23.8\n170\n\n\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\nPorto\n231834\n41.4\n11A\n\n\nCascais\n214239\n97.1\n170\n\n\nLoures\n201349\n11.8\n170\n\n\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\nas keywords left_index e right_index são mais úteis quando pretendemos misturar index e colunas\n\npd.merge(df1i, df2a, left_index=True, right_on='cidade+100khab')\n\n\n\n\n\n\n\n\npopulacao\ncidade+100khab\narea\nnuts3\n\n\n\n\n6\n193324\nBraga\n41.0\n112\n\n\n0\n544325\nLisboa\n100.1\n170\n\n\n1\n385989\nSintra\n23.8\n170\n\n\n2\n304233\nVila Nova de Gaia\n56.3\n11A\n\n\n3\n231834\nPorto\n41.4\n11A\n\n\n4\n214239\nCascais\n97.1\n170\n\n\n5\n201349\nLoures\n11.8\n170\n\n\n7\n177943\nAlmada\n14.7\n170\n\n\n\n\n\n\n\n\n# para fazer reset de um index\ndf1i\ndf1i.reset_index()\n\n\n\n\n\n\n\n\ncidade\npopulacao\n\n\n\n\n0\nBraga\n193324\n\n\n1\nLisboa\n544325\n\n\n2\nSintra\n385989\n\n\n3\nVila Nova de Gaia\n304233\n\n\n4\nPorto\n231834\n\n\n5\nCascais\n214239\n\n\n6\nLoures\n201349\n\n\n7\nAlmada\n177943\n\n\n\n\n\n\n\n\n# posso continuar a fazer reset do index\n# isso irá acrescentando colunas\ndf1i.reset_index(inplace = True) # o 'inplace = True' altera o dataframe original \ndf1i\ndf1i.reset_index()\n\n\n\n\n\n\n\n\nindex\ncidade\npopulacao\n\n\n\n\n0\n0\nBraga\n193324\n\n\n1\n1\nLisboa\n544325\n\n\n2\n2\nSintra\n385989\n\n\n3\n3\nVila Nova de Gaia\n304233\n\n\n4\n4\nPorto\n231834\n\n\n5\n5\nCascais\n214239\n\n\n6\n6\nLoures\n201349\n\n\n7\n7\nAlmada\n177943\n\n\n\n\n\n\n\nInner e Outer Joins\n\ndf11 = pd.DataFrame({'cidade': ['Lisboa','Sintra'],\n                    'populacao': [544325, 385989]})\ndf12 = pd.DataFrame({'cidade': ['Lisboa','Porto', ],\n                    'area': [ 100.1, 97.1]})\ndf11\nprint()\ndf12\nprint()\npd.merge(df11, df12)\n\n\n\n\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\n\n\n\n\n0\nLisboa\n544325\n100.1\n\n\n\n\n\n\n\npor omissão é realizado o inner join mas podemos especificar o tipo de join\n\npd.merge(df11, df12, how='outer')\nprint() # paenas para acrescentr uma linha vazia\npd.merge(df11, df12, how='left')\n\n\n\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\n\n\n\n\n0\nLisboa\n544325\n100.1\n\n\n1\nSintra\n385989\nNaN\n\n\n\n\n\n\n\nSobreposição de Nomes de Colunas\n\ndf13 = pd.DataFrame({'cidade': ['Lisboa','Porto'],\n                    'area': [ 100, 97.5]})\ndf12\ndf13\nprint()\npd.merge(df12, df13, on='cidade')\n\n\n\n\n\n\n\n\n\n\n\ncidade\narea_x\narea_y\n\n\n\n\n0\nLisboa\n100.1\n100.0\n\n\n1\nPorto\n97.1\n97.5\n\n\n\n\n\n\n\npodemos indicar os sufixos que prentedemos para conhecermos a origem\n\npd.merge(df12, df13, on=\"cidade\", suffixes=[\"_12\", \"_13\"])\n\n\n\n\n\n\n\n\ncidade\narea_12\narea_13\n\n\n\n\n0\nLisboa\n100.1\n100.0\n\n\n1\nPorto\n97.1\n97.5\n\n\n\n\n\n\n\n\n\n5.1.4.3 Agregar e Agrupar\n\ndf5\n\ndf5.describe()\n\n\n\n\n\n\n\n\npopulacao\narea\n\n\n\n\ncount\n8.000000\n8.000000\n\n\nmean\n281654.500000\n48.275000\n\n\nstd\n126731.169791\n34.415601\n\n\nmin\n177943.000000\n11.800000\n\n\n25%\n199342.750000\n21.525000\n\n\n50%\n223036.500000\n41.200000\n\n\n75%\n324672.000000\n66.500000\n\n\nmax\n544325.000000\n100.100000\n\n\n\n\n\n\n\n\nprint(df5['populacao'].sum(), df5['populacao'].mean())\n\n2253236 281654.5\n\n\nvalores agrupados\n\ndf5.groupby('nuts3').populacao.mean()\n\n# ou de forma equivalente\ndf5.groupby('nuts3')['populacao'].mean()\n\nnuts3\n112    193324.0\n11A    268033.5\n170    304769.0\nName: populacao, dtype: float64\n\n\no object groupby suporta iteração sobre os grupos, isto pode ser útil para inspeccionarmos manualmente os grupos\n\n# inspecao da estrutura\nfor (group_name, group_data) in df5.groupby('nuts3'):\n    print(\"{0} shape={1}\".format(group_name, group_data.shape))\n\n112 shape=(1, 5)\n11A shape=(2, 5)\n170 shape=(5, 5)\n\n\n\n# summary statistics por grupo\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Nuts3:\", group_name)\n    print(\"Mean value:\", group_data['populacao'].mean())\n    print(\"Median value:\", group_data['populacao'].median())\n    print(\"Standard deviation:\", group_data['populacao'].std())\n    print()\n\nNuts3: 112\nMean value: 193324.0\nMedian value: 193324.0\nStandard deviation: nan\n\nNuts3: 11A\nMean value: 268033.5\nMedian value: 268033.5\nStandard deviation: 51193.82385112486\n\nNuts3: 170\nMean value: 304769.0\nMedian value: 214239.0\nStandard deviation: 157289.52373886824\n\n\n\n\n# inspeccionar valores unicos\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(\"Unique values:\", group_data['cidade'].count())\n    print(\"Unique values:\", group_data['cidade'].nunique())\n    print(\"Unique values:\", group_data['cidade'].unique())\n    print()\n\nCategory: 112\nUnique values: 1\nUnique values: 1\nUnique values: ['Braga']\n\nCategory: 11A\nUnique values: 2\nUnique values: 2\nUnique values: ['Vila Nova de Gaia' 'Porto']\n\nCategory: 170\nUnique values: 5\nUnique values: 5\nUnique values: ['Lisboa' 'Sintra' 'Cascais' 'Loures' 'Almada']\n\n\n\n\n# inspeccionar os tops\nN = 1\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(group_data.nlargest(N, 'area'))\n    print()\n\nCategory: 112\n  cidade  populacao  area nuts3 nuts3_dsg\n0  Braga     193324  41.0   112    Cávado\n\nCategory: 11A\n              cidade  populacao  area nuts3        nuts3_dsg\n3  Vila Nova de Gaia     304233  56.3   11A  Área Met. Porto\n\nCategory: 170\n   cidade  populacao   area nuts3         nuts3_dsg\n1  Lisboa     544325  100.1   170  Área Met. Lisboa\n\n\n\n\n# inspecao visual\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(group_data.head())\n    print()\n\nCategory: 112\n  cidade  populacao  area nuts3 nuts3_dsg\n0  Braga     193324  41.0   112    Cávado\n\nCategory: 11A\n              cidade  populacao  area nuts3        nuts3_dsg\n3  Vila Nova de Gaia     304233  56.3   11A  Área Met. Porto\n4              Porto     231834  41.4   11A  Área Met. Porto\n\nCategory: 170\n    cidade  populacao   area nuts3         nuts3_dsg\n1   Lisboa     544325  100.1   170  Área Met. Lisboa\n2   Sintra     385989   23.8   170  Área Met. Lisboa\n5  Cascais     214239   97.1   170  Área Met. Lisboa\n6   Loures     201349   11.8   170  Área Met. Lisboa\n7   Almada     177943   14.7   170  Área Met. Lisboa\n\n\n\n\n# filtrar grupos\nfor group_name, group_data in df5.groupby('nuts3'):\n    if group_data['area'].max() &gt; 100:\n        print(\"Categorias com area &gt; 100:\", group_name)\n\nCategorias com area &gt; 100: 170\n\n\n\n# Criar visualizacoes por grupo\nimport matplotlib.pyplot as plt\n\ndf5.set_index('cidade', inplace = True )\nfor group_name, group_data in df5.groupby('nuts3'):\n    group_data['populacao'].plot(kind='bar', title=group_name)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n# para aplicar funcao dentro do grupo\n# Function to calculate percentage change within each group\ndef calculate_relative_percentage(group,col_name):\n    total_sum = group[col_name].sum()\n    group['relative_percentage'] = (group[col_name] / total_sum) * 100\n    return group\n\n# Apply the custom analysis to each group\nresult_df = pd.DataFrame()\nfor name, group in df5.groupby('nuts3'):\n    group = calculate_relative_percentage(group, 'populacao')\n    result_df = pd.concat([result_df, group])\n\nresult_df\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\nrelative_percentage\n\n\ncidade\n\n\n\n\n\n\n\n\n\nBraga\n193324\n41.0\n112\nCávado\n100.000000\n\n\nVila Nova de Gaia\n304233\n56.3\n11A\nÁrea Met. Porto\n56.752794\n\n\nPorto\n231834\n41.4\n11A\nÁrea Met. Porto\n43.247206\n\n\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\n35.720497\n\n\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\n25.329938\n\n\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\n14.059107\n\n\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\n13.213221\n\n\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\n11.677238\n\n\n\n\n\n\n\n\ndf5.groupby('nuts3')['populacao'].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nnuts3\n\n\n\n\n\n\n\n\n\n\n\n\n112\n1.0\n193324.0\nNaN\n193324.0\n193324.00\n193324.0\n193324.00\n193324.0\n\n\n11A\n2.0\n268033.5\n51193.823851\n231834.0\n249933.75\n268033.5\n286133.25\n304233.0\n\n\n170\n5.0\n304769.0\n157289.523739\n177943.0\n201349.00\n214239.0\n385989.00\n544325.0\n\n\n\n\n\n\n\nFunções de Agregação\nvarias funções de agregação podem ser aplicadas em simultâneo\n\ndf5.groupby('nuts3')['populacao'].aggregate([\"min\", \"median\", \"mean\", \"max\"])\n\n\n\n\n\n\n\n\nmin\nmedian\nmean\nmax\n\n\nnuts3\n\n\n\n\n\n\n\n\n112\n193324\n193324.0\n193324.0\n193324\n\n\n11A\n231834\n268033.5\n268033.5\n304233\n\n\n170\n177943\n214239.0\n304769.0\n544325\n\n\n\n\n\n\n\nUtilização de filtros\n\ndef filter_func(x):\n    \"\"\"Defino a função de filtro\"\"\"\n    return x['populacao'].std() &gt; 100000\n\n# a função de filtro é aplicado ao grupo\ndf5.groupby('nuts3').filter(filter_func)\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\ncidade\n\n\n\n\n\n\n\n\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\n\n\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\n\n\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\n\n\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\n\n\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\n\ndf5.groupby('nuts3')['populacao'].std()\n\nnuts3\n112              NaN\n11A     51193.823851\n170    157289.523739\nName: populacao, dtype: float64\n\n\nTambém é comum passar as colunas de mapeamento dum dicionário para operações a serem aplicadas nessa coluna\n\ndf5.groupby('nuts3').aggregate({'populacao': 'min', 'area': 'max'})\n\n\n\n\n\n\n\n\npopulacao\narea\n\n\nnuts3\n\n\n\n\n\n\n112\n193324\n41.0\n\n\n11A\n231834\n56.3\n\n\n170\n177943\n100.1\n\n\n\n\n\n\n\nMétodo Transform (conserva o nr de linhas original)\nNa transformação, a saída tem o mesmo formato da entrada\n\ndef center(x):\n    return x - x.mean()\n\ndf5\nprint()\ndf5.groupby('nuts3')['populacao'].transform(center)\n\n\n\n\ncidade\nBraga                     0.0\nLisboa               239556.0\nSintra                81220.0\nVila Nova de Gaia     36199.5\nPorto                -36199.5\nCascais              -90530.0\nLoures              -103420.0\nAlmada              -126826.0\nName: populacao, dtype: float64\n\n\nMétodo Apply\n\ndef norm_by_area(x):\n    # x is a DataFrame of group values\n    x['populacao'] /= x['area'].sum()\n    return x\n\ndf5.groupby('nuts3').apply(norm_by_area)\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\2164890277.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df5.groupby('nuts3').apply(norm_by_area)\n\n\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\nnuts3\ncidade\n\n\n\n\n\n\n\n\n112\nBraga\n4715.219512\n41.0\n112\nCávado\n\n\n11A\nVila Nova de Gaia\n3113.950870\n56.3\n11A\nÁrea Met. Porto\n\n\nPorto\n2372.917093\n41.4\n11A\nÁrea Met. Porto\n\n\n170\nLisboa\n2199.292929\n100.1\n170\nÁrea Met. Lisboa\n\n\nSintra\n1559.551515\n23.8\n170\nÁrea Met. Lisboa\n\n\nCascais\n865.612121\n97.1\n170\nÁrea Met. Lisboa\n\n\nLoures\n813.531313\n11.8\n170\nÁrea Met. Lisboa\n\n\nAlmada\n718.961616\n14.7\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\n\n# com o group by por nuts3 somam-se as áreas da nuts3 \n# por exemplo na 11A será 56.3 + 41.4 = 97.7\n# como pop VNGaia = 304233 Porto = 231834 \nprint(\"Pop normalizada por nuts3 de {0} é {1}\". format(\"VNGaia\", 304233/97.7))\nprint(\"Pop normalizada por nuts3 de {0} é {1}\". format(\"Porto\", 231834/97.7))\n\nPop normalizada por nuts3 de VNGaia é 3113.950870010235\nPop normalizada por nuts3 de Porto é 2372.9170931422723\n\n\n\ndf5.groupby('cidade').apply(norm_by_area)\n\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\ncidade\ncidade\n\n\n\n\n\n\n\n\nAlmada\nAlmada\n12104.965986\n14.7\n170\nÁrea Met. Lisboa\n\n\nBraga\nBraga\n4715.219512\n41.0\n112\nCávado\n\n\nCascais\nCascais\n2206.374871\n97.1\n170\nÁrea Met. Lisboa\n\n\nLisboa\nLisboa\n5437.812188\n100.1\n170\nÁrea Met. Lisboa\n\n\nLoures\nLoures\n17063.474576\n11.8\n170\nÁrea Met. Lisboa\n\n\nPorto\nPorto\n5599.855072\n41.4\n11A\nÁrea Met. Porto\n\n\nSintra\nSintra\n16218.025210\n23.8\n170\nÁrea Met. Lisboa\n\n\nVila Nova de Gaia\nVila Nova de Gaia\n5403.783304\n56.3\n11A\nÁrea Met. Porto\n\n\n\n\n\n\n\n\n# com o group by por cidade não há lugar a somas... \n# as áreas são VNGaia = 56.3 Porto = 41.4 \n# como pop VNGaia = 304233 Porto = 231834 \nprint(\"Pop normalizada por cidade de{0} é {1}\". format(\"VNGaia\", 304233/56.3))\nprint(\"Pop normalizada por cidade de{0} é {1}\". format(\"Porto\", 231834/41.4))\n\nPop normalizada por cidade deVNGaia é 5403.783303730018\nPop normalizada por cidade dePorto é 5599.855072463768\n\n\nDiferenças entre Apply e Transform\n\ntransform() pode receber uma função, uma função de string, uma lista de funções e um dicionário. No entanto, apply() só é pode receber uma função.\n\ntransform() não pode produzir resultados agregados\n\napply() funciona com várias séries (várias colunas) ao mesmo tempo. No entanto, transform() só pode funcionar com uma série de cada vez.\n\n\n# Função de string\ndf5['populacao'].transform('sqrt')\n\n# lista de funções\ndf5['area'].transform([np.sqrt, np.exp])\n\n# Dicionário\ndf5.transform({\n    'populacao': np.sqrt,\n    'area': np.exp,\n})\n\n\n\n\n\n\n\n\npopulacao\narea\n\n\ncidade\n\n\n\n\n\n\nBraga\n439.686252\n6.398435e+17\n\n\nLisboa\n737.783844\n2.970829e+43\n\n\nSintra\n621.280130\n2.168746e+10\n\n\nVila Nova de Gaia\n551.573205\n2.823445e+24\n\n\nPorto\n481.491433\n9.545343e+17\n\n\nCascais\n462.859590\n1.479089e+42\n\n\nLoures\n448.719289\n1.332524e+05\n\n\nAlmada\n421.832905\n2.421748e+06\n\n\n\n\n\n\n\n\n# Apply consegue produzir agregados\ndf5.apply(lambda x:x.sum())\n\npopulacao                                              2253236\narea                                                     386.2\nnuts3                                 11217017011A11A170170170\nnuts3_dsg    CávadoÁrea Met. LisboaÁrea Met. LisboaÁrea Met...\ndtype: object\n\n\n\n## mas não funciona com o transform\ndf5.transform(lambda x:x.sum())\n\n\ndef subtract_two(x):\n    return x['populacao'] - x['area']\n  \n# apply funciona com várias séries em simultâneo\ndf5.apply(subtract_two, axis=1)\n\ncidade\nBraga                193283.0\nLisboa               544224.9\nSintra               385965.2\nVila Nova de Gaia    304176.7\nPorto                231792.6\nCascais              214141.9\nLoures               201337.2\nAlmada               177928.3\ndtype: float64\n\n\n\n# mas o transform não\ndf5.transform(subtract_two, axis=1)\n\nEspecificar as Split Keys para os grupos\nPodemos fazer grupos com uma lista, série ou index a especificar as keys pelas quais se faz o agrupamento. A key pode ser uma série ou lista com o comprimento da DataFrame.\n\nL = [0, 1, 0, 1, 2, 0, 3, 1]\ndf5.groupby(L).sum()\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\n\n\n0\n793552\n161.9\n112170170\nCávadoÁrea Met. LisboaÁrea Met. Lisboa\n\n\n1\n1026501\n171.1\n17011A170\nÁrea Met. LisboaÁrea Met. PortoÁrea Met. Lisboa\n\n\n2\n231834\n41.4\n11A\nÁrea Met. Porto\n\n\n3\n201349\n11.8\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\n\n# forma mais verbosa equivalente ao que temos usado até agora\n# aqui explictamos que a key é df5['nuts3'] e não apenas 'nuts3'\ndf5.groupby(df5['nuts3']).sum()\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3_dsg\n\n\nnuts3\n\n\n\n\n\n\n\n112\n193324\n41.0\nCávado\n\n\n11A\n536067\n97.7\nÁrea Met. PortoÁrea Met. Porto\n\n\n170\n1523845\n247.5\nÁrea Met. LisboaÁrea Met. LisboaÁrea Met. Lisb...\n\n\n\n\n\n\n\n\n# com um dicionário\n\ndf2g = df5.set_index('nuts3')\nmapping = {'11A': 'norte', '112': 'norte', '170': 'centro'}\ndf2g\nprint()\ndf2g.groupby(mapping).sum()\n\n\n\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3_dsg\n\n\nnuts3\n\n\n\n\n\n\n\ncentro\n1523845\n247.5\nÁrea Met. LisboaÁrea Met. LisboaÁrea Met. Lisb...\n\n\nnorte\n729391\n138.7\nCávadoÁrea Met. PortoÁrea Met. Porto\n\n\n\n\n\n\n\n\n\n5.1.4.4 Pivot Tables\n\nimport seaborn as sns # importamos esta package para termos acesso a um dataset\n\ntitanic = sns.load_dataset('titanic')\n\ntitanic.head()\nprint()\ntitanic.describe()\n\n\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\nPreparar manualmente a Pivot Table\n\ntitanic.groupby('sex')[['survived']].mean()\nprint()\ntitanic.groupby(['sex', 'class'], observed=True)['survived'].mean().unstack() # unstack para ter a pivot table\n\n\n\n\n\n\n\n\n\n\nclass\nFirst\nSecond\nThird\n\n\nsex\n\n\n\n\n\n\n\nfemale\n0.968085\n0.921053\n0.500000\n\n\nmale\n0.368852\n0.157407\n0.135447\n\n\n\n\n\n\n\n\nprint()\ntitanic.pivot_table(index='sex', columns='class',\n                    values='survived', aggfunc='mean', observed=True)\n\n\n\n\n\n\n\n\n\n\nclass\nFirst\nSecond\nThird\n\n\nsex\n\n\n\n\n\n\n\nfemale\n0.968085\n0.921053\n0.500000\n\n\nmale\n0.368852\n0.157407\n0.135447\n\n\n\n\n\n\n\nSyntax Pivot Table\n\ntitanic.pivot_table('survived', index='sex', columns='class', aggfunc='mean')\nprint()\nage_group = pd.cut(titanic['age'], [0, 18, 80])\nage_group\nprint()\n# multilevel pivot table\ntitanic.pivot_table('survived', index=['sex', age_group], \n                    columns='class', aggfunc='mean')\n\n\n\n\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\4189409504.py:1: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('survived', index='sex', columns='class', aggfunc='mean')\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\4189409504.py:7: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('survived', index=['sex', age_group],\n\n\n\n\n\n\n\n\n\nclass\nFirst\nSecond\nThird\n\n\nsex\nage\n\n\n\n\n\n\n\nfemale\n(0, 18]\n0.909091\n1.000000\n0.511628\n\n\n(18, 80]\n0.972973\n0.900000\n0.423729\n\n\nmale\n(0, 18]\n0.800000\n0.600000\n0.215686\n\n\n(18, 80]\n0.375000\n0.071429\n0.133663\n\n\n\n\n\n\n\nSummary Statistics na DataFrame\n\ntitanic.describe()\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\ntodas estas funções estão disponíveis como fomos vendo nos exemplos anteriores\n\ntitanic.pivot_table('fare', index=['sex'], \n                    columns='class', aggfunc='mean')\n                    \nprint()\n\ntitanic.pivot_table('survived', index=['class'], aggfunc='count')\n\n\n\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\4083826376.py:1: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('fare', index=['sex'],\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\4083826376.py:6: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('survived', index=['class'], aggfunc='count')\n\n\n\n\n\n\n\n\n\nsurvived\n\n\nclass\n\n\n\n\n\nFirst\n216\n\n\nSecond\n184\n\n\nThird\n491"
  },
  {
    "objectID": "500-mod5.html#estatisticas-oficiais",
    "href": "500-mod5.html#estatisticas-oficiais",
    "title": "5  Data Science (Intermediate)",
    "section": "5.2 Estatisticas Oficiais",
    "text": "5.2 Estatisticas Oficiais\n\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"PT_2012_Hosp.csv\"\n\nler os dados:\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nprint()\ndf_hosp.describe()\n\ndf_hosp = df_hosp.reset_index() # passar o index para uma coluna; podia ser feito com inplace = True\nprint()\ndf_hosp.head()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\281557973.py:1: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nDTCC_COD\nCC_DSG\nC10001\nC20001\nC21001\nC21011\nC21021\n...\nC30001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\n\n\n\n\n0\n2012\n229\n17\n1504\nBarreiro\n1458.0\n247.0\n159.0\n2.0\n11.0\n...\n493.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n\n\n1\n2012\n206\n17\n1507\nMontijo\n144.0\n0.0\n0.0\n0.0\n0.0\n...\n46.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n\n\n2\n2012\n65\n16\n502\nCastelo Branco\n894.0\n111.0\n89.0\n0.0\n8.0\n...\n337.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n\n\n3\n2012\n106\n17\n1114\nVila Franca de Xira\n801.0\n166.0\n108.0\n0.0\n12.0\n...\n264.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n\n\n4\n2012\n209\n11\n1315\nValongo\n221.0\n13.0\n13.0\n0.0\n0.0\n...\n88.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n\n\n\n\n5 rows × 64 columns\n\n\n\n\n5.2.1 Pre-processamento\n\n5.2.1.1 Exclusão de colunas:\n\n# exclusão da coluna nordem\ndf_hosp = df_hosp.drop(columns=['NORDEM'])\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNUTS2\nDTCC_COD\nCC_DSG\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\n...\nC30001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\n\n\n\n\n0\n2012\n17\n1504\nBarreiro\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n...\n493.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n\n\n1\n2012\n17\n1507\nMontijo\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n46.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n\n\n2\n2012\n16\n502\nCastelo Branco\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n...\n337.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n\n\n3\n2012\n17\n1114\nVila Franca de Xira\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n...\n264.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n\n\n4\n2012\n11\n1315\nValongo\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n...\n88.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n\n\n\n\n5 rows × 63 columns\n\n\n\n\n\n5.2.1.2 Variáveis com demasiadas categorias:\n\ndf_hosp['NUTS2'].value_counts().sort_values()\nprint()\ndf_hosp['DTCC_COD'].value_counts()\nprint()\ndf_hosp['DTCC_COD'].value_counts(normalize=True) # resultados percentuais\n\n\n\n\n\nDTCC_COD\n1106    0.158416\n603     0.099010\n1312    0.069307\n602     0.019802\n1110    0.019802\n          ...   \n1503    0.009901\n1419    0.009901\n1401    0.009901\n1418    0.009901\n1408    0.009901\nName: proportion, Length: 63, dtype: float64\n\n\nAgregar todos os municipios com menos de dez por cento das ocorrências na amostra:\n\n# verifica contagens para a coluna DTCC_COD\ndf_hosp['DTCC_COD_COUNT']= df_hosp.DTCC_COD.map(df_hosp.DTCC_COD.value_counts(normalize=True)) # Constroi a nova coluna\n\n# Correr sem criar a coluna primeiro\ndf_hosp.loc[df_hosp['DTCC_COD_COUNT'] &lt; 0.1, 'DTCC_COD_NEW'] = 'outro' # Constroi a nova coluna\ndf_hosp.loc[df_hosp['DTCC_COD_COUNT'] &gt;= 0.1, 'DTCC_COD_NEW'] = df_hosp['CC_DSG'] # atualiza os valores em falta\n\nprint()\ndf_hosp.loc[:,['DTCC_COD_NEW','DTCC_COD', 'CC_DSG']] # todas as linhas e as colunas selecionadas\n\n\n\n\n\n\n\n\n\n\n\nDTCC_COD_NEW\nDTCC_COD\nCC_DSG\n\n\n\n\n0\noutro\n1504\nBarreiro\n\n\n1\noutro\n1507\nMontijo\n\n\n2\noutro\n502\nCastelo Branco\n\n\n3\noutro\n1114\nVila Franca de Xira\n\n\n4\noutro\n1315\nValongo\n\n\n...\n...\n...\n...\n\n\n96\noutro\n1110\nOeiras\n\n\n97\nLisboa\n1106\nLisboa\n\n\n98\noutro\n1312\nPorto\n\n\n99\noutro\n1312\nPorto\n\n\n100\noutro\n1408\nConstância\n\n\n\n\n101 rows × 3 columns\n\n\n\n\n# exclusão das colunas já tratadas\ndf_hosp = df_hosp.drop(columns=['DTCC_COD', 'CC_DSG','DTCC_COD_COUNT'])\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n4\n2012\n11\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\noutro\n\n\n\n\n5 rows × 62 columns\n\n\n\n\n\n5.2.1.3 Missing data\nsem o scikit\nverificar onde temos dados em falta o método isna()\n\n#Check missing data\ndf_hosp.isna().sum()\n\nANO              0\nNUTS2            0\nC10001          15\nC20001          15\nC21001          15\n                ..\nC31051          15\nC31061          15\nC31071          15\nC32001          15\nDTCC_COD_NEW     0\nLength: 62, dtype: int64\n\n\npor haver linhas com todos os valores missing então há colunas que devem ser excluídos.\n\ndf_hosp[df_hosp.isna().any(axis=1)]\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n11\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n12\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n13\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n14\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n15\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n16\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n54\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n55\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n56\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n57\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n58\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n59\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n71\n2012\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n72\n2012\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n73\n2012\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n\n\n15 rows × 62 columns\n\n\n\n\n# dropna pode ser parametrizado para linhas ou colunas, e até thresholds\ndf_hosp1 = df_hosp.dropna()\ndf_hosp1\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n4\n2012\n11\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\noutro\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\n2012\n17\n96.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n9.0\n0.0\n0.0\n0.0\n3.0\n1.0\n4.0\n1.0\n20.0\noutro\n\n\n97\n2012\n17\n191.0\n42.0\n42.0\n0.0\n1.0\n0.0\n2.0\n0.0\n...\n5.0\n0.0\n0.0\n3.0\n0.0\n0.0\n2.0\n0.0\n27.0\nLisboa\n\n\n98\n2012\n11\n66.0\n21.0\n16.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n8.0\noutro\n\n\n99\n2012\n11\n491.0\n132.0\n76.0\n0.0\n4.0\n1.0\n3.0\n0.0\n...\n6.0\n0.0\n1.0\n3.0\n0.0\n1.0\n1.0\n0.0\n93.0\noutro\n\n\n100\n2012\n16\n59.0\n6.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n16.0\noutro\n\n\n\n\n86 rows × 62 columns\n\n\n\n\n# vamos fazer uma cópia para testes\ndf1 = df_hosp1.copy()\ndf1\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n4\n2012\n11\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\noutro\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\n2012\n17\n96.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n9.0\n0.0\n0.0\n0.0\n3.0\n1.0\n4.0\n1.0\n20.0\noutro\n\n\n97\n2012\n17\n191.0\n42.0\n42.0\n0.0\n1.0\n0.0\n2.0\n0.0\n...\n5.0\n0.0\n0.0\n3.0\n0.0\n0.0\n2.0\n0.0\n27.0\nLisboa\n\n\n98\n2012\n11\n66.0\n21.0\n16.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n8.0\noutro\n\n\n99\n2012\n11\n491.0\n132.0\n76.0\n0.0\n4.0\n1.0\n3.0\n0.0\n...\n6.0\n0.0\n1.0\n3.0\n0.0\n1.0\n1.0\n0.0\n93.0\noutro\n\n\n100\n2012\n16\n59.0\n6.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n16.0\noutro\n\n\n\n\n86 rows × 62 columns\n\n\n\nVamos introduzir um NaN num registo para experimentarmos outras formas de tratamento\n\ndf1.loc[0:3, :]\n\ndf1.loc[2,'C21001'] = np.nan\n\ndf1.loc[0:3, :]\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\nNaN\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n\n\n4 rows × 62 columns\n\n\n\nvamos inserir o valor médio da C21001 (Médicos - Especialistas - Total) para a nuts 16, mas de uma forma genérica para qualquer que fosse a coluna numérica:\n\n# passo 1 seleccionar a/as colunas onde há valores missing\ncols_missing = df1.isna().sum().index[df1.isna().sum().values &gt;0].tolist()\nprint(cols_missing)\n\n# passo 2 seleccionar o conjunto de colunas da dataframe que são numéricas\ncols_numerical = df1.select_dtypes(include=['number']).columns.tolist()\nprint(cols_numerical)\n\n# passo 3 imputar a mediana na for missing data\ncols_numerical_missing = [ col for col in cols_missing if (col in cols_numerical)] # loop for condicional\ndf1[cols_numerical_missing] = df1[cols_numerical_missing].fillna(df1.groupby('NUTS2')[cols_numerical_missing].transform('mean'))\n\ndf1.loc[0:3, :]\n\n['C21001']\n['ANO', 'NUTS2', 'C10001', 'C20001', 'C21001', 'C21011', 'C21021', 'C21031', 'C21041', 'C21061', 'C21071', 'C21081', 'C21091', 'C21101', 'C21111', 'C21121', 'C21131', 'C21141', 'C21151', 'C21161', 'C21171', 'C21181', 'C21191', 'C21201', 'C21211', 'C21221', 'C21231', 'C21241', 'C21251', 'C21261', 'C21271', 'C21281', 'C21291', 'C21301', 'C21311', 'C21321', 'C21331', 'C21341', 'C21351', 'C21361', 'C21371', 'C21381', 'C21391', 'C21401', 'C21411', 'C21421', 'C21431', 'C21441', 'C22001', 'C23001', 'C24001', 'C30001', 'C31001', 'C31011', 'C31021', 'C31031', 'C31041', 'C31051', 'C31061', 'C31071', 'C32001']\n\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n\n\n4 rows × 62 columns\n\n\n\nquando não aplicamos o transform temos resultados diferentes:\n\n# obtemos uma dataframe com 7 linhas e index na nuts2\ndf1.groupby('NUTS2')[cols_numerical_missing].mean()\nprint()\n\n# obtemos o mesmo nº de linhas que a dataframe tinha com a média calculada pelo group by\n# sem index\ndf1.groupby('NUTS2')[cols_numerical_missing].transform('mean')\nprint()\n\n# verificação das médias\ndf1.groupby('NUTS2')['C21001'].mean()\n\n\n\n\n\nNUTS2\n11     57.933333\n15     61.500000\n16     38.250000\n17    152.300000\n18     68.500000\n20     30.000000\n30     29.000000\nName: C21001, dtype: float64\n\n\nusando o scikit\n\ndf1.loc[2,'C21001'] = np.nan\ndf1.loc[0:3, :]\n\n# o simple imputer não pode ser usado em colunas categóricas \n# com a estratégia de média, só em colunas numéricas\ndf1 = df1.drop(columns = ['DTCC_COD_NEW'])\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(df1)\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SimpleImputer?Documentation for SimpleImputeriFittedSimpleImputer() \n\n\ne agora atua sobre a dataframe:\n\nx = pd.DataFrame(imputer.transform(df1))\nx\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n\n0\n2012.0\n17.0\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n493.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n\n\n1\n2012.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n46.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n\n\n2\n2012.0\n16.0\n894.0\n111.0\n74.4\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n337.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n\n\n3\n2012.0\n17.0\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n264.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n\n\n4\n2012.0\n11.0\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n88.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n81\n2012.0\n17.0\n96.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n29.0\n9.0\n0.0\n0.0\n0.0\n3.0\n1.0\n4.0\n1.0\n20.0\n\n\n82\n2012.0\n17.0\n191.0\n42.0\n42.0\n0.0\n1.0\n0.0\n2.0\n0.0\n...\n32.0\n5.0\n0.0\n0.0\n3.0\n0.0\n0.0\n2.0\n0.0\n27.0\n\n\n83\n2012.0\n11.0\n66.0\n21.0\n16.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n10.0\n2.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n8.0\n\n\n84\n2012.0\n11.0\n491.0\n132.0\n76.0\n0.0\n4.0\n1.0\n3.0\n0.0\n...\n99.0\n6.0\n0.0\n1.0\n3.0\n0.0\n1.0\n1.0\n0.0\n93.0\n\n\n85\n2012.0\n16.0\n59.0\n6.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n17.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n16.0\n\n\n\n\n86 rows × 61 columns\n\n\n\no método SimpleImputer do scikit.impute não suporta o groupby directamente mas é muito útil e suporta categorias quando a estratégia selecionada é ‘most_frequent’ ou ‘constant’ além disso pode ser usado indirectamente o groupby recorrendo a uma função lambda aplicada nas colunas dentro da função transform\n\nx.iloc[:4,:6]\n\ndf1.iloc[:4,:6]\n\ndf1['C21001'] = df1.groupby('NUTS2')['C21001'].transform(\n    lambda col: imputer.fit_transform(col.to_frame()).flatten(),)\nprint()\n\ndf1.iloc[:4,:6]\n\n\n\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.00\n2.0\n\n\n1\n2012\n17\n144.0\n0.0\n0.00\n0.0\n\n\n2\n2012\n16\n894.0\n111.0\n38.25\n0.0\n\n\n3\n2012\n17\n801.0\n166.0\n108.00\n0.0\n\n\n\n\n\n\n\nImputação de Variáveis Categóricas\n\n# Seleciona todas as colunas categóricas e atribui-lhes \n# um novo nível criado 'missing'\ncols_categorical = df_hosp.select_dtypes(include=['object']).columns.tolist()\n\ndf_hosp[cols_categorical] = df_hosp[cols_categorical].fillna('missing')\nprint(cols_categorical)\n\n['DTCC_COD_NEW']\n\n\nexemplo em que usamos vários imputadores em simultâneo, seria possível usando o ColumnTransformer do módulo sklearn.compose e especificando as colunas a fectar é apresentado abaixo:\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nA = [[np.nan,2,'Jose'],[4,np.nan,'Maria'],[7,8,'Joao'],[10,5,np.nan]]\n\ncol_trans = ColumnTransformer(\n[('imp0', SimpleImputer(strategy='constant', fill_value=1), [0]),\n ('imp1', SimpleImputer(strategy='mean'), [1]),\n ('imp2', SimpleImputer(strategy='constant', fill_value='desconhecido'), [2])],\nremainder='passthrough')\n\ncol_trans.fit_transform(A)\n\narray([[1, 2.0, 'Jose'],\n       [4, 5.0, 'Maria'],\n       [7, 8.0, 'Joao'],\n       [10, 5.0, 'desconhecido']], dtype=object)\n\n\nImputação com o vizinho mais próximo\n\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputer.fit_transform(X)\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\n\n\n5.2.1.4 Variáveis Categóricas\n\n## Typecast da coluna para categoria em usando o pandas\ndf1['NUTS2'] = pd.Categorical(df1.NUTS2)\n\ndf1.dtypes\n\nprint()\n## Typecast da coluna para categoria em python\ndf_hosp11 = df_hosp1.copy()\ndf_hosp11['NUTS2']= df_hosp1.NUTS2.astype('category')\ndf_hosp11.dtypes\n\n\n\n\nANO                int64\nNUTS2           category\nC10001           float64\nC20001           float64\nC21001           float64\n                  ...   \nC31051           float64\nC31061           float64\nC31071           float64\nC32001           float64\nDTCC_COD_NEW      object\nLength: 62, dtype: object\n\n\nCriação de Variáveis Categóricas\ncriar colunas à custa de outras colunas directamente usando o método map:\n\ndf_hosp2 = df1.copy()\ndf_hosp2\n\ndef label(value): # função para criar uma variável dummy\n    if value == 0:\n        return \"no\"\n    if value &gt; 0:\n        return \"yes\"\n\ndf_hosp2['t_cirurgia'] = df_hosp2['C21071'].map(label)\ndf_hosp2.head()\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\nyes\n\n\n1\n2012\n17\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\nno\n\n\n2\n2012\n16\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\nyes\n\n\n3\n2012\n17\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\nyes\n\n\n4\n2012\n11\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\nyes\n\n\n\n\n5 rows × 62 columns\n\n\n\nVariáveis Dummy\n\n# Definimos y como o nosso target\nX = df_hosp2.drop(columns=['t_cirurgia'])\ny = df_hosp2['t_cirurgia'].values\n\n#Transforma as variáveis categoricas em dummies com drop da baseline\ndf_x = pd.get_dummies(X, drop_first = True)\n\ndf_x.head()\n\n\n\n\n\n\n\n\nANO\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\nC21071\n...\nC31051\nC31061\nC31071\nC32001\nNUTS2_15\nNUTS2_16\nNUTS2_17\nNUTS2_18\nNUTS2_20\nNUTS2_30\n\n\n\n\n0\n2012\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n19.0\n...\n3.0\n5.0\n0.0\n454.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n2012\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n46.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\n2012\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n10.0\n...\n0.0\n0.0\n0.0\n337.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n12.0\n...\n2.0\n7.0\n0.0\n228.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n2012\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n...\n1.0\n6.0\n0.0\n80.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 66 columns\n\n\n\nVariáveis Dummy com Scikit\nScikit tem vários encoders os mais comuns são o OrdinalEncoder e o HotEncoder.\n\nOrdinalEncoder é usado para transformar variáveis categóricas em numéricas.\nOneHotEncoder é usado para transformar variáveis categóricas em variáveis dummy.\n\n(criar encoder, fazer o fit e efectuar a transformação)\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')\n\n# Definimos X_sk como a coluna para criar dummies nos exemplos scikit\n# atenção df_hosp2[['NUTS2']] é uma dataframe, df_hosp2['NUTS2'] é uma serie\nX_sk = df_hosp2[['NUTS2']]\n\nprint(type(df_hosp2[['NUTS2']]))\nprint()\nencoder = encoder.fit(X_sk)\n\nX_encoded = pd.DataFrame(encoder.transform(X_sk).toarray()) # transforma a matriz em dataframe\nX_encoded.head()\nprint()\n\nX_encoded = X_encoded.rename(columns = {0 : \"Nut0\", 1 : \"Nut1\", 2 : \"Nut2\", 3 : \"Nut3\"\n                                       , 4 : \"Nut4\", 5 : \"Nut5\", 6 : \"Nut6\"}) # renomeia as colunas\nX_encoded\nprint()\n\n# merge com a dataframe dos hospitais\ndf_hosp2 = df_hosp2.join(X_encoded)\ndf_hosp2 = df_hosp2.drop(columns = 'NUTS2') # drop da coluna original\ndf_hosp2.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nANO\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\nC21071\n...\nC31071\nC32001\nt_cirurgia\nNut0\nNut1\nNut2\nNut3\nNut4\nNut5\nNut6\n\n\n\n\n0\n2012\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n19.0\n...\n0.0\n454.0\nyes\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n2012\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n46.0\nno\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n2012\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n10.0\n...\n0.0\n337.0\nyes\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n2012\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n12.0\n...\n0.0\n228.0\nyes\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n2012\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n...\n0.0\n80.0\nyes\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 68 columns\n\n\n\n\n\n5.2.1.5 Variáveis Correlacionadas\nem modelos de regressão podemos ter problemas com variáveis correlacionadas (multicolinearidade)\n\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = df_x.corr().abs()\ncor_matrix\nprint()\n\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\nprint(upper_tri)\nprint()\n\n\n          ANO  C10001    C20001    C21001    C21011    C21021    C21031  \\\nANO       NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nC10001    NaN     NaN  0.982999  0.983593  0.783939  0.926770  0.662279   \nC20001    NaN     NaN       NaN  0.996023  0.758603  0.937420  0.689197   \nC21001    NaN     NaN       NaN       NaN  0.780568  0.944305  0.689538   \nC21011    NaN     NaN       NaN       NaN       NaN  0.708871  0.501019   \n...       ...     ...       ...       ...       ...       ...       ...   \nNUTS2_16  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_17  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_18  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_20  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_30  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \n\n            C21041    C21061    C21071  ...    C31051    C31061    C31071  \\\nANO            NaN       NaN       NaN  ...       NaN       NaN       NaN   \nC10001    0.856563  0.550499  0.900382  ...  0.285700  0.405187  0.334934   \nC20001    0.864047  0.560911  0.880763  ...  0.259565  0.412341  0.336556   \nC21001    0.860977  0.563814  0.891950  ...  0.250113  0.405222  0.341762   \nC21011    0.638383  0.576456  0.691786  ...  0.256874  0.146949  0.135031   \n...            ...       ...       ...  ...       ...       ...       ...   \nNUTS2_16       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_17       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_18       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_20       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_30       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n\n            C32001  NUTS2_15  NUTS2_16  NUTS2_17  NUTS2_18  NUTS2_20  NUTS2_30  \nANO            NaN       NaN       NaN       NaN       NaN       NaN       NaN  \nC10001    0.993045  0.027644  0.205653  0.380013  0.035742  0.020845  0.057632  \nC20001    0.966742  0.022840  0.205238  0.370057  0.001802  0.042949  0.064700  \nC21001    0.968224  0.017892  0.212531  0.400595  0.013943  0.044323  0.064488  \nC21011    0.757519  0.022719  0.112272  0.370731  0.002489  0.041404  0.058901  \n...            ...       ...       ...       ...       ...       ...       ...  \nNUTS2_16       NaN       NaN       NaN  0.352410  0.175322  0.069438  0.098783  \nNUTS2_17       NaN       NaN       NaN       NaN  0.150756  0.059708  0.084941  \nNUTS2_18       NaN       NaN       NaN       NaN       NaN  0.029704  0.042258  \nNUTS2_20       NaN       NaN       NaN       NaN       NaN       NaN  0.016737  \nNUTS2_30       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n\n[66 rows x 66 columns]\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# o heatmap é uma boa forma de visualizar as matrizes de correlações\nplt.figure(figsize = (20,20))\nsns.heatmap(cor_matrix)\n\nplt.show()\n\n\n\n\n\n# seleciona para remover as colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] &gt; 0.9)]\nprint(to_drop)\n\n['C20001', 'C21001', 'C21021', 'C21071', 'C21251', 'C21421', 'C21431', 'C23001', 'C24001', 'C30001', 'C32001']\n\n\n\ndf_hosp3 = df_x.drop(columns=to_drop, axis=1)\ndf_hosp3.head()\n\n\n\n\n\n\n\n\nANO\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\nC21101\nC21111\n...\nC31041\nC31051\nC31061\nC31071\nNUTS2_15\nNUTS2_16\nNUTS2_17\nNUTS2_18\nNUTS2_20\nNUTS2_30\n\n\n\n\n0\n2012\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n2.0\n1.0\n...\n2.0\n3.0\n5.0\n0.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n2012\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\n2012\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n2.0\n2.0\n7.0\n0.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n2012\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n6.0\n0.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 55 columns\n\n\n\n\n\n5.2.1.6 Class Imbalance\n\nfrom sklearn.linear_model import LogisticRegression\n\n# exemplo de declaração do regressor balanceando as classes\nLr = LogisticRegression(class_weight='balanced') # declarar uma regressão logística com classes balanceadas\n\n\n\n5.2.1.7 Normalização e Standardização\n\nshape = (4, 2)\nshape\n\nimport numpy.random as rd\n\n# *shape faz unpack do tuplo shape\nrd.rand(*shape) # gerador de samples de uma distribuição uniforme\n\narray([[0.67990644, 0.17029312],\n       [0.13332023, 0.87907197],\n       [0.02816634, 0.38036025],\n       [0.65046877, 0.46764812]])\n\n\n\n# random.RandomState.lognormal(mean=0.0, sigma=1.0, size=None) \n# gerador de samples de uma distribuição log-normal\nrd.rand(*shape) * rd.lognormal(0, 1, shape) \n\narray([[1.78713792, 1.51205203],\n       [0.04938606, 0.05755813],\n       [0.00308035, 0.15499421],\n       [0.54919712, 2.13729148]])\n\n\n\nimport numpy.random as rd\nimport pandas as pd\nfrom sklearn.preprocessing import Normalizer\nimport seaborn as sns\n\nimport warnings # para suprimir warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nshape = (100, 2)\n# *shape faz unpack do tuplo shape\ndf = pd.DataFrame(rd.rand(*shape) * rd.lognormal(1, 0.4, shape)\n                  , columns=[\"weight\", \"age\"]) # gerar um dataframe com 100 linhas e 2 colunas\nndf = pd.DataFrame(Normalizer(norm=\"l2\").fit_transform(df),\n                   columns=[\"norm_weight\", \"norm_age\"]) # normalizar os dados\n\n# kernel density estimate (KDE) plot é um método para \n# visualizar a distribuição de observações num dataset\nsns.kdeplot(data=pd.concat([df, ndf], axis=1), fill=True, \n            common_norm=False, palette=\"crest\",alpha=.5, linewidth=1,)\n            \nplt.show()\n            \ndf\n\n\n\n\n\n\n\n\n\n\n\nweight\nage\n\n\n\n\n0\n0.342849\n5.180668\n\n\n1\n2.755412\n1.808409\n\n\n2\n0.732323\n0.370073\n\n\n3\n1.826203\n2.140827\n\n\n4\n0.990985\n0.438438\n\n\n...\n...\n...\n\n\n95\n1.725187\n0.504171\n\n\n96\n2.463615\n3.370624\n\n\n97\n0.805599\n1.107084\n\n\n98\n0.860474\n0.102522\n\n\n99\n0.195935\n3.549530\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nfor d in [df]:\n    sns.pairplot(d.reset_index(), hue=\"index\", diag_kind=None)\n    \nplt.show()\n\n\n\n\n\n# repete o exemplo com shape(50,2) faz standardization\nimport numpy.random as rd\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\nshape = (50, 2)\n# *shape faz unpack do tuplo shape\ndf = pd.DataFrame(rd.rand(*shape) * rd.normal(10, 6, shape),\n                  columns=[\"weight\", \"age\"])\nndf = pd.DataFrame(StandardScaler().fit_transform(df),\n                   columns=[\"norm_weight\", \"norm_age\"])\n\n# kernel density estimate (KDE) plot é um método para \n# visualizar a distribuição de observações num dataset\nsns.kdeplot(data=pd.concat([df, ndf], axis=1), \n            fill=True, common_norm=False, palette=\"crest\",\n            alpha=.5, linewidth=1,)\n            \nplt.show()\n\n\n\n\n\nfor d in [df]:\n    sns.pairplot(d.reset_index(), hue=\"index\", palette=\"crest\", diag_kind=None)\n\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Define a pre-processing pipeline\npipeline = Pipeline([\n('selector', VarianceThreshold()),\n('scaler', StandardScaler()),\n('KNN', KNeighborsClassifier())])\n\n\n#Simple pipeline execution with default parameters\npipeline.fit(X, y)\nprint('Training set score: ' + str(pipeline.score(X,y)))\n\npipeline.predict(X)\n\ny\n\npipeline.predict_proba(X)\n\n\n\n\n5.2.2 Gravar os dados\n\ndf_prep = pd.concat([X, pd.DataFrame(y, columns=[\"t_cirurgia\"])], axis=1)\n\ndf_prep.head()\n\n# Exportação da dataframe\nfileout = 'df_prep.csv'\ndf_prep.to_csv(f\"{datadir}{fileout}\",  index=False)"
  },
  {
    "objectID": "500-mod5.html#introdução-a-machine-learning",
    "href": "500-mod5.html#introdução-a-machine-learning",
    "title": "5  Data Science (Intermediate)",
    "section": "5.3 Introdução a Machine Learning",
    "text": "5.3 Introdução a Machine Learning\n\n5.3.1 Regressão\nQUESTÃO:\nCom este conjunto de dados que inclui o número de médicos e enfermeiros em várias especialidades será que consigo estimar o nº de total de Enfermeiros - Especialistas - Em Saúde Infantil e Pediátrica em cada hospital?\n\nColuna C31011\n\n\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"df_prep.csv\"\n\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nprint()\ndf_hosp.describe()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\390302264.py:1: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\n\n\n\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC30001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\n\n\n\n\ncount\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n...\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n\n\nmean\n118.081395\n14.976744\n642.744186\n113.872093\n73.979651\n0.813953\n6.941860\n0.360465\n2.953488\n0.302326\n...\n212.290698\n22.941860\n3.127907\n6.220930\n4.476744\n3.465116\n0.965116\n3.465116\n1.220930\n189.348837\n\n\nstd\n68.264230\n3.620129\n809.928453\n172.413511\n108.256016\n2.144825\n10.384779\n1.146994\n5.346592\n1.701693\n...\n255.792649\n26.911775\n4.752204\n10.678622\n5.740930\n8.559409\n1.482825\n4.421066\n2.783993\n240.396475\n\n\nmin\n3.000000\n11.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n57.500000\n11.000000\n120.250000\n7.750000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n34.250000\n3.250000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n30.000000\n\n\n50%\n119.000000\n16.000000\n338.500000\n42.500000\n33.000000\n0.000000\n2.000000\n0.000000\n0.000000\n0.000000\n...\n102.500000\n10.000000\n0.000000\n0.000000\n3.000000\n1.000000\n0.000000\n2.000000\n0.000000\n85.500000\n\n\n75%\n175.750000\n17.000000\n872.500000\n162.500000\n115.500000\n0.000000\n10.750000\n0.000000\n3.750000\n0.000000\n...\n321.250000\n35.750000\n4.000000\n11.000000\n6.000000\n3.000000\n1.000000\n5.000000\n1.000000\n282.250000\n\n\nmax\n229.000000\n30.000000\n5325.000000\n1161.000000\n719.000000\n13.000000\n51.000000\n6.000000\n31.000000\n13.000000\n...\n1515.000000\n103.000000\n21.000000\n50.000000\n28.000000\n72.000000\n6.000000\n19.000000\n15.000000\n1515.000000\n\n\n\n\n8 rows × 61 columns\n\n\n\n\ndf_hosp = df_hosp.reset_index() # passar o index para uma coluna; podia ser feito com inplace = True\n\n\n# Definimos y como o nosso target\nX = df_hosp.drop(columns=['t_cirurgia'])\ny = df_hosp['t_cirurgia'].values\n\n\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = X.corr().abs()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n\n\n# seleciona para remover as colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] &gt; 0.9)]\nprint(to_drop)\n\n['C20001', 'C21001', 'C21021', 'C21071', 'C21251', 'C21421', 'C21431', 'C23001', 'C24001', 'C30001', 'C32001']\n\n\n\ndf_hosp = df_hosp.drop(columns=to_drop, axis=1) # drop das colunas altamente correlacionadas\n\n\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC22001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n4.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n7.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n\n\n\n\n5 rows × 52 columns\n\n\n\nprimeiro modelo de regressão linear com uma variavél explicativa:\n\nimport statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001',data = df_hosp).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.679\nModel:                            OLS   Adj. R-squared:                  0.675\nMethod:                 Least Squares   F-statistic:                     177.9\nDate:                Wed, 29 May 2024   Prob (F-statistic):           1.89e-22\nTime:                        22:27:40   Log-Likelihood:                -206.66\nNo. Observations:                  86   AIC:                             417.3\nDf Residuals:                      84   BIC:                             422.2\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.2111      0.385     -0.549      0.585      -0.976       0.554\nC31001         0.1455      0.011     13.339      0.000       0.124       0.167\n==============================================================================\nOmnibus:                       27.749   Durbin-Watson:                   2.131\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              128.206\nSkew:                          -0.795   Prob(JB):                     1.45e-28\nKurtosis:                       8.766   Cond. No.                         46.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Este é o R entre as 2 variáveis\nr = cor_matrix.loc['C31001','C31011']\nr\n\n0.8242029193194886\n\n\n\n# E este é o r quadrado\nr2 = r**2\nr2\n\n0.6793104522147675\n\n\nmodelos com duas variáveis explicativas:\n\nimport statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001 + C21011',data = df_hosp).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.681\nModel:                            OLS   Adj. R-squared:                  0.673\nMethod:                 Least Squares   F-statistic:                     88.60\nDate:                Wed, 29 May 2024   Prob (F-statistic):           2.55e-21\nTime:                        22:27:40   Log-Likelihood:                -206.43\nNo. Observations:                  86   AIC:                             418.9\nDf Residuals:                      83   BIC:                             426.2\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.2542      0.391     -0.650      0.518      -1.032       0.524\nC31001         0.1441      0.011     12.916      0.000       0.122       0.166\nC21011         0.0935      0.140      0.668      0.506      -0.185       0.372\n==============================================================================\nOmnibus:                       25.855   Durbin-Watson:                   2.130\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              118.604\nSkew:                          -0.715   Prob(JB):                     1.76e-26\nKurtosis:                       8.573   Cond. No.                         47.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# import statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001 + C21361',data = df_hosp).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.697\nModel:                            OLS   Adj. R-squared:                  0.689\nMethod:                 Least Squares   F-statistic:                     95.36\nDate:                Wed, 29 May 2024   Prob (F-statistic):           3.12e-22\nTime:                        22:27:40   Log-Likelihood:                -204.26\nNo. Observations:                  86   AIC:                             414.5\nDf Residuals:                      83   BIC:                             421.9\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.2519      0.377     -0.669      0.505      -1.001       0.497\nC31001         0.1263      0.014      9.138      0.000       0.099       0.154\nC21361         0.0760      0.035      2.186      0.032       0.007       0.145\n==============================================================================\nOmnibus:                       23.605   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               93.192\nSkew:                          -0.684   Prob(JB):                     5.80e-21\nKurtosis:                       7.913   Cond. No.                         48.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nmodelo com todas as variáveis explicativas:\n\n# reorganiza as colunas para colocar a coluna target no fim\nlast_cols = ['C31011']\nfirst_cols = [col for col in df_hosp.columns if col not in last_cols]\n\ndf = df_hosp[first_cols+last_cols]\ndf.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC22001\nC31001\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\nC31011\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n4.0\n39.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n8.0\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n0.0\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n0.0\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n7.0\n36.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n4.0\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n8.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n0.0\n\n\n\n\n5 rows × 52 columns\n\n\n\n\nstring_cols = ' + '.join(df.columns[:-1])\nest = smf.ols('C31011 ~ {}'.format(string_cols),data = df).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       1.000\nModel:                            OLS   Adj. R-squared:                  1.000\nMethod:                 Least Squares   F-statistic:                 1.313e+25\nDate:                Wed, 29 May 2024   Prob (F-statistic):          4.89e-260\nTime:                        22:27:41   Log-Likelihood:                 1869.0\nNo. Observations:                  71   AIC:                            -3638.\nDf Residuals:                      21   BIC:                            -3525.\nDf Model:                          49                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept         -2.744e-17    4.8e-19    -57.221      0.000   -2.84e-17   -2.64e-17\nt_cirurgia[T.yes] -3.011e-15    6.7e-13     -0.004      0.996    -1.4e-12    1.39e-12\nANO                -5.21e-16   9.65e-16     -0.540      0.595   -2.53e-15    1.49e-15\nNORDEM            -3.381e-16   6.81e-15     -0.050      0.961   -1.45e-14    1.38e-14\nNUTS2              1.128e-15   9.23e-14      0.012      0.990   -1.91e-13    1.93e-13\nC10001              2.89e-16   5.08e-15      0.057      0.955   -1.03e-14    1.09e-14\nC21011              4.33e-15   2.34e-12      0.002      0.999   -4.86e-12    4.87e-12\nC21031            -3.053e-15    1.8e-12     -0.002      0.999   -3.76e-12    3.75e-12\nC21041             1.887e-15   3.77e-13      0.005      0.996   -7.81e-13    7.85e-13\nC21061            -6.883e-15    1.4e-12     -0.005      0.996   -2.92e-12    2.91e-12\nC21081             -2.22e-14   8.34e-12     -0.003      0.998   -1.74e-11    1.73e-11\nC21091            -9.267e-15   3.65e-12     -0.003      0.998    -7.6e-12    7.58e-12\nC21101            -3.553e-15   1.75e-12     -0.002      0.998   -3.65e-12    3.64e-12\nC21111            -1.443e-15   8.72e-13     -0.002      0.999   -1.82e-12    1.81e-12\nC21121            -3.372e-16    1.7e-13     -0.002      0.998   -3.54e-13    3.53e-13\nC21131             3.997e-15   1.87e-12      0.002      0.998   -3.89e-12    3.89e-12\nC21141            -5.107e-15   7.91e-13     -0.006      0.995   -1.65e-12    1.64e-12\nC21151            -2.446e-15   1.18e-12     -0.002      0.998   -2.46e-12    2.46e-12\nC21161             1.593e-14   9.24e-12      0.002      0.999   -1.92e-11    1.92e-11\nC21171            -1.943e-15   2.47e-13     -0.008      0.994   -5.15e-13    5.11e-13\nC21181              1.21e-14   8.42e-13      0.014      0.989   -1.74e-12    1.76e-12\nC21191              1.36e-15   7.98e-13      0.002      0.999   -1.66e-12    1.66e-12\nC21201             -1.11e-15   8.42e-13     -0.001      0.999   -1.75e-12    1.75e-12\nC21211             7.461e-14   6.04e-12      0.012      0.990   -1.25e-11    1.26e-11\nC21221            -1.421e-14    2.1e-12     -0.007      0.995   -4.37e-12    4.34e-12\nC21231            -2.956e-15   4.92e-13     -0.006      0.995   -1.03e-12    1.02e-12\nC21241             8.882e-16    4.3e-13      0.002      0.998   -8.94e-13    8.95e-13\nC21261            -7.994e-15   4.99e-12     -0.002      0.999   -1.04e-11    1.04e-11\nC21271             1.443e-15   5.72e-13      0.003      0.998   -1.19e-12    1.19e-12\nC21281             1.943e-15    2.4e-12      0.001      0.999   -4.98e-12    4.98e-12\nC21291             4.219e-15      7e-13      0.006      0.995   -1.45e-12    1.46e-12\nC21301            -1.332e-15   1.27e-12     -0.001      0.999   -2.65e-12    2.65e-12\nC21311             -2.22e-16   4.36e-13     -0.001      1.000   -9.07e-13    9.07e-13\nC21321            -1.915e-15   1.14e-12     -0.002      0.999   -2.37e-12    2.36e-12\nC21331              1.11e-15   1.62e-13      0.007      0.995   -3.36e-13    3.38e-13\nC21341             1.887e-15   6.94e-13      0.003      0.998   -1.44e-12    1.44e-12\nC21351            -5.024e-15    4.3e-13     -0.012      0.991      -9e-13     8.9e-13\nC21361             1.832e-15   3.55e-13      0.005      0.996   -7.36e-13     7.4e-13\nC21371             5.412e-16   2.02e-13      0.003      0.998   -4.19e-13     4.2e-13\nC21381              3.86e-16   1.22e-13      0.003      0.998   -2.53e-13    2.54e-13\nC21391            -4.441e-15   1.01e-12     -0.004      0.997   -2.11e-12    2.11e-12\nC21401            -8.604e-16   3.76e-13     -0.002      0.998   -7.83e-13    7.81e-13\nC21411            -5.773e-15   2.11e-12     -0.003      0.998    -4.4e-12    4.39e-12\nC21441              1.45e-16   3.21e-27   4.51e+10      0.000    1.45e-16    1.45e-16\nC22001            -7.633e-16   1.57e-13     -0.005      0.996   -3.28e-13    3.26e-13\nC31001                1.0000   3.68e-13   2.72e+12      0.000       1.000       1.000\nC31021               -1.0000   4.24e-13  -2.36e+12      0.000      -1.000      -1.000\nC31031               -1.0000    2.4e-13  -4.16e+12      0.000      -1.000      -1.000\nC31041               -1.0000   3.75e-13  -2.67e+12      0.000      -1.000      -1.000\nC31051               -1.0000   5.72e-13  -1.75e+12      0.000      -1.000      -1.000\nC31061               -1.0000   4.48e-13  -2.23e+12      0.000      -1.000      -1.000\nC31071               -1.0000   5.96e-13  -1.68e+12      0.000      -1.000      -1.000\n==============================================================================\nOmnibus:                       59.367   Durbin-Watson:                   0.132\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              330.141\nSkew:                          -2.462   Prob(JB):                     2.05e-72\nKurtosis:                      12.346   Cond. No.                     3.45e+19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.8e-31. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n\n\n\n\n\nWarning\n\n\n\na variável C31001 permite construir Y directamente!\n\n\n\ndf = df.drop(columns='C31001', axis=1)\ndf.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC21441\nC22001\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\nC31011\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n4.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n8.0\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n0.0\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n0.0\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n7.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n4.0\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n0.0\n\n\n\n\n5 rows × 51 columns\n\n\n\ne voltamos a fazer o modelo\n\nstring_cols = ' + '.join(df.columns[:-1])\nest = smf.ols('C31011 ~ {}'.format(string_cols),data = df).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.988\nModel:                            OLS   Adj. R-squared:                  0.963\nMethod:                 Least Squares   F-statistic:                     39.38\nDate:                Wed, 29 May 2024   Prob (F-statistic):           1.05e-13\nTime:                        22:27:41   Log-Likelihood:                -55.748\nNo. Observations:                  71   AIC:                             209.5\nDf Residuals:                      22   BIC:                             320.4\nDf Model:                          48                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept           6.22e-07   2.44e-07      2.544      0.018    1.15e-07    1.13e-06\nt_cirurgia[T.yes]    -0.4189      0.378     -1.108      0.280      -1.203       0.365\nANO                   0.0013      0.000      2.544      0.018       0.000       0.002\nNORDEM               -0.0108      0.003     -3.364      0.003      -0.017      -0.004\nNUTS2                -0.0718      0.051     -1.399      0.176      -0.178       0.035\nC10001                0.0082      0.002      3.480      0.002       0.003       0.013\nC21011                4.9274      0.857      5.750      0.000       3.150       6.705\nC21031               -2.0528      0.951     -2.159      0.042      -4.024      -0.081\nC21041                0.2306      0.213      1.084      0.290      -0.211       0.672\nC21061               -1.6767      0.730     -2.298      0.031      -3.190      -0.164\nC21081                8.8434      4.456      1.985      0.060      -0.397      18.084\nC21091               -7.3059      1.433     -5.097      0.000     -10.278      -4.333\nC21101               -1.4882      0.965     -1.542      0.137      -3.490       0.514\nC21111               -1.0788      0.451     -2.394      0.026      -2.013      -0.144\nC21121               -0.1928      0.090     -2.150      0.043      -0.379      -0.007\nC21131                1.5267      1.035      1.475      0.154      -0.619       3.673\nC21141                0.2519      0.456      0.553      0.586      -0.693       1.197\nC21151               -2.7579      0.354     -7.788      0.000      -3.492      -2.023\nC21161                6.1177      5.197      1.177      0.252      -4.660      16.895\nC21171               -0.3612      0.120     -2.998      0.007      -0.611      -0.111\nC21181                0.4058      0.480      0.845      0.407      -0.590       1.402\nC21191               -0.3214      0.457     -0.703      0.490      -1.270       0.627\nC21201                0.3619      0.482      0.750      0.461      -0.638       1.362\nC21211                5.3379      3.315      1.610      0.122      -1.537      12.213\nC21221               -2.4970      1.093     -2.285      0.032      -4.763      -0.231\nC21231               -0.8779      0.215     -4.077      0.000      -1.324      -0.431\nC21241                0.5824      0.216      2.692      0.013       0.134       1.031\nC21261               -2.2529      2.856     -0.789      0.439      -8.176       3.670\nC21271                0.4035      0.320      1.259      0.221      -0.261       1.068\nC21281                0.1880      1.389      0.135      0.894      -2.692       3.068\nC21291                1.1723      0.320      3.668      0.001       0.510       1.835\nC21301               -0.0261      0.739     -0.035      0.972      -1.558       1.506\nC21311               -0.5173      0.228     -2.272      0.033      -0.989      -0.045\nC21321                0.1573      0.659      0.239      0.814      -1.209       1.524\nC21331                0.1142      0.091      1.259      0.221      -0.074       0.302\nC21341                0.6779      0.375      1.806      0.085      -0.101       1.457\nC21351               -0.4512      0.230     -1.959      0.063      -0.929       0.026\nC21361                0.8000      0.115      6.943      0.000       0.561       1.039\nC21371                0.1325      0.114      1.167      0.256      -0.103       0.368\nC21381               -0.2458      0.047     -5.195      0.000      -0.344      -0.148\nC21391                0.0751      0.588      0.128      0.899      -1.145       1.295\nC21401               -0.2867      0.209     -1.370      0.185      -0.721       0.147\nC21411               -4.0923      0.862     -4.748      0.000      -5.880      -2.305\nC21441            -1.082e-15   8.85e-16     -1.223      0.234   -2.92e-15    7.52e-16\nC22001               -0.2374      0.076     -3.129      0.005      -0.395      -0.080\nC31021                0.0500      0.102      0.490      0.629      -0.162       0.262\nC31031               -0.4693      0.082     -5.756      0.000      -0.638      -0.300\nC31041                0.0147      0.020      0.731      0.473      -0.027       0.057\nC31051                0.2098      0.209      1.006      0.326      -0.223       0.642\nC31061                0.1607      0.079      2.047      0.053      -0.002       0.324\nC31071                0.4654      0.147      3.158      0.005       0.160       0.771\n==============================================================================\nOmnibus:                        3.460   Durbin-Watson:                   2.030\nProb(Omnibus):                  0.177   Jarque-Bera (JB):                3.515\nSkew:                          -0.073   Prob(JB):                        0.172\nKurtosis:                       4.080   Cond. No.                     4.42e+19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.7e-31. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n5.3.1.1 Regressão com Scikit\n\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC22001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n4.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n7.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n\n\n\n\n5 rows × 52 columns\n\n\n\n\n# seleciona para remover as colunas ano e ordem\nto_drop = ['ANO','NORDEM']\ndf = df_hosp.drop(columns=to_drop, axis=1) # drop das colunas ano e ordem\n\n\ndf1 = df.dropna() # drop das linhas com valores missing\n\n\n# define a variável target e as features\nX = df1.drop(columns=['C31011'])\ny = df1['C31011'].values\n\n\n## Typecast da coluna para categoria em pandas\nX['NUTS2'] = pd.Categorical(X.NUTS2)\n#X.dtypes\n\nX.shape\n\n(71, 49)\n\n\n\n# cria variáveis dummy e faz drop da baseline\nX = pd.get_dummies(X, drop_first = True)\n\nX.shape\n\n(71, 54)\n\n\n\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\n\nthresholder = VarianceThreshold(threshold=.2) # define o threshold de variância \nX = thresholder.fit_transform(X) # aplica o threshold para excluir variáveis com baixa variância\n\nX.shape\n\n(71, 45)\n\n\n\nfrom sklearn.model_selection import train_test_split \n\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, test_size = 0.2 ,random_state = 2002)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(56, 45)\n(15, 45)\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train) # escalar os dados em pre-processamento\n\nscaler.mean_\nprint()\nscaler.scale_\n\n\n\n\narray([8.74428584e+02, 2.52133751e+00, 1.33618689e+00, 6.04110072e+00,\n       2.07849779e+00, 9.99840549e-01, 1.31852964e+00, 2.49463711e+00,\n       3.02772057e+00, 1.94667430e+00, 2.03155904e+00, 3.17756806e+00,\n       9.61303258e+00, 2.59414569e+00, 1.79275398e+00, 2.14991398e+00,\n       6.67914436e-01, 1.98198198e+00, 3.08634253e+00, 7.65777916e-01,\n       3.85710152e+00, 2.14575697e+00, 3.19473219e+00, 1.56817846e+00,\n       4.09205302e+00, 2.73115061e+00, 5.38442458e+00, 2.92197646e+00,\n       5.15425325e+00, 1.15123603e+01, 6.52663304e+00, 8.26658429e+00,\n       9.99362041e-01, 4.31888207e+00, 2.01777813e+00, 7.27395322e+00,\n       2.81339215e+01, 1.19658848e+01, 6.17888108e+00, 4.58535824e+00,\n       1.65205114e+00, 4.30397894e+00, 2.98160816e+00, 4.42842742e-01,\n       4.79157424e-01])\n\n\n\nX_scaled = scaler.transform(X_train)\n\nX_train\n\nprint()\n\nX_scaled\n\n\n\n\narray([[ 5.24932601,  4.75937868,  4.12955491, ..., -0.48511692,\n        -0.60485838, -1.34164079],\n       [-0.4012212 , -0.39661489, -0.3608349 , ..., -0.14972744,\n        -0.60485838,  0.74535599],\n       [ 1.30961034,  1.18984467, -0.3608349 , ...,  1.52721994,\n        -0.60485838,  0.74535599],\n       ...,\n       [-0.52930484, -0.39661489, -0.3608349 , ..., -0.48511692,\n        -0.60485838,  0.74535599],\n       [-0.1702132 , -0.39661489, -0.3608349 , ...,  0.18566203,\n        -0.60485838,  0.74535599],\n       [-0.70427625, -0.39661489, -0.3608349 , ..., -0.14972744,\n         1.65327957, -1.34164079]])\n\n\n\nfrom sklearn.linear_model import LinearRegression \n\nlr = LinearRegression()\nlr.fit(X_scaled,y_train) # treina o modelo\n\nlr.coef_\nprint()\ny_pred = lr.predict(X_test) # faz a previsão mas não com os dados escalados\n\n\n\n\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n-45066.18373373444\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred)\nplt.xlim(0, 10)\nplt.ylim(0, 1000)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nfizemos o scale dos dados de train mas não dos dados de teste.\n\n\nVamos tentar de outra forma\n\n# define a variável target e os predictors\nX_2nd = df1.drop(columns=['C31011'])\ny_2nd = df1['C31011'].values\n\n\n## Typecast da coluna para categoria em pandas\nX_2nd['NUTS2'] = pd.Categorical(X_2nd.NUTS2)\n# cria variáveis dummy e faz drop da baseline\nX_2nd = pd.get_dummies(X_2nd, drop_first = True)\n\n\n#Split data for machine learning\nX_2nd_train, X_2nd_test, y_2nd_train, y_2nd_test = train_test_split(X_2nd,  y_2nd, test_size = 0.2 ,random_state = 2002)\nprint(X_2nd_train.shape)\nprint(X_2nd_test.shape)\n\n(56, 54)\n(15, 54)\n\n\n\nlr2 = LinearRegression()\nlr2.fit(X_2nd_train,y_2nd_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nlr2.coef_\n\narray([ 1.12385995e-16, -1.45716772e-15, -1.75554016e-14,  9.52016244e-15,\n       -7.43849426e-15, -1.13381526e-13,  2.98538971e-13,  9.90596494e-14,\n       -4.58522109e-14, -1.13797860e-15, -1.36890499e-13,  8.04911693e-16,\n        9.93649607e-15,  6.64468480e-14, -3.02535774e-15, -7.33657926e-15,\n       -2.55351296e-15, -3.55271368e-15,  5.83977311e-14,  4.89712437e-14,\n       -5.42621503e-15,  3.33066907e-15,  1.65534253e-13, -1.52100554e-14,\n       -1.09690035e-13,  8.93729535e-15,  6.70297151e-14, -2.44249065e-15,\n        1.72362125e-14, -3.02535774e-15,  2.22044605e-16, -4.88498131e-15,\n        1.55431223e-15, -3.94129174e-15,  1.77635684e-15, -7.53563878e-15,\n       -1.02140518e-14, -3.44724249e-14,  2.77555756e-16,  0.00000000e+00,\n        1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.39055434e-14,\n        2.06779038e-15, -8.94423424e-15,  7.41073869e-15, -5.88973315e-14,\n        6.56259769e-13,  2.09728068e-15])\n\n\n\ny_2nd_pred = lr2.predict(X_2nd_test)\n\n\n# we choose the x axis as index, chossing year will give a discrete plot\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_2nd_test, y= y_2nd_pred)\nplt.xlim(-2, 20)\nplt.ylim(-2, 20)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\n\ny_2nd_test\n\ny_2nd_pred\n\nr2_score(y_2nd_test, y_2nd_pred)\n\n1.0\n\n\ntemos de excluir C31001!!\n\n  # define a variável target e os predictors\nX_3rd = df1.drop(columns=['C31011', 'C31001'])\ny_3rd = df1['C31011'].values\n\n\n## Typecast da coluna para categoria em pandas\nX_3rd['NUTS2'] = pd.Categorical(X_3rd.NUTS2)\n# cria variáveis dummy e faz drop da baseline\nX_3rd = pd.get_dummies(X_3rd, drop_first = True)\n\n\n#Split data for machine learning\nX_3rd_train, X_3rd_test, y_3rd_train, y_3rd_test = train_test_split(X_3rd,  y_3rd, test_size = 0.2 ,random_state = 2002)\nprint(X_3rd_train.shape)\nprint(X_3rd_test.shape)\n\n(56, 53)\n(15, 53)\n\n\n\nlr3 = LinearRegression()\nlr3.fit(X_3rd_train,y_3rd_train)\nlr3.coef_\n\narray([-2.89065165e-03,  5.41320479e+00,  7.52508894e+00, -2.51991314e+00,\n       -1.38806027e+00,  3.89513026e+01, -6.38321113e+01, -1.85914497e+01,\n        1.39598080e+01,  1.13122965e-01,  2.48179866e+01,  2.76124915e+00,\n       -9.72914071e+00,  1.57918396e+01,  8.85392279e-01,  1.35181584e+00,\n        1.49772845e-01,  1.77607406e+00, -2.73519188e+01, -1.50593912e+01,\n        1.73988830e-01,  1.28398713e+00, -1.93473069e+01,  3.98023055e+00,\n        1.29523386e+01, -3.31543616e+00, -1.49145538e+01, -3.28300461e-02,\n       -2.80431048e+00,  7.84716044e-01,  1.85229891e+00,  2.71964178e+00,\n       -2.34258012e-01,  9.26484042e-01, -4.03718916e-01, -8.32495907e-01,\n        1.32607411e+00, -1.49737685e+00, -1.67066361e-12,  5.15923351e-01,\n       -8.48951953e-01, -1.25860009e-01,  1.97142581e-01, -2.58385488e-01,\n       -4.37086520e-02, -1.17080763e-01, -3.85317775e+00,  4.22138930e-01,\n        1.70055611e+00, -7.94168448e+00,  8.94329198e+00, -1.63172288e+02,\n        1.23030119e+00])\n\n\n\ny_3rd_pred = lr3.predict(X_3rd_test)\n\n\nr2_score(y_3rd_test, y_3rd_pred)\n\n-816.2729091062955\n\n\n\n\n5.3.1.2 Avaliação dos modelos de regressão\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n\n\nprint(\"                   MAE             MSE           RMSE          MedAE         R2\")\nprint(\"     1ª Tentativa {:.12f} {:.10f} {:.8f} {:.8f} {:.8f}\"\n        .format(mean_absolute_error(y_test, y_pred),\n                mean_squared_error(y_test,y_pred),\n                mean_squared_error(y_test,y_pred,squared=False), # dá a raiz quadrada do MSE\n                median_absolute_error(y_test,y_pred),\n                r2_score(y_test,y_pred)))\nprint(\"     2º Tentativa: {:.12f} {:.10f} {:.10f} {:.10f} {:.12f}\"\n        .format(mean_absolute_error(y_2nd_test, y_2nd_pred),\n                mean_squared_error(y_2nd_test,y_2nd_pred),\n                mean_squared_error(y_2nd_test,y_2nd_pred,squared=False),\n                median_absolute_error(y_2nd_test,y_2nd_pred),\n                r2_score(y_2nd_test,y_2nd_pred)))\nprint(\"     3º Tentativa: {:.11f} {:.8f} {:.8f} {:.8f} {:.8f}\"\n        .format(mean_absolute_error(y_3rd_test, y_3rd_pred),\n                mean_squared_error(y_3rd_test,y_3rd_pred),\n                mean_squared_error(y_3rd_test,y_3rd_pred,squared=False),\n                median_absolute_error(y_3rd_test,y_3rd_pred),\n                r2_score(y_3rd_test,y_3rd_pred)))\n\n                   MAE             MSE           RMSE          MedAE         R2\n     1ª Tentativa 553.426660102138 603699.9634376692 776.98131473 364.80029201 -45066.18373373\n     2º Tentativa: 0.000000000000 0.0000000000 0.0000000000 0.0000000000 1.000000000000\n     3º Tentativa: 48.28875523857 10947.82465798 104.63185298 5.86979198 -816.27290911\n\n\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n\n\n\n\n\n\n\n\nWarning\n\n\n\no valor negativo de R2 indica que o modelo é pior que um modelo constante\n\n\n\n\n5.3.1.3 Gravar o modelo\n\nimport pickle\n\n# escolher o nome do ficheiro\nfilename = \"data\\linearRegression_SK.pickle\"\n\n# gravar o modelo\npickle.dump(lr3, open(filename, \"wb\"))\n\n\n# fazer load do modelo\nloaded_model = pickle.load(open(filename, \"rb\"))\n\n\n# ve rificar que conseguimos carregar o modelo gravado\nloaded_model.coef_\n\n\n\n\n5.3.2 Classificação\nQUESTÃO:\nCom este conjunto de dados que inclui o número de médicos e enfermeiros em várias especialidades será que consigo estimar se essa unidade em particular tem serviço de cirurgia?\nColuna t_cirurgia criada a partir da coluna C21071 que deve ser retirada depois de criada a etiqueta.\n\n5.3.2.1 Classification Trees\n\nimport numpy as np\nimport pandas as pd\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\2009169450.py:4: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\nANO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012.0\n229.0\n17.0\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\nyes\n\n\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\nno\n\n\n2012.0\n65.0\n16.0\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\nyes\n\n\n2012.0\n106.0\n17.0\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\nyes\n\n\n2012.0\n209.0\n11.0\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\nyes\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ndf_hosp = df_hosp.reset_index() # \n\ndf = df_hosp.drop(columns='C21071', axis=1)  # drop da coluna C21071\n\ndf['t_cirurgia'] = df['t_cirurgia'].fillna(0) # preencher os missing values com 0\n\ndf['t_cirurgia'] = df['t_cirurgia'].replace({'yes': 1, 'no': 0}) # substituir os valores yes e no por 1 e 0\n\ndf.head()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12592\\1416912252.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df['t_cirurgia'] = df['t_cirurgia'].replace({'yes': 1, 'no': 0}) # substituir os valores yes e no por 1 e 0\n\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n1\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n0\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n1\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n1\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n1\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ndf1 = df.dropna() # drop das linhas com valores missing\n\ndf1.describe()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\n\n\ncount\n86.0\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n...\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n\n\nmean\n2012.0\n118.081395\n14.976744\n642.744186\n113.872093\n73.979651\n0.813953\n6.941860\n0.360465\n2.953488\n...\n22.941860\n3.127907\n6.220930\n4.476744\n3.465116\n0.965116\n3.465116\n1.220930\n189.348837\n0.488372\n\n\nstd\n0.0\n68.264230\n3.620129\n809.928453\n172.413511\n108.256016\n2.144825\n10.384779\n1.146994\n5.346592\n...\n26.911775\n4.752204\n10.678622\n5.740930\n8.559409\n1.482825\n4.421066\n2.783993\n240.396475\n0.502797\n\n\nmin\n2012.0\n3.000000\n11.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2012.0\n57.500000\n11.000000\n120.250000\n7.750000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n3.250000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n30.000000\n0.000000\n\n\n50%\n2012.0\n119.000000\n16.000000\n338.500000\n42.500000\n33.000000\n0.000000\n2.000000\n0.000000\n0.000000\n...\n10.000000\n0.000000\n0.000000\n3.000000\n1.000000\n0.000000\n2.000000\n0.000000\n85.500000\n0.000000\n\n\n75%\n2012.0\n175.750000\n17.000000\n872.500000\n162.500000\n115.500000\n0.000000\n10.750000\n0.000000\n3.750000\n...\n35.750000\n4.000000\n11.000000\n6.000000\n3.000000\n1.000000\n5.000000\n1.000000\n282.250000\n1.000000\n\n\nmax\n2012.0\n229.000000\n30.000000\n5325.000000\n1161.000000\n719.000000\n13.000000\n51.000000\n6.000000\n31.000000\n...\n103.000000\n21.000000\n50.000000\n28.000000\n72.000000\n6.000000\n19.000000\n15.000000\n1515.000000\n1.000000\n\n\n\n\n8 rows × 62 columns\n\n\n\n\n# conta o nº de diferentes valores na coluna\ndf1['t_cirurgia'].nunique()\n\nprint()\n# verifica se há nulos no dataframe\ndf1.isnull().any()\n\n\n\n\nANO           False\nNORDEM        False\nNUTS2         False\nC10001        False\nC20001        False\n              ...  \nC31051        False\nC31061        False\nC31071        False\nC32001        False\nt_cirurgia    False\nLength: 62, dtype: bool\n\n\n\n5.3.2.1.1 Information Gain - Entropia\n\n# função para calcular a entropia\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef compute_impurity(feature, impurity_criterion): # função para calcular a impureza de uma feature\n    \"\"\"\n    This function calculates impurity of a feature.\n    Supported impurity criteria: 'entropy', 'gini'\n    input: feature (this needs to be a Pandas series)\n    output: feature impurity\n    \"\"\"\n    probs = feature.value_counts(normalize=True)\n    \n    if impurity_criterion == 'entropy':\n        impurity = -1 * np.sum(np.log2(probs) * probs)\n    elif impurity_criterion == 'gini':\n        impurity = 1 - np.sum(np.square(probs))\n    else:\n        raise ValueError('Unknown impurity criterion')\n        \n    return(round(impurity, 3))\n\n\n# Exemplos\nprint('impurity using entropy:', compute_impurity(df1['t_cirurgia'], 'entropy'))\nprint('impurity using gini index:', compute_impurity(df1['t_cirurgia'], 'gini'))\n\nimpurity using entropy: 1.0\nimpurity using gini index: 0.5\n\n\n\nfor level in df1['NUTS2'].unique(): # loop sobre os níveis da feature NUTS2\n    print('level name:', level)\n    df_feature_level = df1[df1['NUTS2']== level]\n    print('corresponding data partition:')\n    print(df_feature_level.head(5))\n    print('partition target feature impurity:', compute_impurity(df_feature_level['t_cirurgia'], 'entropy'))\n    print('partition weight:', str(len(df_feature_level)) + '/' + str(len(df1)))\n    print('====================')\n\nlevel name: 17.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n0   2012.0   229.0   17.0  1458.0   247.0   159.0     2.0    11.0     0.0   \n1   2012.0   206.0   17.0   144.0     0.0     0.0     0.0     0.0     0.0   \n3   2012.0   106.0   17.0   801.0   166.0   108.0     0.0    12.0     0.0   \n21  2012.0   151.0   17.0  2587.0   529.0   337.0     3.0    32.0     0.0   \n27  2012.0    12.0   17.0    71.0     7.0     7.0     0.0     0.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n0      5.0  ...    39.0     8.0    19.0     2.0     2.0     3.0     5.0   \n1      0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n3      7.0  ...    36.0     4.0    16.0     5.0     2.0     2.0     7.0   \n21    14.0  ...    40.0     8.0    11.0     6.0     3.0     0.0     0.0   \n27     0.0  ...     4.0     0.0     0.0     0.0     4.0     0.0     0.0   \n\n    C31071  C32001  t_cirurgia  \n0      0.0   454.0           1  \n1      0.0    46.0           0  \n3      0.0   228.0           1  \n21    12.0   793.0           0  \n27     0.0     8.0           1  \n\n[5 rows x 62 columns]\npartition target feature impurity: 1.0\npartition weight: 20/86\n====================\nlevel name: 16.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n2   2012.0    65.0   16.0   894.0   111.0   38.25     0.0     8.0     0.0   \n5   2012.0   115.0   16.0  1461.0   246.0  171.00     2.0    20.0     0.0   \n6   2012.0   189.0   16.0   136.0    10.0    8.00     0.0     0.0     0.0   \n7   2012.0   199.0   16.0  2032.0   483.0  299.00     2.0    30.0     2.0   \n11  2012.0    88.0   16.0   190.0    17.0   15.00     0.0     2.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n2      5.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n5      6.0  ...    48.0    11.0    21.0     3.0     8.0     1.0     3.0   \n6      0.0  ...     5.0     2.0     0.0     1.0     0.0     1.0     1.0   \n7     12.0  ...   103.0    21.0    26.0    17.0    18.0     4.0    14.0   \n11     2.0  ...    10.0     2.0     0.0     1.0     1.0     1.0     5.0   \n\n    C31071  C32001  t_cirurgia  \n2      0.0   337.0           1  \n5      1.0   518.0           1  \n6      0.0    53.0           1  \n7      3.0   577.0           1  \n11     0.0    76.0           1  \n\n[5 rows x 62 columns]\npartition target feature impurity: 0.99\npartition weight: 25/86\n====================\nlevel name: 11.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n4   2012.0   209.0   11.0   221.0    13.0    13.0     0.0     0.0     0.0   \n8   2012.0    57.0   11.0   586.0    86.0    51.0     0.0     5.0     0.0   \n9   2012.0   148.0   11.0   262.0    17.0    13.0     0.0     1.0     0.0   \n10  2012.0   124.0   11.0   293.0    36.0    33.0     0.0     4.0     0.0   \n20  2012.0    14.0   11.0   392.0    53.0    33.0     0.0     0.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n4      0.0  ...     8.0     0.0     0.0     1.0     0.0     1.0     6.0   \n8      0.0  ...    53.0     7.0    12.0    14.0     7.0     4.0     7.0   \n9      0.0  ...    17.0     0.0     0.0     7.0     0.0     0.0    10.0   \n10     1.0  ...    15.0     4.0     0.0     7.0     0.0     1.0     3.0   \n20     0.0  ...    76.0     0.0     0.0     0.0    72.0     2.0     2.0   \n\n    C31071  C32001  t_cirurgia  \n4      0.0    80.0           1  \n8      2.0   184.0           1  \n9      0.0    73.0           0  \n10     0.0    91.0           1  \n20     0.0    52.0           0  \n\n[5 rows x 62 columns]\npartition target feature impurity: 1.0\npartition weight: 30/86\n====================\nlevel name: 30.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n13  2012.0   205.0   30.0   515.0    83.0    57.0     0.0     0.0     0.0   \n14  2012.0   131.0   30.0   169.0     1.0     1.0     0.0     0.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n13     0.0  ...    10.0     0.0     0.0     3.0     2.0     0.0     5.0   \n14     0.0  ...     3.0     0.0     0.0     1.0     1.0     0.0     1.0   \n\n    C31071  C32001  t_cirurgia  \n13     0.0   187.0           1  \n14     0.0    48.0           0  \n\n[2 rows x 62 columns]\npartition target feature impurity: 1.0\npartition weight: 2/86\n====================\nlevel name: 20.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n15  2012.0    41.0   20.0   488.0    46.0    30.0     0.0     3.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n15     2.0  ...    19.0     1.0     8.0     7.0     0.0     0.0     1.0   \n\n    C31071  C32001  t_cirurgia  \n15     2.0   107.0           1  \n\n[1 rows x 62 columns]\npartition target feature impurity: -0.0\npartition weight: 1/86\n====================\nlevel name: 18.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n16  2012.0   130.0   18.0   415.0    43.0    34.0     0.0     3.0     0.0   \n34  2012.0   105.0   18.0  1359.0   251.0   146.0     2.0    14.0     1.0   \n50  2012.0   223.0   18.0  1454.0   273.0   154.0     3.0    11.0     0.0   \n53  2012.0   224.0   18.0   800.0    87.0    46.0     0.0     2.0     0.0   \n54  2012.0   190.0   18.0   384.0    35.0    30.0     0.0     3.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n16     0.0  ...    12.0     0.0     1.0     5.0     0.0     0.0     0.0   \n34     8.0  ...    75.0    14.0    17.0    13.0     9.0     4.0    10.0   \n50    10.0  ...    72.0    12.0    18.0    13.0    14.0     2.0    12.0   \n53     0.0  ...    36.0     4.0    10.0     3.0    10.0     6.0     3.0   \n54     0.0  ...    14.0     1.0     2.0     4.0     1.0     3.0     2.0   \n\n    C31071  C32001  t_cirurgia  \n16     6.0   153.0           0  \n34     8.0   382.0           0  \n50     1.0   411.0           0  \n53     0.0   235.0           1  \n54     1.0   110.0           1  \n\n[5 rows x 62 columns]\npartition target feature impurity: 0.918\npartition weight: 6/86\n====================\nlevel name: 15.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n41  2012.0   208.0   15.0   183.0     5.0     5.0     0.0     0.0     0.0   \n42  2012.0   164.0   15.0  1391.0   172.0   118.0     1.0    11.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n41     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n42     2.0  ...    34.0     9.0    13.0     6.0     4.0     0.0     0.0   \n\n    C31071  C32001  t_cirurgia  \n41     0.0    71.0           1  \n42     2.0   492.0           1  \n\n[2 rows x 62 columns]\npartition target feature impurity: -0.0\npartition weight: 2/86\n====================\n\n\n\n# função para calcular o information gain\ndef comp_feature_information_gain(df, target, descriptive_feature, split_criterion): \n    \"\"\"\n    This function calculates information gain for splitting on \n    a particular descriptive feature for a given dataset\n    and a given impurity criteria.\n    Supported split criterion: 'entropy', 'gini'\n    \"\"\"\n    \n    print('target feature:', target)\n    print('descriptive_feature:', descriptive_feature)\n    print('split criterion:', split_criterion)\n            \n    target_entropy = compute_impurity(df[target], split_criterion)\n    print('the target entropy is', target_entropy)\n\n    # we define two lists below:\n    # entropy_list to store the entropy of each partition\n    # weight_list to store the relative number of observations in each partition\n    entropy_list = list()\n    weight_list = list()\n    \n    # loop over each level of the descriptive feature\n    # to partition the dataset with respect to that level\n    # and compute the entropy and the weight of the level's partition\n    for level in df[descriptive_feature].unique():\n        df_feature_level = df[df[descriptive_feature] == level]\n        entropy_level = compute_impurity(df_feature_level[target], split_criterion)\n        entropy_list.append(round(entropy_level, 3))\n        weight_level = len(df_feature_level) / len(df)\n        print('the level is {} and the weight is {}'.format(level, weight_level))\n        weight_list.append(round(weight_level, 3))\n\n    print('impurity of partitions:', entropy_list)\n    print('weights of partitions:', weight_list)\n\n    feature_remaining_impurity = np.sum(np.array(entropy_list) * np.array(weight_list))\n    print('remaining impurity:', feature_remaining_impurity)\n    \n    information_gain = target_entropy - feature_remaining_impurity\n    print('information gain:', information_gain)\n    \n    print('====================')\n\n    return(information_gain)\n\nVamos ver que a variável do Número de Pessoal ao Serviço - Total C10001 é bastante promissora para faser a divisão do espaço, e a variável do Número de Médicos - Total C20001 é bastante melhor do que o Número de Médicos - Especialistas - Anatomia Patológica - Total C21011\n\nsplit_criterion = 'entropy' # escolher o critério de divisão\nfor feature in df1.drop(columns=['t_cirurgia','ANO','NORDEM']).columns:\n    feature_info_gain = comp_feature_information_gain(df1, 't_cirurgia', feature, split_criterion) # calcular o information gain\n\n\n\n5.3.2.1.2 trees\n\n# define a variável target e os predictors\nX = df1.drop(columns=['t_cirurgia']) # dataframe sem a coluna target\ny = df1['t_cirurgia'].values # target em formato de array\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nclf = DecisionTreeClassifier(max_depth=3) # define a profundidade da árvore\n\nmodel = clf.fit(X,y) # treina o modelo\n\n\n#from sklearn import tree\n\ntext_representation = tree.export_text(clf)\nprint(text_representation) # visualização do modelo\n\n|--- feature_58 &lt;= 1.50\n|   |--- feature_48 &lt;= 0.50\n|   |   |--- feature_20 &lt;= 1.00\n|   |   |   |--- class: 0\n|   |   |--- feature_20 &gt;  1.00\n|   |   |   |--- class: 1\n|   |--- feature_48 &gt;  0.50\n|   |   |--- feature_1 &lt;= 179.50\n|   |   |   |--- class: 0\n|   |   |--- feature_1 &gt;  179.50\n|   |   |   |--- class: 1\n|--- feature_58 &gt;  1.50\n|   |--- feature_60 &lt;= 73.50\n|   |   |--- feature_51 &lt;= 57.50\n|   |   |   |--- class: 1\n|   |   |--- feature_51 &gt;  57.50\n|   |   |   |--- class: 0\n|   |--- feature_60 &gt;  73.50\n|   |   |--- feature_58 &lt;= 9.50\n|   |   |   |--- class: 1\n|   |   |--- feature_58 &gt;  9.50\n|   |   |   |--- class: 0\n\n\n\n\n# gravar o log da Árvore\nwith open(\"data\\decision_tree.log\", \"w\") as fout: # \n    fout.write(text_representation)\n\n\nimport matplotlib.pyplot as plt\n\n# class_names = True em vez da lista  faz print y(0) e y(1)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(clf, class_names= [\"No\",\"Yes\"], filled=True)\n\n\n\n\n\n# gravar a imagem da tree\n\nfig.savefig(\"images/decision_tree.png\")\n\n\n\n\n5.3.2.2 Classification com Scikit\n\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\nfrom sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder # For scaling variables\n\nfrom sklearn.model_selection import train_test_split,cross_val_score # For splitting data\nfrom sklearn.tree import DecisionTreeClassifier # For Decision Tree\n\n# For model evaluation\nfrom sklearn.metrics import accuracy_score,confusion_matrix \nfrom sklearn.metrics import roc_curve,roc_auc_score \n\n\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, \n                                                    test_size = 0.2 ,\n                                                    random_state = 1984)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(68, 61)\n(18, 61)\n\n\n\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.preprocessing import StandardScaler #for scaling variables\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\n\n#Define a pipeline\npipeline = Pipeline([\n('scaler', StandardScaler()),\n('selector', VarianceThreshold()),\n('TREE', DecisionTreeClassifier())])\n\n\n5.3.2.2.1 usando a Pipeline\n\nfrom sklearn import set_config # para configurar a visualização da pipeline\n\nset_config(display=\"diagram\")\npipeline\n\nPipeline(steps=[('scaler', StandardScaler()), ('selector', VarianceThreshold()),\n                ('TREE', DecisionTreeClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('scaler', StandardScaler()), ('selector', VarianceThreshold()),\n                ('TREE', DecisionTreeClassifier())])  StandardScaler?Documentation for StandardScalerStandardScaler()  VarianceThreshold?Documentation for VarianceThresholdVarianceThreshold()  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n# execução da pipeline com parameteros de omissão\npipeline.fit(X_train, y_train)\nprint('Training set score: ' + str(pipeline.score(X_train,y_train)))\nprint('Test set score: ' + str(pipeline.score(X_test,y_test)))\n\nprint()\ny_pred = pipeline.predict(X_test)\ny_pred\n\nprint()\ny_proba = pipeline.predict_proba(X_test)\ny_proba\n\nTraining set score: 1.0\nTest set score: 0.5555555555555556\n\n\n\n\narray([[0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.]])\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n           pred: No  pred: Yes\nreal: No          5          5\nreal: Yes         3          5\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\nprint('Precision: %.3f' % precision_score(y_test, y_pred))\nprint('Recall: %.3f' % recall_score(y_test, y_pred))\nprint('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n\nPrecision: 0.500\nRecall: 0.625\nAccuracy: 0.556\n\n\n\n\nfrom sklearn.metrics import roc_curve, balanced_accuracy_score\n# antes era plot_roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,\n                                       pipeline.predict_proba(X_test)[:,1],)\n\nRocCurveDisplay.from_estimator(pipeline, X_test, y_test)\n\nplt.show()\n\n\n\n\n\n\n5.3.2.2.2 sem Pipeline\n\nclf2 = DecisionTreeClassifier(max_depth=2) \n\nmodel2 = clf2.fit(X_train, y_train)\n\ny_class = model2.predict(X_test)\ny_class\n\nprint()\ny_class_proba = model2.predict_proba(X_test)\ny_class_proba\n\nprint()\nmodel.predict_proba(X_test)[:, 1]\n\ntree.plot_tree(clf2, class_names= [\"No\",\"Yes\"], filled=True)\n\n\n\n\n\n[Text(0.5, 0.8333333333333334, 'x[57] &lt;= 0.5\\ngini = 0.5\\nsamples = 68\\nvalue = [34, 34]\\nclass = No'),\n Text(0.25, 0.5, 'x[49] &lt;= 0.5\\ngini = 0.444\\nsamples = 36\\nvalue = [24, 12]\\nclass = No'),\n Text(0.375, 0.6666666666666667, 'True  '),\n Text(0.125, 0.16666666666666666, 'gini = 0.278\\nsamples = 18\\nvalue = [15, 3]\\nclass = No'),\n Text(0.375, 0.16666666666666666, 'gini = 0.5\\nsamples = 18\\nvalue = [9, 9]\\nclass = No'),\n Text(0.75, 0.5, 'x[1] &lt;= 15.5\\ngini = 0.43\\nsamples = 32\\nvalue = [10, 22]\\nclass = Yes'),\n Text(0.625, 0.6666666666666667, '  False'),\n Text(0.625, 0.16666666666666666, 'gini = 0.0\\nsamples = 4\\nvalue = [4, 0]\\nclass = No'),\n Text(0.875, 0.16666666666666666, 'gini = 0.337\\nsamples = 28\\nvalue = [6, 22]\\nclass = Yes')]\n\n\n\n\n\npodemos mudar o threshold\n\nthreshold = 0.7\ny_pred = (model2.predict_proba(X_test)[:, 1] &gt; threshold)\nconfusion_matrix(y_test, y_pred)\n\nprint()\ncm_thr70 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr70,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n\n           pred: No  pred: Yes\nreal: No          7          3\nreal: Yes         5          3\n\n\n\nfrom sklearn.metrics import roc_curve,RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,model2.predict_proba(X_test)[:,1],\n                                 drop_intermediate=False) # temos de fazer o unpack dos 3 resultados\n\nRocCurveDisplay.from_estimator(model,X_test,y_test)\n\nplt.show()\n\n\n\n\n\nprint(fpr)\nprint(tpr)\nprint(thresholds)\n\n[0.  0.3 0.6 1. ]\n[0.    0.375 0.75  1.   ]\n[       inf 0.78571429 0.5        0.16666667]\n\n\n\n\n5.3.2.2.3 gravar o modelo\n\n# escolher o nome do ficheiro\nfilename = \"data\\Tree.pickle\"\n\n# gravar o modelo\npickle.dump(model2, open(filename, \"wb\"))"
  },
  {
    "objectID": "600-mod6.html#dados-geográficos",
    "href": "600-mod6.html#dados-geográficos",
    "title": "6  Visualização de Dados Geográficos",
    "section": "6.1 Dados Geográficos",
    "text": "6.1 Dados Geográficos\nSistema de Informação Geográfica (SIG) é um sistema que permite manipular, armazenar, e fazer a análise espacial de dados geográficos.\n\n6.1.1 Tipo de dados\npontos - Point(2,10) linha - LineString([(1,2),(1,5),…]) polígonos - Polygon([(13,1),(14,4),…])\n Preparação do ambiente python:\natravés do prompt ANACONDA, instalar os packages: GEOPANDAS FOLIUM NBB\n### Coordenadas Geográficas\nLatitude e Longitude\nConhecer a Projeção Geográfica é importante para combinar dados de diferentes fontes de informação.\nCoordinate Reference System (CRS): define como as coordenadas são representadas no plano.\nEPSG Geodetic Parameter Dataset: registo global dos diferentes CRS.\nEPSG: European Petroleum Survey Group para identificar os diferentes sistemas de coordenadas.\nWGS84 - World Geodetic System 1984 associado ao sistema de posicionamento global (GPS).\n\nEPSG 4326\ncoordenadaas em graus decimais\n\nPseudo-Mercator/WGS84\n\nEPSG 3857\ncoordenadas em metros\n\n\n6.1.1.1 Exercicio\nMostrar Influencia do tipo de Projeção Geográfica\n\n# Este Script Permite Visualizar diferentes Projeções\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Definir Figura do MATPLOTLIB\n# Conseguem mostrar mais SUBPLOTS alterando argumeto 2 (1,2)\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n# Carregar a geogrfia do mundo a partir d eum shapefile dentro um ficheiro ZIP\n# Atenção Mudar o caminho!\nworld = gpd.read_file(r\"data\\ne_110m_admin_0_countries.zip\")\n\n# # Alternativa - Ler os Dados de um Link GEOJSON:\n# url = \"http://d2ad6b4ur7yvpq.cloudfront.net/naturalearth-3.3.0/ne_110m_land.geojson\"\n# # Ler Ficheiro GEOJSON do URL\n# world = gpd.read_file(url)\n\n# Sistema de coordenadas definidos na listagem em baixo:\n# \n# WGS 84 (Lat/Lon): ['EPSG:4326','WGS 84 (Lat/Lon)']\n# Web Mercator Metros: ['EPSG:3857','Web Mercator']\n# Vander Grinten: ['ESRI:54029','World Robinson']\n# Robinson: ['ESRI:54030','World Robinson - National Geographic']\n# Gall-Peters - ['ESRI:53016','Gall Stereographic']\n# Peirce Quincuncial: ['ESRI:54091','Peirce quincuncial North Pole in a square']\n# winkel-Tripel: ['ESRI:53018','Sphere Winkel I']\n# Goode Homolosine (Molweide) - ['EPSG:7619','Interrupted Goode Homolonsine']\n# Albers Equal Area: ['ESRI:102008','North America Albers Equal Area Conic']\n# Eckert: ['ESRI:53010','Sphere Eckert VI']\n# Portugal Continental: ['EPSG:3763','Portugal Continental: PT-TM06/ETRS89']\n# Açores UTM Fuso 26: ['EPSG:5015','TRF93 / PTRA08 - UTM fuso 26 - Grupo Central e Oriental do Arquipélago dos Açores']\n\n# sistema de Coordenadas para Visualizar - 12 exemplos:\nsistcoord = [['EPSG:4326','WGS 84 (Lat/Lon)'], ['EPSG:3857','Web Mercator'],\n             ['ESRI:54029','World Robinson'], ['ESRI:54030','World Robinson - National Geographic'], \n             ['ESRI:53016','Gall Stereographic'], ['ESRI:54091','Peirce quincuncial North Pole in a square'],\n             ['ESRI:53018','Sphere Winkel I'],['EPSG:7619','Interrupted Goode Homolonsine'],\n             ['ESRI:102008','North America Albers Equal Area Conic'], ['ESRI:53010','Sphere Eckert VI'],\n             ['EPSG:3763','Portugal Continental: PT-TM06/ETRS89'], ['EPSG:5015','TRF93 / PTRA08 - UTM fuso 26 - Grupo Central e Oriental do Arquipélago dos Açores']]\n\n\n# Para mostrar\nmapa1 = 0\nmapa2 = 11\n\n# ------------------------------------------\n# Mudar Projeçoes na indicação da Listagem\n#world.to_crs(epsg=2264).plot(ax=ax[0])\n\nworld.to_crs(sistcoord[mapa1][0]).plot(ax=ax[0])\nax[0].set_title(sistcoord[mapa1][1])\n\n#world.to_crs(epsg=4087).plot(ax=ax[1])\nworld.to_crs(sistcoord[mapa2][0]).plot(ax=ax[1])\nax[1].set_title(sistcoord[mapa2][1])\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n6.1.2 GeoPandas e MatPlotLib\nAs mesmas funcionalidades do package Pandas podem ser usadas com o GeoPandas, usa também outros packages como Shapely e Fiona\n\n\n\n6.1.3 Criar dados GeoPandas\n\n  # Importar GeoPandas\nimport geopandas as gpd\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\NUTS3_2015_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_nuts3 = gpd.read_file(file_path, encoding=encoding)\n\nprint(gdf_nuts3.info())\nprint(gdf_nuts3.head())\ngdf_nuts3.loc[1,'geometry']\n\n#gdf_nuts3.head()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 25 entries, 0 to 24\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       25 non-null     object  \n 1   NUTS2       25 non-null     object  \n 2   NUTS3       25 non-null     object  \n 3   NUTS3_DSG   25 non-null     object  \n 4   SUM_AREA_K  25 non-null     float64 \n 5   Shape_Leng  25 non-null     float64 \n 6   Shape_Area  25 non-null     float64 \n 7   geometry    25 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.7+ KB\nNone\n  NUTS1 NUTS2 NUTS3                    NUTS3_DSG   SUM_AREA_K     Shape_Leng  \\\n0     1    11   111                   Alto Minho  4005.236217  348704.286332   \n1     1    11   112                       Cávado  2230.779298  308604.284583   \n2     1    11   119                          Ave  2589.276604  418041.964330   \n3     1    11   11A  Área Metropolitana do Porto  3596.793275  523879.094897   \n4     1    11   11B                  Alto Tâmega  5240.318314  473818.956252   \n\n     Shape_Area                                           geometry  \n0  4.006183e+09  MULTIPOLYGON (((-911484.585 5182506.803, -9118...  \n1  2.231519e+09  POLYGON ((-895962.526 5133087.503, -895581.858...  \n2  2.589188e+09  POLYGON ((-893252.407 5115827.796, -892566.222...  \n3  3.597098e+09  POLYGON ((-974333.659 5081342.266, -973571.650...  \n4  5.239871e+09  POLYGON ((-880130.085 5150064.989, -878083.752...  \n\n\n\n\n\nMostrar Objecto Geometry\n\n# Informação de um Polygon\n# print(gdf_nuts3.loc[1,'geometry'])\n\nObjecto de Geometria\n\n# Tipo Objecto GeoPandas\nprint (type(gdf_nuts3))\n# Tipo de Dados da coluna de Geometria\nprint (type(gdf_nuts3.geometry))\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;\n\n\nVisualizar GDF\n\n# Mostrar Geografia\n#import geopandas as gpd\nimport matplotlib.pyplot as plt\n\ngdf_nuts3.plot(column = 'NUTS3_DSG',\n              legend = False)\nplt.show()\n\n\n\n\nSelecionar registos dum GDF\n\n# Selecção utilizando os operadores comuns\n# Selecção pode ser efetuada com ou sem a função LOC\ngdf_nuts3_continente = gdf_nuts3[gdf_nuts3['NUTS3'] &lt; '2']\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '11']\n\n# Mostrar numero registos selecionados\nprint(\"Nº registos Continente\", len(gdf_nuts3_continente))\nprint(\"Nº registos NUTS2 11\", len(gdf_nuts3_sel))\n\n# Selecionar utilizando comparação de um String\nselected_rows_like = gdf_nuts3[gdf_nuts3['NUTS3'].str.startswith('11')]\nprint('Nº registos que começam com 11',len(selected_rows_like))\n                        \n# Selecionar Linhas quando existem numa Listagem\nlista_selecao = ['11A', '11E','16E','170']\nselected_rows = gdf_nuts3[gdf_nuts3.NUTS3.isin(lista_selecao)] \nprint('Nº registos que estão numa lista - vs 1',len(selected_rows))\nselected_rows = gdf_nuts3.loc[gdf_nuts3.NUTS3.isin(lista_selecao)] \nprint('Nº registos que estão numa lista - vs 2',len(selected_rows))\n\n# Selecao dos Dados a Mostrar\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '11']\ngdf_nuts3_sel.plot(column = 'NUTS3_DSG',\n              legend = True,\n                  edgecolor = 'black')\n\nNº registos Continente 23\nNº registos NUTS2 11 8\nNº registos que começam com 11 8\nNº registos que estão numa lista - vs 1 4\nNº registos que estão numa lista - vs 2 4\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nVisualização Interactiva com Explore\n\n#import geopandas as gpd\n\n# Selecao dos Dados a Mostrar\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '11']\ngdf_nuts3_sel.explore(column = 'NUTS3_DSG',\n              legend = True,\n                  edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nMostrar geografia\n\n# Mostrar Geografia\n\n# Mostrar Áreas NUTS3 Região Centro\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '16']\n\n# Definir Legenda \n# Definir Localização (loc), Localização em relação ao eixos (bbox_to_anchor), Nº colunas (ncol)\nlgnd_kwds = {'title': 'Nuts3 (Centro)',\n               'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 1}\n# Argumetnos: Atributo (column), Colormap (cmap), colocar Legenda (legend), \n#  Legenda definida (legend_kwds), \ngdf_nuts3_sel.plot(column = 'NUTS3_DSG',\n              cmap = 'tab20',\n              legend = True,\n              legend_kwds  = lgnd_kwds,\n              edgecolor = 'dimgray')\n\n# Rotulos  Eixos\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Áreas NUTS3 de Portugal Continental')\n\nplt.show()\n\n\n\n\n\n6.1.3.0.1 Exercício\n\n# packages necessários\nimport geopandas as gpd\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\CAOP20200_MN_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_caop = gpd.read_file(file_path, encoding=encoding)\n\nprint(gdf_caop.info())\nprint(gdf_caop.head())\nprint(gdf_caop.DISTRITO.unique()) \n\ngdf_caop.loc[1,'geometry']\n\nprint()\nprint(gdf_caop.NUTS3_02.unique()) # lista com a opções de NUTS3_02\n\n# cria nova coluna para legenda\ngdf_caop['DTMN2'] = gdf_caop['DTMNDSG'] + '(' + gdf_caop['DTMN'].astype(str) + ')'\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 308 entries, 0 to 307\nData columns (total 12 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   OBJECTID    308 non-null    int64   \n 1   DTMN        308 non-null    object  \n 2   DTMNDSG     308 non-null    object  \n 3   ILHA        30 non-null     object  \n 4   NUTS3_02    308 non-null    object  \n 5   NUTS3_15    308 non-null    object  \n 6   NUTS3_24    308 non-null    object  \n 7   OBJECTID_2  308 non-null    int64   \n 8   NUTS1_15    278 non-null    object  \n 9   NUTS2_15    278 non-null    object  \n 10  DISTRITO    278 non-null    object  \n 11  geometry    308 non-null    geometry\ndtypes: geometry(1), int64(2), object(9)\nmemory usage: 29.0+ KB\nNone\n   OBJECTID  DTMN                DTMNDSG                ILHA NUTS3_02  \\\n0         1  4901                  Corvo       Ilha do Corvo      200   \n1         2  4801       Lajes das Flores     Ilha das Flores      200   \n2         3  4802  Santa Cruz das Flores     Ilha das Flores      200   \n3         1  4201                  Lagoa  Ilha de São Miguel      200   \n4         2  4202               Nordeste  Ilha de São Miguel      200   \n\n  NUTS3_15 NUTS3_24  OBJECTID_2 NUTS1_15 NUTS2_15 DISTRITO  \\\n0    PT200      200           0     None     None     None   \n1    PT200      200           0     None     None     None   \n2    PT200      200           0     None     None     None   \n3      200      200           0     None     None     None   \n4      200      200           0     None     None     None   \n\n                                            geometry  \n0  POLYGON ((-3463610.566 4817991.232, -3463602.6...  \n1  POLYGON ((-3479397.272 4791980.698, -3479270.4...  \n2  POLYGON ((-3474231.695 4797050.671, -3474205.1...  \n3  POLYGON ((-2845070.611 4547832.086, -2845103.7...  \n4  POLYGON ((-2812174.924 4560067.448, -2812097.9...  \n[None 'Évora' 'Lisboa' 'Portalegre' 'Porto' 'Santarém' 'Castelo Branco'\n 'Faro' 'Viana do Castelo' 'Viseu' 'Coimbra' 'Setúbal' 'Vila Real'\n 'Aveiro' 'Braga' 'Guarda' 'Bragança' 'Beja' 'Leiria']\n\n['200' '183' '182' '16B' '115' '114' '185' '166' '150' '111' '165' '162'\n '16C' '181' '117' '118' '161' '116' '113' '164' '168' '167' '171' '172'\n '16A' '169' '184' '163' '112' '300']\n\n\nvizualisar com explore\n\ngdf_caop.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\ncontagens e filtros\n\n# conta o número de registos por NUTS3_02\nprint(gdf_caop.NUTS3_24.value_counts())\n\n# selecionar linhas a partir de uma lista\nlista_selecao = ['11A','11E','16E','170']\n\nselect_rows = gdf_caop.loc[gdf_caop.NUTS3_24.isin(lista_selecao)]\n\nprint('Nº de registos que estão na lista ', lista_selecao, len(select_rows))\n\nNUTS3_24\n200    19\n11D    19\n192    19\n11A    17\n150    16\n111    16\n1C3    15\n196    15\n194    14\n1C4    14\n1C2    13\n1D1    12\n191    11\n1D2    11\n1D3    11\n11C    11\n300    11\n193    10\n11E     9\n1A0     9\n1B0     9\n195     8\n119     8\n11B     6\n1C1     5\nName: count, dtype: int64\nNº de registos que estão na lista  ['11A', '11E', '16E', '170'] 26\n\n\nFazer a Seleção de um DISTRITO e mostrar os munícipios\n\nimport matplotlib.pyplot as plt\n\n# Selecao dos Dados a Mostrar\ngdf_caop_sel = gdf_caop.loc[gdf_caop.DISTRITO == 'Guarda'] \n\n# Definir Legenda \n# Definir Localização (loc), Localização em relação ao eixos (bbox_to_anchor), Nº colunas (ncol)\nlgnd_kwds = {'title': 'Municipios',\n              'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 1}\n             \ngdf_caop_sel.plot(column = 'DTMN2',\n              legend = True,\n              legend_kwds  = lgnd_kwds,\n              edgecolor = 'black')\n\n# Rotulos  Eixos\nplt.xlabel('X Coordinados')\nplt.ylabel('Y Coordinados')\nplt.title('Municipios do Distrito da Guarda')\n# Rodar xticks; ha é o alinhamento\nplt.xticks(rotation=45, ha='right')\n              \n\n\n# **Codigo Alternativo Utilizando ax objecto**\n# fig, ax = plt.subplots()\n# gdf_caop_sel.plot(ax=ax,\n#               column = 'DTMN2',\n#               cmap = 'tab20',\n#               legend = True,\n#               legend_kwds  = lgnd_kwds,\n#               edgecolor = 'dimgray')\n# \n# # Rotulos  Eixos - Diferentes Funções\n# ax.set_xlabel('X Coordinados')\n# ax.set_ylabel('Y Coordinados')\n \nplt.show()\n\n\n\n\nusando o explore\n\ngdf_caop_sel.explore(column = 'DISTRITO',\n              legend = True,\n                  edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n6.1.4 Importar dados de tabelas\n\n6.1.4.1 ACES\n\n# Importar dados DGS Utentes em Centros de Saude\n# Ver referencia:  https://dados.gov.pt/pt/datasets/evolucao-mensal-de-utentes-atendidos-nos-centros-de-saude-agregado-por-aces-no-ambito-da-saude-oral-nos-cuidados-de-saude-primarios-socsp/#resources\n# Atributos: Período, ARS, ACES, Localização Geográfica, Sexo, Faixa Etária, Nº Utentes,ID \nimport pandas as pd\n\n# Link DGS (Utentes Centro Saude)  \nficheiro = r'http://dados.gov.pt/pt/datasets/r/dc54ea6f-31f3-483b-a719-718d0d7451f3'\n# Ler ficheiro do computador:\n#ficheiro = r'C:\\TEMp\\utentes-atendidos-nos-centros-de-saude-no-ambito-da-soep.csv'\n\n# Importar em DataFrame\nencoding = 'utf-8'\ndf_utentes = pd.read_csv(ficheiro, sep=';', encoding=encoding)\n\n\n# Mostrar informação df\nprint(df_utentes.head(5))\nprint(df_utentes.info())\nprint(df_utentes.describe())\n\n   Período    ARS                           ACES Localização Geográfica  \\\n0  2019-07  Norte                   ULS Nordeste  41.8069684,-6.7587977   \n1  2019-07  Norte  Douro I - Marão e Douro Norte  41.2968711,-7.7483727   \n2  2019-07  Norte  Douro I - Marão e Douro Norte  41.2968711,-7.7483727   \n3  2019-07  Norte           Douro II - Douro Sul  41.0953745,-7.8123805   \n4  2019-07  Norte           Douro II - Douro Sul  41.0953745,-7.8123805   \n\n        Sexo Faixa Etária  Nº Utentes  \\\n0   Feminino        20-34          30   \n1  Masculino        35-49          23   \n2   Feminino        50-64          70   \n3  Masculino        20-34          41   \n4  Masculino        35-49          80   \n\n                                                  ID  \n0                 2019-7/20-34/Feminino/ULS Nordeste  \n1  2019-7/35-49/Masculino/Douro I - Marão e Douro...  \n2  2019-7/50-64/Feminino/Douro I - Marão e Douro ...  \n3        2019-7/20-34/Masculino/Douro II - Douro Sul  \n4        2019-7/35-49/Masculino/Douro II - Douro Sul  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30692 entries, 0 to 30691\nData columns (total 8 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   Período                 30692 non-null  object\n 1   ARS                     30692 non-null  object\n 2   ACES                    30692 non-null  object\n 3   Localização Geográfica  30096 non-null  object\n 4   Sexo                    30692 non-null  object\n 5   Faixa Etária            30692 non-null  object\n 6   Nº Utentes              30692 non-null  int64 \n 7   ID                      30692 non-null  object\ndtypes: int64(1), object(7)\nmemory usage: 1.9+ MB\nNone\n         Nº Utentes\ncount  30692.000000\nmean      59.380457\nstd       71.069367\nmin        1.000000\n25%       15.000000\n50%       36.000000\n75%       78.000000\nmax      851.000000\n\n\npassar para o GDF\n\n# Verificar os Dados\n# Total numero de Pontos Unicos para Ficheiro:\nprint('Num localizações Geográficos:',len(df_utentes['Localização Geográfica'].unique()))\n\n# Verificar valores únicos de outras variáveis\nprint('Num ACES:',len(df_utentes['ACES'].unique()))\nprint('Num ARS:',len(df_utentes['ARS'].unique()))\nprint(df_utentes['Faixa Etária'].unique())\nprint(df_utentes['Sexo'].unique())\n\nNum localizações Geográficos: 51\nNum ACES: 79\nNum ARS: 35\n['20-34' '35-49' '50-64' '&lt;20' '65 e +']\n['Feminino' 'Masculino']\n\n\n\n# DataFrame com dados Utentes: df_utentes\n\n# Criar Colunas lat e Long a partir da coluna 'Localização Geográfica'\n# no ficheiro original a informação existe em apenas uma coluna\nprint (\"Tipo de atributo\", type(df_utentes['Localização Geográfica']))\n# Fazer um Split dos Valores utilizando virgula como separador\ndf_utentes[['lat', 'long']] = df_utentes['Localização Geográfica'].str.split(',', expand=True)\n\n# Mostrar informação df\nprint(df_utentes.info())\n\nTipo de atributo &lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30692 entries, 0 to 30691\nData columns (total 10 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   Período                 30692 non-null  object\n 1   ARS                     30692 non-null  object\n 2   ACES                    30692 non-null  object\n 3   Localização Geográfica  30096 non-null  object\n 4   Sexo                    30692 non-null  object\n 5   Faixa Etária            30692 non-null  object\n 6   Nº Utentes              30692 non-null  int64 \n 7   ID                      30692 non-null  object\n 8   lat                     30096 non-null  object\n 9   long                    30096 non-null  object\ndtypes: int64(1), object(9)\nmemory usage: 2.3+ MB\nNone\n\n\n\n# Criar DataFrame com Pontos Unicos e Numero de Utentos\n\n# Total numero de Pontos Unicos para Ficheiro:\nprint('Num loc:',len(df_utentes['Localização Geográfica'].unique()))\n\n# Converter variaveis numericos para numeric\ndf_utentes['Nº Utentes'] = pd.to_numeric(df_utentes['Nº Utentes'], errors='coerce')\ndf_utentes['lat'] = pd.to_numeric(df_utentes['lat'], errors='coerce')\ndf_utentes['long'] = pd.to_numeric(df_utentes['long'], errors='coerce')\n\n# Criar Novo DataFrame com ACES e Soma Nº Utentes\n# Group by 'ARS', 'ACES', 'Localização Geográfica'\n# O nº de utentes é um exemplo de um atributo que podemos utilizar para visualização\ndf_aces = df_utentes.groupby(['ARS', 'ACES', 'Localização Geográfica','lat', 'long']).agg({\n    'Nº Utentes': 'sum'\n}).reset_index()\n\nprint(df_aces.head())\n\nNum loc: 51\n\n\n        ARS                     ACES          Localização Geográfica  \\\n0  Alentejo         Alentejo Central           38.8442031,-7.5826619   \n1   Algarve      Algarve I - Central  37.0274264,-7.9395983999999995   \n2   Algarve      Algarve I - Central           37.0274264,-7.9395984   \n3   Algarve  Algarve II - Barlavento           37.1387554,-8.5445093   \n4   Algarve   AlgarveIII - Sotavento            37.383008,-7.7293275   \n\n         lat      long  Nº Utentes  \n0  38.844203 -7.582662       45349  \n1  37.027426 -7.939598        5051  \n2  37.027426 -7.939598       37627  \n3  37.138755 -8.544509       35960  \n4  37.383008 -7.729328       23136  \n\n\n\n\n\n\n# Mesmo com Pandas DataFrame já é possível mostrar a geografia\n# Importar Bibliotecas\n#import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Mostrar a localização\nplt.scatter(df_aces.long, df_aces.lat)\n \n# Show the plot\nplt.show()\n\n\n\n\n\n# Criar GeoDataFrame \n#import pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Criar a Coluna de geometry no DF a partir informação atributos long e lat \n# Utiliza-se o Point Object do modulo shapely\n# lambda - função anônima no Python \n# df_aces.apply - applica a função a cada linha do DF\n# Argumento apply: axis = 1. Função deve ocorrer para todos as linhas\ndf_aces['geometry'] = df_aces.apply(lambda x: Point(float(x.long), float(x.lat)), axis=1)\n \n# Create a GeoDataFrame from art and verify the type\ngdf_aces = gpd.GeoDataFrame(df_aces, crs = \"EPSG:4326\", geometry = df_aces.geometry)\n\nprint(type(gdf_aces))\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\n\n\nimportar dados poligonos\n\n# Shapefile NUTS3: NUTS3_2015_PT.shp (dados estão em WebMercator)\n# Mostrar Geografia\n#import geopandas as gpd\n#import matplotlib.pyplot as plt\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\NUTS3_2015_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_nuts3 = gpd.read_file(file_path, encoding=encoding)\n\nprint(gdf_nuts3.info())\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 25 entries, 0 to 24\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       25 non-null     object  \n 1   NUTS2       25 non-null     object  \n 2   NUTS3       25 non-null     object  \n 3   NUTS3_DSG   25 non-null     object  \n 4   SUM_AREA_K  25 non-null     float64 \n 5   Shape_Leng  25 non-null     float64 \n 6   Shape_Area  25 non-null     float64 \n 7   geometry    25 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.7+ KB\n\n\nNone\n\n\nmostrar pontos com Polygons\n\n# Existem diferenças entre o CRS!\nprint(\"CRS of the GeoDataFrame:\", gdf_aces.crs)\nprint(\"CRS of the GeoDataFrame:\", gdf_nuts3.crs)\n\n# Diferença nos CRS -&gt; Converter para WebMercator\ngdf_aces_m = gdf_aces.to_crs(\"epsg:3857\")\n\n# Obter apenas Municipios do Continente:\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS1 == '1']\n\n# Mostrar os 2 mapas:\n# Argumento cmap tem a predefinição de cores\n# Lista Referencias ColorMaps: https://matplotlib.org/stable/gallery/color/colormap_reference.html\nax = gdf_nuts3_sel.plot(column='NUTS3', \n                        cmap='Set2', \n                        legend=True, \n                        figsize=(10, 8))\n\n# Imprimir os pontos no mesmo ax (subplot)\n# Lista MatPlotLib cores: https://matplotlib.org/stable/gallery/color/named_colors.html\ngdf_aces_m.plot(ax=ax, \n                color='red',\n                markersize=30, \n                edgecolor='Black', \n                label='Cidades')\n\n# Adicionar Legenda\nax.legend()\nplt.title('Localização das ACES por NUTS3')\n\n# Mostrar o plot\nplt.show()\n\nCRS of the GeoDataFrame: EPSG:4326\nCRS of the GeoDataFrame: EPSG:3857\n\n\n\n\n\nO package Contextily permite adicionar um basemap\n\n# Import contextily\nimport contextily\n    \n# Existem diferenças entre o CRS!\nprint(\"CRS of the GeoDataFrame:\", gdf_aces.crs)\nprint(\"CRS of the GeoDataFrame:\", gdf_nuts3.crs)\n\n# Diferença nos CRS -&gt; Converter para WebMercator\ngdf_aces_m = gdf_aces.to_crs(\"epsg:3857\")\n\n# Obter apenas Municipios do Continente:\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS1 == '1']\n\n# Mostrar os 2 mapas:\n# Argumento cmap tem a predefinição de cores\n# Lista Referencias ColorMaps: https://matplotlib.org/stable/gallery/color/colormap_reference.html\n# Adicionar Transperência (alpha) ao layer\nax = gdf_nuts3_sel.plot(column='NUTS3', \n                        cmap='Set2', \n                        legend=True, \n                        figsize=(10, 8),\n                       alpha = 0.5)\n\n# Imprimir os pontos no mesmo ax (subplot)\n# Lista MatPlotLib cores: https://matplotlib.org/stable/gallery/color/named_colors.html\ngdf_aces_m.plot(ax=ax, \n                color='red',\n                markersize=30, \n                edgecolor='Black', \n                label='ACES')\n\n# Adicionar um BaseMap no Ax\ncontextily.add_basemap(ax)\n\n# Adicionar Legenda\nax.legend()\nplt.title('Localização das ACES por NUTS3')\n\n# Show the plot\nplt.show()\n\nCRS of the GeoDataFrame: EPSG:4326\nCRS of the GeoDataFrame: EPSG:3857\n\n\n\n\n\nObter Informações sobre o Objecto de Geometria\nA partir do geometry conseguimos obter um conjunto de caracteristicas\n\n# Tipo de Dados da coluna de Geometria\nprint (type(gdf_aces.geometry))\nprint (type(gdf_nuts3_sel.geometry))\nprint(gdf_nuts3_sel.info())\n\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 23 entries, 0 to 22\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       23 non-null     object  \n 1   NUTS2       23 non-null     object  \n 2   NUTS3       23 non-null     object  \n 3   NUTS3_DSG   23 non-null     object  \n 4   SUM_AREA_K  23 non-null     float64 \n 5   Shape_Leng  23 non-null     float64 \n 6   Shape_Area  23 non-null     float64 \n 7   geometry    23 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.6+ KB\nNone\n\n\nobter Área e Centroid\n\n# Obter Informaçao Geografia - Utilizando gdf_caop\n# Importar geometry\nfrom shapely.geometry import Point, Polygon\n\n# Adicionar Nova coluna com area em Km2\n# Atenção Deveriamos adicionar novos atributos sempre a (Geo)DataFrame de Origem\n# Não adicionar a seleção, neste caso gdf_nuts3_sel!\ngdf_nuts3['area_km2'] = gdf_nuts3['geometry'].area / 10**6  \n\n# Ordenar por Area\narea_gdf = gdf_nuts3.sort_values(by='area_km2', ascending=False)\n\n# Print the GeoDataFrame with the new column\nprint(area_gdf[['NUTS3','NUTS3_DSG', 'area_km2']])\n\n   NUTS3                     NUTS3_DSG      area_km2\n19   184                Baixo Alentejo  13733.680581\n22   187              Alentejo Central  12124.085355\n16   16J     Beiras e Serra da Estrela  10917.782234\n21   186                 Alto Alentejo  10140.463336\n7    11E      Terras de Trás-os-Montes   9909.588021\n18   181              Alentejo Litoral   8565.542590\n8    150                       Algarve   7899.809228\n14   16H                   Beira Baixa   7847.568545\n11   16E             Região de Coimbra   7444.322853\n6    11D                         Douro   7118.775462\n20   185               Lezíria do Tejo   7110.526353\n15   16I                    Médio Tejo   5639.532885\n13   16G              Viseu Dão Lafões   5638.896615\n4    11B                   Alto Tâmega   5239.870518\n17   170  Área Metropolitana de Lisboa   4963.947294\n12   16F              Região de Leiria   4157.670418\n0    111                    Alto Minho   4006.182814\n23   200    Região Autónoma dos Açores   3789.073859\n9    16B                         Oeste   3711.889332\n3    11A   Área Metropolitana do Porto   3597.098437\n5    11C                Tâmega e Sousa   3236.885188\n10   16D              Região de Aveiro   2942.595609\n2    119                           Ave   2589.188487\n1    112                        Cávado   2231.518979\n24   300    Região Autónoma da Madeira   1136.433784\n\n\nadicionar nova coluna com o centroide\n\n# Criar Nova coluna com Centroid\n# Adicionar Nova coluna com centroid\ngdf_nuts3['mn_center'] = gdf_nuts3['geometry'].centroid  \n\n# Print the GeoDataFrame with the new column\nprint(gdf_nuts3[['NUTS3','NUTS3_DSG', 'mn_center']])\n\n   NUTS3                     NUTS3_DSG                         mn_center\n0    111                    Alto Minho   POINT (-946980.726 5142757.347)\n1    112                        Cávado   POINT (-941320.248 5103934.014)\n2    119                           Ave   POINT (-909301.972 5085897.504)\n3    11A   Área Metropolitana do Porto   POINT (-943820.268 5025611.477)\n4    11B                   Alto Tâmega   POINT (-848339.419 5111654.442)\n5    11C                Tâmega e Sousa   POINT (-905197.685 5040987.781)\n6    11D                         Douro   POINT (-825322.310 5034791.692)\n7    11E      Terras de Trás-os-Montes   POINT (-759535.812 5097352.556)\n8    150                       Algarve   POINT (-905203.263 4473163.600)\n9    16B                         Oeste  POINT (-1015558.750 4762587.544)\n10   16D              Região de Aveiro   POINT (-949322.164 4958172.134)\n11   16E             Região de Coimbra   POINT (-927707.382 4898122.619)\n12   16F              Região de Leiria   POINT (-962098.688 4841198.729)\n13   16G              Viseu Dão Lafões   POINT (-881906.240 4968423.075)\n14   16H                   Beira Baixa   POINT (-825652.276 4850783.449)\n15   16I                    Médio Tejo   POINT (-920812.797 4807647.369)\n16   16J     Beiras e Serra da Estrela   POINT (-810641.235 4941054.650)\n17   170  Área Metropolitana de Lisboa  POINT (-1006290.656 4683604.593)\n18   181              Alentejo Litoral   POINT (-952473.033 4579022.311)\n19   184                Baixo Alentejo   POINT (-870119.976 4561859.007)\n20   185               Lezíria do Tejo   POINT (-958500.668 4738718.638)\n21   186                 Alto Alentejo   POINT (-848325.426 4749055.249)\n22   187              Alentejo Central   POINT (-872968.675 4665202.808)\n23   200    Região Autónoma dos Açores  POINT (-3038400.829 4629675.418)\n24   300    Região Autónoma da Madeira  POINT (-1887053.937 3862343.766)\n\n\ncalcular a distancia entre dois pontos\n\n#import geopandas as gpd\n#from shapely.geometry import Point, Polygon\n\n# Calcular diancia entre 2 ACES ('Amadora' and 'Lisboa Central')\n\n# Como dados são em Graus Decimais será necessário de os converter para Metricos\nprint(\"CRS gdf_aces:\", gdf_aces.crs)\n\norigem = 'Amadora'\ndestino = 'Grande Porto V - Porto Ocidental'\n# Converter para WebMercator\ngdf_aces_m = gdf_aces.to_crs(\"epsg:3857\")\n\n# Filtrar as geometrias para obter a Origem e Destino definidos\norigem_geometry = gdf_aces_m[gdf_aces_m['ACES'] == origem]['geometry'].iloc[0]\ndestino_geometry = gdf_aces_m[gdf_aces_m['ACES'] == destino]['geometry'].iloc[0]\n\n# Calculate the distance between 'Amadora' and 'Lisboa Central'\ndistance_km = origem_geometry.distance(destino_geometry) / 1000  # Converter para quilómetro\n\nprint(f\"A distância entre {origem} e {destino} é aproximadamente  {distance_km:.2f} quilómetros.\")\n\nCRS gdf_aces: EPSG:4326\nA distância entre Amadora e Grande Porto V - Porto Ocidental é aproximadamente  356.68 quilómetros.\n\n\n\n\n\n6.1.5 Juntar Dados a um GDF\n\n6.1.5.1 Censos 2021\nimporta dados de Censos2021 por NUTS3 e Municipio\n\n# Obter Password e Utilizador para Ligacao SQL\n#from getpass import getpass # para ler a password sem a mostrar\nmy_user = '\"BRUNO.LIMA\"'  \nmy_password = 'Pa$$w0rd5' # getpass(\"Password: \")\n\n# Ler Dados da BD\n# criar conexão\nimport cx_Oracle \nimport pandas as pd\nhost = 'c21oradev01.int.ine.pt'\nport = '1521'\nservice = 'FORMACAO'\ndsn_tns = cx_Oracle.makedsn(host, port, service_name=service) \n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\n# Cursor:\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\n# ---------------------------------\n# Ler View com Dados por NUTS3:\n\n# SQL String\nmy_sql = \"\"\"\nselect *\nfrom BDIFRM.V_BGRI2021_N3_PT \n\"\"\"\n\n# Executar o cursor c com a string como parâmetro\nc.execute(my_sql)\n# Criar Nomes colunas\nnames = [ x[0] for x in c.description]\ndf_n3_c2021 = pd.DataFrame(c.fetchall(), columns = names)\n\n# Fechar cursor\nc.close()\n\n# fechar conexão\nconn.close()\n\nfazer JOIN dos dados GDF com as NUTS3\n\n# Juntar Dados NUTS3 do C2021 a GDF com as areas NUTS3\n\n# gdf_nuts3 obtido de Shapefile NUTS3: NUTS3_2015_PT.shp (dados estão em WebMercator)\n# Colunas Comuns: NUTS3 e NUTS3 - podemos ver os valores em ambos os DF\n#print(sorted(df_n3_c2021.NUTS3.unique()))\n#print(sorted(gdf_nuts3.NUTS3.unique()))\n\n# Fazer o Join, especificar: DF\ngdf_nuts3_2 = gdf_nuts3.merge(df_n3_c2021, left_on='NUTS3', right_on='NUTS3', how='left') \n\nprint(gdf_nuts3_2.info())\n\n\n\n\n6.1.6 Criar Mapas Temáticos\nVisualizar a classificação da relação entre 2 variáveis por NUTS3\n\n# Import packages\n# import matplotlib.pyplot as plt\n# import pandas as pd\n# import geopandas as gpd\n \n# Exemplo criar Plot dos Areas NUTS3 - Mostrando total de população 65+ no total de população\n# Utilizar geodataframe com join \"gdf_nuts3_2\"\n\n# Selecionar Dados Portugal Continental:\n# Fazer Seleção da NUTS1_x , houve rename da coluna apos join\ngdf_nuts3_sel = gdf_nuts3_2.loc[gdf_nuts3_2.NUTS1_x == '1']\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 2}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf_nuts3_sel.plot(column=gdf_nuts3_sel.N_INDIVIDUOS_65_OU_MAIS/gdf_nuts3_sel.N_INDIVIDUOS, \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=7, \n                      cmap='YlGn', \n                      legend=True,\n                      edgecolor = 'dimgray',\n                      legend_kwds  = lgnd_kwds)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Relacao população 65+ no total de população')\nplt.show()\n\n\n6.1.6.1 Exercício\nCriar Mapa Temático das NUTS com apresentação de cidades Estatisticas\n\n# Fazer o Join de NUTS e dados dos Censos , especificar: DF\ngdf_nuts3_2 = gdf_nuts3.merge(df_n3_c2021, left_on='NUTS3', right_on='NUTS3', how='left')\n\nprint(gdf_nuts3_2.info())\n\n# Importar os dados das cidades\nficheiro = r'data\\CIDADES_PONTOS_CONT.csv'\n\n# Importar em DataFrame\nencoding = 'utf-8'\ndf_cidades = pd.read_csv(ficheiro, sep=';', encoding=encoding)\n\ndf_cidades.head()\n\n#from shapely.geometry import Point, Polygon\n\n# Criar coluna com geometry (mudar nome df e nomes atributos)\ndf_cidades['geometry'] = df_cidades.apply(lambda x: Point(float(x.LONGITUDE), float(x.LATITUDE)), axis=1)\n\n# Criar uma gdf a partir da coluna geometry (mudar nome df)\ngdf_cidades = gpd.GeoDataFrame(df_cidades, crs = \"EPSG:4326\", geometry = df_cidades.geometry)\n\n# Converter Cidades para WebMercator\ngdf_cidades_m = gdf_cidades.to_crs(\"epsg:3857\")\n\n# Utilizar: gdf_nuts3_2\n# Input variáveis: gdf_aces e gdf_nuts3_2\n\n# Selecionar Dados Portugal Continental:\n# Fazer Seleção da NUTS1_x , houve rename da coluna apos join\ngdf_nuts3_sel = gdf_nuts3_2.loc[gdf_nuts3_2.NUTS1_x == '1']\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 2}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf_nuts3_sel.plot(column=(gdf_nuts3_sel.N_EDIFICIOS_CONSTR_ANTES_1945+gdf_nuts3_sel.N_EDIFICIOS_CONSTR_1946_1980)/gdf_nuts3_sel.N_EDIFICIOS_CLASSICOS, \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=7, \n                      cmap='YlGnBu', \n                      legend=True,\n                      edgecolor = 'gray',\n                      legend_kwds  = lgnd_kwds)\n\n# Adicionar os Pontos no mesmo AX\ngdf_cidades_m.plot(ax=ax, color='lightcoral', markersize=10, edgecolor='Black', label='Cidades')\n\n# Adicionar Legenda\nplt.title('Rácio Edificios construidos até 1980 (com representação das cidades)')\n\n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Relacao edificios até 1980 no total de edificios')\nplt.show()\n\n\ngdf_cidades_m.explore()\n\n\n\n\n6.1.7 Folium\n\nimport folium\n\n# Mostrar Zona do INE \n# Lisboa: 38.738345820101536, -9.138601637922605\n# Porto: 41.150887961411634, -8.629046747840249\n# Longitude e Latitude\nine = folium.Map(location = [41.150887961411634, -8.629046747840249], \n                 zoom_start = 15)\n\nine\n\nImportar Dataset com NUTS3\n\n# Importar GeoPandas\n#import geopandas as gpd\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\NUTS3_2015_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_nuts3 = gpd.read_file(file_path, encoding=encoding)\n\ngdf_nuts3.loc[1,'geometry']\n\n\n\n\nMostrar Localização Central da GDF das NUTS3\n\n# Obter a Localização Central\n# Print the head of the urban_polygon\n#import geopandas as gdp\nimport folium\n\n# Obter o centroid de toda a geometria\n# Converter para 4326 e a seguir obter o centroid de união de toda a geometria\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS1 == '1'] # Selecionar apenas o continente\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude longitude\ncenter_map = [centroid.y, centroid.x]\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=8, tiles='OpenStreetMap')\n\nfolium_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nadicionar geografia (GDF) ao mapa\n\n# Adicionar a Geografia a mapa\n# Print the head of the urban_polygon\n# import geopandas as gdp\n# import folium\n# from shapely.geometry import Point, Polygon\n\n# Converter para 4326 e a seguir obter o centroid de união de toda a geometria\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=7, tiles='OpenStreetMap')\n\n# Adicionar Geografia folium map\n# folium.GeoJson constructor\nfolium.GeoJson(gdf_nuts3_sel).add_to(folium_map)\n\nfolium_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nadicionar pop e tooltip\n\ngdf_nuts3_sel.info()\n\n# Incluir Informação de POPUP\n# Print the head of the urban_polygon\n#import geopandas as gdp\n#import folium\nimport folium.plugins \n\n# Converter para 4326 e a seguir obter o centroid de união de toda a geometria\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=7, tiles='OpenStreetMap')\n\n# Codigo Especifico para tooltip\ntooltip = folium.GeoJsonTooltip(\n    fields=[\"NUTS3\"],\n    aliases=[\"NUTS3\"],\n    localize=True,\n    sticky=False,\n    labels=True,\n    style=\"\"\"\n        background-color: #F0EFEF;\n        border: 2px solid black;\n        border-radius: 3px;\n        box-shadow: 3px;\n    \"\"\",\n    max_width=800,\n)\n\n# Definir popup e nome\nfolium.GeoJson(gdf_nuts3_sel,\n        zoom_on_click = True,\n        popup = folium.GeoJsonPopup(\n            fields=['NUTS3','NUTS3_DSG'],\n            aliases=['NUTS3','NUTS3_DSG']\n        ),\n        tooltip = tooltip       \n               \n    ).add_to(folium_map) \n\n\nfolium_map\n# folium_map.save(r'data\\xxmap.html')\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 23 entries, 0 to 22\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       23 non-null     object  \n 1   NUTS2       23 non-null     object  \n 2   NUTS3       23 non-null     object  \n 3   NUTS3_DSG   23 non-null     object  \n 4   SUM_AREA_K  23 non-null     float64 \n 5   Shape_Leng  23 non-null     float64 \n 6   Shape_Area  23 non-null     float64 \n 7   geometry    23 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.6+ KB\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.1.8 Criar Mapa Temático em Folium\n\n# Import packages\n# import pandas as pd\n# import geopandas as gpd\n# import folium, folium.plugins \n\n# Mesmo Exemplo criar Plot dos Areas NUTS3 - Mostrando total de população 65+ no total de população\n# Assegurar que foi efetuado Merge com Dados C2021\n# GDF input: gdf_nuts3_2\n\n# Selecionar Dados Portugal Continental: (copy() para criar nova coluna)\ngdf_nuts3_sel = gdf_nuts3_2.loc[gdf_nuts3_2.NUTS1_x == '1'].copy()\n\n# Novo Atributo\ngdf_nuts3_sel['ind65'] = gdf_nuts3_sel.N_INDIVIDUOS_65_OU_MAIS/gdf_nuts3_sel.N_INDIVIDUOS\n\n# Obter o centroid de toda a geometria\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=6, tiles='OpenStreetMap')\n\n# Criar choropleth\nchoropleth = folium.Choropleth(\n    geo_data=gdf_nuts3_sel,\n    name='População 65+ mais',\n    data=gdf_nuts3_sel,\n    columns=['NUTS3', 'ind65'],\n    key_on='feature.properties.NUTS3',\n    fill_color='Reds', #\n    fill_opacity=0.5,\n    line_opacity=1.0,\n    bins =8,\n    legend_name='Relacao população 65+ no total de população'\n)\n\nfolium.GeoJson(gdf_aces,\nzoom_on_click = True).add_to(folium_map)\n\n# Adicionar a mapa\nchoropleth.add_to(folium_map)\n\n# Widget para controlar os layer visiveis            \nfolium.LayerControl().add_to(folium_map)\n\nfolium_map.save(r'data\\xxmap.html')\n\nfolium_map\n\n# Limpar Object da memoria\ngdf_nuts3_sel = None"
  },
  {
    "objectID": "700-mod7.html",
    "href": "700-mod7.html",
    "title": "7  Módulo 7",
    "section": "",
    "text": "Conteúdos\n\n\n\nConceitos de Classe, Programação recursiva, RegExp."
  },
  {
    "objectID": "800-mod8.html",
    "href": "800-mod8.html",
    "title": "8  Módulo 8",
    "section": "",
    "text": "Conteúdos\n\n\n\nAprendizagem supervisionada utilizando o scikit."
  },
  {
    "objectID": "900-mod9.html",
    "href": "900-mod9.html",
    "title": "9  Módulo 9",
    "section": "",
    "text": "Conteúdos\n\n\n\nIntrodução à análise espacial de dados geográficos.\nOs formatos de dados JSON e GEOJSON.\nComo obter dados através de um API."
  }
]