[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python 2024",
    "section": "",
    "text": "Programação em Python"
  },
  {
    "objectID": "index.html#formação-em-data-science",
    "href": "index.html#formação-em-data-science",
    "title": "Python 2024",
    "section": "Formação em Data Science",
    "text": "Formação em Data Science\n\n\n\nFormadores: Bartholomeus Schoenmakers, Luis Ferreira, Sónia Quaresma\nBasic de 2024-04-08 a 202404-12\nIntermediate de 2024-04-15 a 2024-04-22\nAdvanced de 2024-05-06 a 2024-05-14\nFormação presencial realizada nas instalações do INE - Porto.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nOs materiais de formação a utilizados estão disponiveis na cloud do INE:\nhttps://cloud2.ine.pt/index.php/s/Grf224s54pNascC\npassword: Python.Porto2023\n\n\n\nUsing Python with the RStudio IDE\n\n\n\n\n\n\nshortcut\n\n\n\n\n\nPara inserir uma nova chunk de python, no RStudio:\nTools &gt; Modify Keyboard Shortcuts &gt; Insert Chunk Python &gt;\nCtrl+Alt+P\n\n\n\n\n\n\n\n\n\npackages a instalar\n\n\n\n\n\npip install plotnine cutecharts xgboost pysal pykrige osmnx"
  },
  {
    "objectID": "100-mod1.html#o-que-é-o-python",
    "href": "100-mod1.html#o-que-é-o-python",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.1 o que é o Python?",
    "text": "1.1 o que é o Python?\nlinguagem de programação:\n\ninterpretada e orientada a objectos (mas permite funções)\naprendizagem rápida e simples\ngratuita\n\nmodo interativo vs modo script\nIniciar -&gt; Anaconda prompt (se o Anaconda estiver configurado).\nmodo interativo vs modo script\ncriado um ficheiro.py, pode ser executado:\npython -u \"c:\\Users\\documents\\ficheiro.py\""
  },
  {
    "objectID": "100-mod1.html#funções-básicas",
    "href": "100-mod1.html#funções-básicas",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.2 Funções básicas",
    "text": "1.2 Funções básicas\n\n# imprimir na consola\nprint('olá mundo!')\n\nolá mundo!\n\n\n\n# docstring, funciona como comentário mas não é recomendado\n\n\"\"\"\npermite escrever texto \nem multiplas\nlinhas\n\"\"\"\n\n'\\npermite escrever texto \\nem multiplas\\nlinhas\\n'\n\n\nimprimir números e /ou cálculos:\n\nprint(3+6)\n\n9\n\n\npara concatenar texto formatado\n\nnumero = 4\nprint(f\"O número é: {numero*2}\")\n\nO número é: 8\n\n\nimprimir múltiplas linas\n\nprint(\"\"\"\nlinha1\nlinha2\n\"\"\")\n\n\nlinha1\nlinha2\n\n\n\nraw strings\n\n# forçar a imprimir exactamente o que está entre '' \nprint(r'tudo\\namora')\n\n# ou então podemos imprimir com a qubra de linha\nprint('tudo\\namora')\n\ntudo\\namora\ntudo\namora\n\n\nold string formating%\n\n# se x for string\nx = '15'\n\nprint(\"x como string = %s\" %(x))\n\nx como string = 15\n\n\n\ny = int(x)\nprint(\"x como integer = %d\" %(y))\n\nprint(\"x como float = %09.4f\" %(y))\n\nx como integer = 15\nx como float = 0015.0000\n\n\n\ninput(\"Escreve um input: \")\n\nfunção que avalia o seu argumento:\n\nnumero = eval('2')\n\n# em alternativa podemos especificar o tipo que pretendemos\nnumero = int('2')\n\nprint(f\"o dobro do numero é: {numero *2}\")\n\no dobro do numero é: 4\n\n\nem alternativa podemos especificar o tipo que pretendemos\n\ninteiro = int('2')\n\nprint(f\"o dobro do numero é: {inteiro *2}\")\n\no dobro do numero é: 4\n\n\nNo modo script podemos criar um ficheiro com o código que queremos executar, por exemplo o ficheiro mod1.py e excecutar na linha de comandos (Iniciar -&gt; Anaconda prompt):\nC:\\Users\\bruno.lima\\Documents\\Python\\python2024\\exercicios&gt;python mod1.py"
  },
  {
    "objectID": "100-mod1.html#aritmética",
    "href": "100-mod1.html#aritmética",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.3 Aritmética",
    "text": "1.3 Aritmética\n\n# quatro operações básicas\n\nprint(2+2, 5-2, 4*2, 10/5)\n\n# potencias\n4**3\n\n# calcula horas\nminutos = 70\nprint(f\"são {minutos//60} horas e {minutos-60} minutos\")\n\n4 3 8 2.0\nsão 1 horas e 10 minutos"
  },
  {
    "objectID": "100-mod1.html#definir-e-alterar-variáveis",
    "href": "100-mod1.html#definir-e-alterar-variáveis",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.4 Definir e alterar variáveis",
    "text": "1.4 Definir e alterar variáveis\n\n# por convenção devemos usar nome do formato case_snake\ncores_do_arco_iris = ['red','green','yellow','blue','orange','indigo', 'violet']\n\nnumero_de_tons = len(cores_do_arco_iris)\n\nnumero_de_tons\n\n7\n\n\ncasting\n\nx = int(3)\ny = str(3)\nz = float(3)"
  },
  {
    "objectID": "100-mod1.html#exercicios",
    "href": "100-mod1.html#exercicios",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.5 Exercicios",
    "text": "1.5 Exercicios\n\n# calcular o vlume de um cilindro dado o raio da base e a altura\n\nraio_base = 2\naltura = 10\n\narea_base = 3.14*raio_base**2\n\nvolume = area_base*float(altura)\n\nprint(f\"o volume é: {volume} m3\")\n\no volume é: 125.60000000000001 m3\n\n\nobtem os primeiros 4 divisores de um número N, separados por ‘++++’\n\nN = 12\nlista = []\n\nfor i in range(1, N+1):\n  if N%i == 0:\n    lista.append(i)\n    \nprint(*lista[:4], sep = '+++')\n\n1+++2+++3+++4\n\n\nobtem os primeiros 4 múltiplos de um número, separados por ‘++++’\n\nnumero=3\n\nprint(f'os múltiplos de {numero} são: {numero *1}', numero *2, numero *3, numero *4, sep = ' ++++ ', end = \" ^^\")\n\nos múltiplos de 3 são: 3 ++++ 6 ++++ 9 ++++ 12 ^^"
  },
  {
    "objectID": "100-mod1.html#tipos-de-dados",
    "href": "100-mod1.html#tipos-de-dados",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.6 Tipos de dados",
    "text": "1.6 Tipos de dados\n\nstr\nnumérico:\n\nint\nfloat\ncomplex\n\nbool\nsequencia:\n-list -tuple -range\ndict\nset / frozenset\nbinario:\n\nbytes\nbytearray\nmemoryview\n\n\n\n1.6.1 string\n\na = \"um conjunto de letras\"\n\ntipo=type(a)\nlen(a)\n\nprint(f\"o tipo da variável 'a' é {tipo}\")\n\nprint(f\"a primeira letra é %s e a ultima letra é %s\" % (a[0], a[-1]))\n\nprint(a[:2]) # todas as letras até à segunda posição\n\no tipo da variável 'a' é &lt;class 'str'&gt;\na primeira letra é u e a ultima letra é s\num\n\n\n\n\n1.6.2 int\n\ni1 = 12\ni2 = -1\n\nprint(type(i1), type(i2))\n\n&lt;class 'int'&gt; &lt;class 'int'&gt;\n\n\n\n\n1.6.3 float\n\nf2=-7.7e100\nf3=2E2\n\nprint(f2, f3)\n\n-7.7e+100 200.0\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA instalação dos packages deve ser feita através do terminal Bash com o comando: $ pip install pandas\n\n\n\nimport math\n\nprint(repr(math.pi))\nprint(format(math.pi,'.12g'))\nprint(format(math.pi,'.2f'))\n\n3.141592653589793\n3.14159265359\n3.14\n\n\narredondamento implicito\n\nprint(.1+.1+.1 == .3)\n\nprint(round(.1+.1+.1, 10) == round(.3, 10))\n\nFalse\nTrue\n\n\n\n\n1.6.4 complex(j=parte imaginária)\n\nc1 = 1j\nc2 = 3+5j\n\nprint(type(c1), type(c2))\n\n&lt;class 'complex'&gt; &lt;class 'complex'&gt;\n\n\n\n\n1.6.5 bool\n\nprint(type(True))\n\nprint(f\"o número zero é: {bool(0)}\")\nprint(f\"o número 45 é: {bool(45)}\")\nprint(f\"o nome 'INE' é: {bool('INE')}\")\nprint(f\"o vazio é: {bool('')}\")\n\n&lt;class 'bool'&gt;\no número zero é: False\no número 45 é: True\no nome 'INE' é: True\no vazio é: False\n\n\n\n\n1.6.6 sem tipo\nNoneType\n\n\n1.6.7 conversão entre tipos\n\n# conversão implicita\nx = 10\ny = 5\nz = x+y\n\nprint(type(z))\n\n&lt;class 'int'&gt;\n\n\n\n#conversão para inteiro\nprint(int(1))\nprint(int(2.8))\nprint(int(3))\n\n1\n2\n3\n\n\n\n# conversão para float\nprint(float('3.1'))\n\n3.1\n\n\n\n# conversão para bool\nprint(bool(1))\n\nTrue\n\n\n\n# conversão para string\na = str(2)\nb = str(3.0)\n\nprint(a,b)\n\n2 3.0\n\n\n\n#conversão com eval\na = eval('8**2')\nprint(a)\n\n64"
  },
  {
    "objectID": "100-mod1.html#organização-do-código",
    "href": "100-mod1.html#organização-do-código",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.7 Organização do código",
    "text": "1.7 Organização do código\n\n1.7.1 Programação modular\n\nsimplicidade\nfacilidade de manutenção\nreutilização\nâmbito\n\n\n\n1.7.2 Packages\npackages\n\nsubpackages\n\nmodule: funções, classes, variáveis, código, …\n\n\n\n\n\n1.7.3 Funções\n\nBuilt-in (funções standard do Python)\n\n\n\nUDF (funções definidas pelo utilizador)\nlambda (funções anónimas)\n\n\n\n1.7.4 UDF\ndef nome_da_funcao(parametros):\n  \"\"\" comentário com propósito da função\"\"\"\n  \n  # corpo da função (instruções e lógica)\n  \n  return informacao_a_retornar\nexemplos:\n\ndef soma(x,y):\n  \"\"\" AVISO \"\"\"\n  return(x+y)\n\nsoma(1,2)\n\n3\n\n\n\n\n1.7.5 exercícios\n\n# criar uma função que devolve um valor elevado a uma potencia\n\ndef pot(base, expo):\n  return(base**expo)\n\nprint(pot(5,2))\n\n25"
  },
  {
    "objectID": "100-mod1.html#controlo-da-execução",
    "href": "100-mod1.html#controlo-da-execução",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.8 Controlo da execução",
    "text": "1.8 Controlo da execução\n\n1.8.1 Controlo condicional\n\n\nchove = True\nif (chove):\n  print(\"chove mesmo!\")\nelse:\n  print(\"Faz sol!\")\n\nchove mesmo!\n\n\noperadores de comparação\n'&gt;', '&lt;', '==', '&gt;=', '&lt;=', '!=', 'is' ['not'], ['not'] 'in' \n\na = 1\nb = 2\nc = 3\n\nprint(f\"1-{a} &lt;{b} &lt;{c}\")\n\n1-1 &lt;2 &lt;3\n\n\n\n\n1.8.2 operador trenário\n\nidade = int('20')\ndecisao = 'já pode votar' if idade &gt;=18 else 'ainda não pode votar'\n\nprint(f\"com {idade} anos,  a decisão é: {decisao}\")\n\ncom 20 anos,  a decisão é: já pode votar\n\n\n\n# exemplo de menor legibilidade\nnr_mes = 3\n\nnome_mes = 'janeiro' if nr_mes == 1 else \\\n'fevereiro' if nr_mes == 2 else \\\n'março' if nr_mes == 3 else \\\n'outro'\n\nprint(nome_mes)\n\nmarço\n\n\n\n\n1.8.3 ciclos\n\n\n# definir função oráculo usado mais em baixo\ndef oraculo_mistico(pergunta):\n  ''' retorna uma resposta à pergunta feita\n  ao estilo do jogo \"Bola 8 mágica\"\n  '''\n  import random\n\n  respostas = [\n    \"Sim\", \"Não\", \"Claro\", \"Com certeza\", \"Arrisque\", \"Não conte com isso\", \"Provavelmente\",\n    \"É duvidoso\", \"Talvez\", \"Não tenho certeza\", \"Sem dúvida\", \"Absolutamente\",\n    \"É melhor não dizer agora\", \"Concentre-se e pergunte novamente\", \"Minhas fontes dizem não\",\n    \"As perspectivas não são boas\", \"Não é possível prever agora\", \"Reformule sua pergunta\",\n    \"Não posso responder a isso\", \"Pergunte novamente mais tarde\", \"Probabilidade zero\",\n    \"Está nas estrelas\", \"Muito provável\", \"Os astros são favoráveis\"\n  ]\n\n  return random.choice(respostas)\n\n\ncontinuar_jogo = True\n\nwhile(continuar_jogo):\n  pergunta = input(\"faz pergunta: \")\n  if(pergunta == 'sair'):\n    continuar_jogo = False\n  else:\n    reposta = oraculo_mistico(pergunta) # oraculo_mistico() é uma função ad hoc\n    print(f\"A resposta é: {resposta}\")\n\n\n\n1.8.4 for loop\n\nfor i in range(10):\n  print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n1.8.5 exercícios\nCom um for loop escreve os primeiros 10 multiplos de 7\n\nfor i in range(10):\n  print(i * 7)\n\n0\n7\n14\n21\n28\n35\n42\n49\n56\n63\n\n\nCom recurso a loops,repita uma palavra definida pelo utilizador o número de vezes que ele pretenda, removendo a primeira letra da palavra introduzida\n\npalavra='teste'\nn=5\n\nprint(palavra[1:]*n, sep=' + ')\n\nfor i in range(n):\n  print(palavra[1:])\n\nesteesteesteesteeste\neste\neste\neste\neste\neste\n\n\nSimule o jogo de pedra, papel e tesoura com o computador (papel ganha pedra que ganha tesoura que ganha papel):\n\nfrom random import sample\n\nlista = ['pedra','papel','tesoura']\n\njogador1 = sample(lista,1)[0]\njogador2 = sample(lista,1)[0]\n\nprint(jogador1, jogador2)\n\nganhador = 'empate' if jogador1 == jogador2 else \\\n'jogador1' if jogador1 == 'papel' and jogador2 == 'pedra' else \\\n'jogador1' if jogador1 == 'tesoura' and jogador2 == 'papel' else \\\n'jogador1' if jogador1 == 'pedra' and jogador2 == 'tesoura' else \\\n'jogador2'\n\nprint(f\"o vencedor é o {ganhador}\")\n\npedra papel\no vencedor é o jogador2\n\n\nsolução alternativa:\n\nimport random\nfim_do_jogo = False\n\nwhile (not(fim_do_jogo)):\n  jogada_humana = int(input(\"escolha:\\n1 - pedra\\n2 - papel\\n3 - tesoura\\n0 - terminar o jogo\\n\"))\n  if (jogada_humana == 0):\n    print(\"fim.\\n\")\n    fim_do_jogo = True\n  else:\n    jogada_cpu = random.choice([1, 2, 3]) # porque ainda não conhecemos o randint()\n\n    jogada_cpu_texto = \"pedra\"\n    if (jogada_cpu == 2):\n      jogada_cpu_texto = \"papel\"\n    elif (jogada_cpu == 3):\n      jogada_cpu_texto = \"tesoura\"\n\n    if (jogada_cpu == jogada_humana):\n      print(\"empate\\n\")\n    else:\n      print(f\"o computador jogou '{jogada_cpu_texto}'.\")\n      if ((jogada_cpu == 1 and jogada_humana == 2) or (jogada_cpu == 2 and jogada_humana == 3) or (jogada_cpu == 3 and jogada_humana == 1)):\n        print(\"vitória humana!\\n\\n\")\n      else:\n        print(\"o computador venceu\\n\\n\")"
  },
  {
    "objectID": "100-mod1.html#funções-mais-usadas",
    "href": "100-mod1.html#funções-mais-usadas",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.9 Funções mais usadas",
    "text": "1.9 Funções mais usadas\n\n1.9.1 funções built-in\n\nprint(\"a lista de nomes que podemos usar:\")\ndir()\nlen(dir())\n\na lista de nomes que podemos usar:\n\n\n103\n\n\n\nimport random\nimport math\n\nlen(dir())\n\n106\n\n\nrange()\n\nfor i in range(1, 5, 2):\n  print(i)\n\n1\n3\n\n\nord()\n\nord('a')\n\n97\n\n\npow()\n\nprint(pow(4,2,5) == 4**2 %5)\n\nTrue\n\n\nsum()\nlen()\nmax()\n\nvar1 = 'teste'\nvar2 = 'palavra'\nvar3 = 'coisa'\nmax_val = max(var1, var2, var3, key = len)\n\nprint(max_val)\n\npalavra\n\n\n\n\n1.9.2 ler a partir de ficheiros\nficheiro = open(r\"c:\\path\\ficheiros.txt\", modo_de_acesso)\ntentativa de uniformizar:\n\nimport os\n\npath = os.path.join(os.sep, rooth_path + os.sep = 'directoria')\n\nler linha a linha:\n\nlinhas = [line.strip().split('',1)] for line in open(nome_do_ficheiro)\n\nopen(nome_do_ficheiro).read()\n\nopen(nome_do_ficheiro).readlines()\n\nescrever ficheiros\n\n# @title\nficheiro_escrita = g_path + 'texto_escrito.txt'\n\nwith open(ficheiro_escrita, 'w') as f:\n  f.write('Escrita a funcionar!')\n\nf.close()\n\n\n# @title\nficheiro_escrita = g_path + 'texto_escrito2.txt'\n\nlinhas = [\n    \"aaa\",\n    \"bbb\",\n    \"ccc\"\n]\n\nficheiro = open(ficheiro_escrita, 'w')\nficheiro.writelines(linhas)\nficheiro.close() # to change file access modes\n\n\n# @title\nficheiro_append = g_path + 'texto_append.txt'\n\nlinhas = [\n    \"uma linha\\n\",\n    \"duas linhas\\n\",\n    \"tantas linhas\\n\"\n]\n\nficheiro = open(ficheiro_append, 'a')\nficheiro.writelines(linhas)\nficheiro.close() # to change file access modes\n\n\n\n1.9.3 modulo maths\n\nimport math\n\ndir(math)\n\n['__doc__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'acos',\n 'acosh',\n 'asin',\n 'asinh',\n 'atan',\n 'atan2',\n 'atanh',\n 'cbrt',\n 'ceil',\n 'comb',\n 'copysign',\n 'cos',\n 'cosh',\n 'degrees',\n 'dist',\n 'e',\n 'erf',\n 'erfc',\n 'exp',\n 'exp2',\n 'expm1',\n 'fabs',\n 'factorial',\n 'floor',\n 'fmod',\n 'frexp',\n 'fsum',\n 'gamma',\n 'gcd',\n 'hypot',\n 'inf',\n 'isclose',\n 'isfinite',\n 'isinf',\n 'isnan',\n 'isqrt',\n 'lcm',\n 'ldexp',\n 'lgamma',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'modf',\n 'nan',\n 'nextafter',\n 'perm',\n 'pi',\n 'pow',\n 'prod',\n 'radians',\n 'remainder',\n 'sin',\n 'sinh',\n 'sqrt',\n 'sumprod',\n 'tan',\n 'tanh',\n 'tau',\n 'trunc',\n 'ulp']\n\n\n\n\n1.9.4 modulo statistics\n\nimport statistics\n\ndir(statistics)\n\nsample = [10,203,54,69,221,57,84,29,46,77]\n\n# o valor NaN (Not a Number) afecta o comportamento de muitas destas funções,\n# ou seja, convém remover os NaN das listas antes de invocar estas funções\nres = statistics.mean(sample)\nprint(\"Média: \", res)\n# fmean -&gt; mais rápido, converte todos os valores para float\n\nres = statistics.median(sample)\nprint(\"Mediana: \", res)\n#res = statistics.median_low(data)\n#res = statistics.median_high(data)\n\nres = statistics.stdev(sample)\nprint(\"Devsio padrão: \", res)\n# pstdev - toda a população\n\nres = statistics.mode(sample)\nprint(\"Moda: \", res)\n\nres = statistics.multimode(sample)\nprint(\"Modas (por ordem de aparecimento na lista): \", res)\n\nres = statistics.variance(sample)\nprint(\"Variância da amostra:\", res)\n# pvariance - toda a população\n\nMédia:  85\nMediana:  63.0\nDevsio padrão:  70.52816616233703\nModa:  10\nModas (por ordem de aparecimento na lista):  [10, 203, 54, 69, 221, 57, 84, 29, 46, 77]\nVariância da amostra: 4974.222222222223\n\n\n\n\n1.9.5 modulo random\n\nimport random\ndir(random)\n\nprint(f\"Um valor aleatório entre 0 e 1: {random.random()}\")\n\nUm valor aleatório entre 0 e 1: 0.4114041311241732"
  },
  {
    "objectID": "100-mod1.html#introdução-a-listas",
    "href": "100-mod1.html#introdução-a-listas",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.10 Introdução a listas",
    "text": "1.10 Introdução a listas\n\nlista = [5,7,9]\n\nsum(lista)\n#lista.sum()\n\n21\n\n\n\n# @title juntar listas\naves = [\"águia\", \"papagaio\", \"gaivota\"]\npeixes = [\"salmão\", \"tubarão\", \"carpa\"]\n\nanimais = aves + peixes\nprint(animais)\n\n['águia', 'papagaio', 'gaivota', 'salmão', 'tubarão', 'carpa']\n\n\n\nmamiferos = ['cao','gato','elefante']\n\nfor i in range(len(mamiferos)):\n  print(i, mamiferos[i])\n\n0 cao\n1 gato\n2 elefante\n\n\nacesso posicional\n\nfor i in reversed(lista):\n  print(i)\n\n9\n7\n5\n\n\nalterer listas append(), remove(), pop(),\n\nlista.append(11)\n\nlista.pop()\n\n11\n\n\n\ndel lista[2]\n\nprint(lista)\n\nprint(lista[1::2])\n\ninvertida = lista[::-1]\n\n[5, 7]\n[7]\n\n\nalterar listas\n\nlista = ['a','b',3,4]\n\nlista[2:] = 'r'\n\nprint(lista)\n\n['a', 'b', 'r']\n\n\nsem repetições - sample()\ncom repetições - choices()\nbaralhar - shuffle()\n\nprint(' '.join(['pequena', 'pausa']))\n\npequena pausa\n\n\n\n# @title remove o elemento na posição e devolve esse elemento\nlista = [2, 5, 3, 7]\nprint(\"lista:\",lista)\n\np = 1\nremovido = lista.pop(p) #\n\nprint(f'\\n{p+1}º elemento da lista: {removido}')\nprint(f'lista após remoção do {p+1}º elemento: {lista}')\n\nultimo = lista.pop()\nprint(f'lista após remoção do último elemento: {lista}')\n\nlista: [2, 5, 3, 7]\n\n2º elemento da lista: 5\nlista após remoção do 2º elemento: [2, 3, 7]\nlista após remoção do último elemento: [2, 3]\n\n\n\n# @title insere o elemento x na posição p da lista\nlista = [2, 3, 7]\nprint(\"lista:\",lista)\n\nx = 8\np = 2\nlista.insert(p, x)\n\nprint(f'\\napós inserção do {x} na {p+1}ª posição: {lista}')\n\nlista: [2, 3, 7]\n\napós inserção do 8 na 3ª posição: [2, 3, 8, 7]\n\n\nsegmentar listas (slicing) possiblidades de segmentação lista[ indice_pos_inicial : indice_pos_final : incremento_do_indice ]\n\n# @title\nlista = [1, 2, 3, 4, 5, 6]\n\npares = lista[1::2]\n\nprint(lista, pares, sep=\"\\n\")\n\n[1, 2, 3, 4, 5, 6]\n[2, 4, 6]\n\n\nslicing - alterando a lista\n\n# @title\nlista = [\"a\", \"b\", 3, 4]\n\nlista[2:] = [\"r\"]\n\nprint(lista)\n\n['a', 'b', 'r']\n\n\n\n1.10.1 exercicios\nver ficheiro mod1_ex_dia2_manha.py"
  },
  {
    "objectID": "100-mod1.html#strings-e-dicionários",
    "href": "100-mod1.html#strings-e-dicionários",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.11 Strings e dicionários",
    "text": "1.11 Strings e dicionários\nstring é uma sequencia de caracteres\nstr()\n\nstring = 'teste'\n\ndir(string)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\nunir - .join()\no dicionário é um array associativo: conjunto de chave / valor\ndict()\n\npaises_iso = {\n  'Portugal': 'PT',\n  'Espanha': 'ES',\n  'Franca': 'FR',\n  'Alemanha': 'DE',\n  'Brasil': 'BR',\n  'Argentina': 'AR'\n}\n\npaises_iso['Italia'] = 'IT'\n\nprint(paises_iso)\n\n{'Portugal': 'PT', 'Espanha': 'ES', 'Franca': 'FR', 'Alemanha': 'DE', 'Brasil': 'BR', 'Argentina': 'AR', 'Italia': 'IT'}\n\n\n\npaises_iso.get('Brasil')\n\n'BR'\n\n\n\n# for k, v in paises_iso:\n#   print(k,'{: }', sep = ': ')\n\nexercicio\n\nusers = {\n  '': 'pass1',\n  '': 'pass2',\n  '': 'pass3',\n  '': 'pass4'\n}"
  },
  {
    "objectID": "100-mod1.html#mais-código",
    "href": "100-mod1.html#mais-código",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.12 mais código",
    "text": "1.12 mais código\npodemos definir uma função com um input\n\ndef fib(n):\n  a, b = 0, 1\n  while a &lt; n:\n    print(a, end=' ')\n    a, b = b, a+b\n  print()\n  \nfib(1000)\n\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 \n\n\ne criar uma lista que queremos transformar e enumerar\n\n# python 3: list comprehensions\nfruits = ['Banana', 'Apple', 'Lime']\nloud_fruits = [fruit.upper() for fruit in fruits]\n\nprint(loud_fruits)\n\nlist(enumerate(fruits))\n\n['BANANA', 'APPLE', 'LIME']\n\n\n[(0, 'Banana'), (1, 'Apple'), (2, 'Lime')]\n\n\ne tentar um if else\n\nfechado = True\n\nif fechado:\n  print('porta fechada!')\nelse:\n  print('vamos lá!')\n\nporta fechada!\n\n\nOperadores lógicos\n\nTrue and False\n\nFalse\n\n\n\nTrue or False\n\nTrue\n\n\n\nnot True == False\n\nTrue"
  },
  {
    "objectID": "100-mod1.html#packages-1",
    "href": "100-mod1.html#packages-1",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.13 Packages",
    "text": "1.13 Packages\nFormas de importar\nimport &lt;modulo&gt;[, &lt;nome_modulo&gt;] from &lt;modulo&gt; import &lt;name(s)&gt; as &lt;alt_name&gt; import &lt;modulo&gt; as &lt;alt_name&gt;\npackages disponíveis:\n\n!pip freeze\n\nexemplo de importação de package\n\nimport humanize\n\nhumanize.i18n.activate(\"pt_PT\")\n\nprint(humanize.apnumber(4))\n\n\nimport random\n\nprint(random.random())\n\n0.6725263835327179"
  },
  {
    "objectID": "100-mod1.html#the-zen-of-python",
    "href": "100-mod1.html#the-zen-of-python",
    "title": "1  Programming Tecnhiques (Basics)",
    "section": "1.14 The Zen of Python",
    "text": "1.14 The Zen of Python\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\nLessons from the Zen of Python"
  },
  {
    "objectID": "200-mod2.html#estatística-descritiva-e-inferência",
    "href": "200-mod2.html#estatística-descritiva-e-inferência",
    "title": "2  Data Science (Basics)",
    "section": "2.1 Estatística Descritiva e Inferência",
    "text": "2.1 Estatística Descritiva e Inferência\n\\[D = \\{(\\vec{x}_1, y_1), ..., (\\vec{x}_n, y_n)\\} \\]\n\\((\\vec{x}, y) \\thicksim P\\)\ntipos de dados numericos: int, float\n\n# Creating integer variables\nx = 10\ny = -5\nz = 0\n\n\ntype(x)\n\nint\n\n\n\n# Creating float variables\na = 3.14\nb = -0.5\nc = 2.0\n\n# Using the variables\n(a + b) * c\n\n5.28\n\n\ntipo sequencia: list, tuple, range, numpy.array\n\n# Creating a list of integers\nmy_list = [1, 2, 3, 4, 5]\n\n# Creating a list of strings\nnames = ['Alice', 'Bob', 'Charlie']\n\n# Creating a mixed-type list\nmixed_list = [1, 'hello', 3.14, True]\n\n\n# Checking my_list\nmy_list\n\n[1, 2, 3, 4, 5]\n\n\nos elementos das listas são indexados\n\n# Accessing elements in the list my_list\nmy_list = [1, 2, 3, 4, 5]\n\n# Accessing the first element (index 0)\nfirst_element = my_list[0]\nprint(first_element)  \n\n# Accessing the third element (index 2)\nthird_element = my_list[2]\nprint(third_element)  \n\n# Accessing the last element\nlast_element = my_list[-1]\nprint(last_element)  \n\n# Accessing the second-to-last element\nsecond_to_last_element = my_list[-2]\nprint(second_to_last_element)  \n\n1\n3\n5\n4\n\n\nnum intervalo incluimos o primeiro index mas não o último\n\n# Slicing to get a subset of elements\nsubset = my_list[1:4] \nprint(subset)  \n\n[2, 3, 4]\n\n\no operador * permite repetir a lista\n\n# Using list repetition\nmy_list * 2\n\nmy_list + my_list\n\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\npara multiplicar cada elemento da lista:\n\n# Multiply every element of my_list by 2\nresult = [x * 2 for x in my_list]\nprint(result) \n\n[2, 4, 6, 8, 10]\n\n\nA função lambda em Python é uma função anónima:\n\nmy_list = [1, 2, 3, 4, 5]\nresult = list(map(lambda x: x * 2, my_list))\nprint(result)  \n\n[2, 4, 6, 8, 10]\n\n\npara alterar a lista original temos de fazer um ciclo\n\n# Update each element in the list by multiplying it by 2\nfor i in range(len(my_list)):\n    my_list[i] *= 2\n\nprint(my_list)  \n\n[2, 4, 6, 8, 10]\n\n\nordenação\n\n# Sort my_list in descending order\nmy_list.sort(reverse=True)\n\nprint(my_list) \n\n[10, 8, 6, 4, 2]\n\n\nconcatenar as listas usando o método extend\n\nmy_list = [1, 2, 3]\nanother_list = [4, 5, 6]\n\n# Concatenate another_list to my_list using the + operator\nmy_list += another_list\n\n# Concatenate another_list to my_list using the extend() method\nmy_list.extend(another_list)\n\nprint(my_list)  \n\n[1, 2, 3, 4, 5, 6, 4, 5, 6]\n\n\nou com append\n\n# Append each element from another_list to my_list using the append() method\nfor element in another_list:\n    my_list.append(element)\n\nprint(my_list)  \n\n[1, 2, 3, 4, 5, 6, 4, 5, 6, 4, 5, 6]\n\n\neliminar elementos da lista por index\n\nmy_list = [1, 2, 3, 4, 5]\n\n# Remove the element at index 2 (which is 3) from my_list\ndel my_list[2]\n\nprint(my_list)  \n\n[1, 2, 4, 5]\n\n\nOs tuplos são normalmente usados em vez de listas quando pretendemos que sejam imutáveis, por exemplo coordenadas, configurações, chaves de um dicionário…\n\n# Creating a tuple\nmy_tuple = (1, 2, 3, 4, 5)\nprint(my_tuple) \n\ncoordinates = {(0, 0): 'origin', (1, 1): 'diagonal'}\ncoordinates\n\n(1, 2, 3, 4, 5)\n\n\n{(0, 0): 'origin', (1, 1): 'diagonal'}\n\n\nAs funções podem retornar múltiplos valores na forma de tuplo, permitindo um código conciso e eficiente. A descompactação de tuplos (tuple unpacking) é frequentemente usada para extrair os valores.\n\n# Tuple unpacking\ndef get_coordinates():\n    return 10, 20\n\nx, y = get_coordinates()\nprint(\"x:\", x)\nprint(\"y:\", y)\n\nx: 10\ny: 20\n\n\nranges\n\n# Create a range of eggs\neggs = range(10)\neggs\n\nrange(0, 10)\n\n\n\n# print all the elements in the range\nfor egg in eggs:\n    print(egg, end=\" \")\nprint()\n\ntype(eggs)\n\nlen(eggs)\n\nsum(eggs)\n\n0 1 2 3 4 5 6 7 8 9 \n\n\n45\n\n\ntipo texto (strings): str\ntipo booleano: bool\n\ndef is_even(number):\n    \"\"\"\n    Check if the given number is even.\n    \"\"\"\n    return number % 2 == 0\n\n# Test the function\nprint(is_even(4))  \nprint(is_even(7))  \n\nnum = 18\nif is_even(num):\n    print(f\"{num} is even.\")\nelse:\n    print(f\"{num} is odd.\")\n\nTrue\nFalse\n18 is even.\n\n\ntipo categorico: pandas.Categorical\ndados temporais: datetime, panda.Series, pandas.DataFrame\ndistribuições estatisticas: scipy.stats\nArrays: numpy.array\n\n2.1.1 Dados quantitativos\n\nalturas = [1.65,1.73,1.78,1.67,1.82,1.76,1.75,1.74,1.75,1.67,1.67,1.69]\nlen(alturas)\ntype(alturas)\nalturas.sort(reverse=True)\nmedia_alturas = sum(alturas)/len(alturas)\nmedia_alturas\n\n1.7233333333333334\n\n\nrepresentação gráfica com um scatterplot\n\npesos = [92, 87, 102, 78, 87, 76, 69, 62, 63, 58, 61, 62]\n\n# importing the required module\nimport matplotlib.pyplot as plt\n  \n# x axis values\nx = alturas\n# corresponding y axis values\ny = pesos\n  \n# plotting points as a scatter plot\n# s - point size, alpha - opacity\nplt.scatter(x, y, color= \"green\", marker= \"*\", s=40)\n\n# naming the x axis\nplt.xlabel('Altura em m')\n# naming the y axis\nplt.ylabel('Peso em Kg')\n  \n# giving a title to my graph\nplt.title('Alturas e Pesos dos Formandos')\n  \n# function to show the plot\nplt.show()\n\n?plt.scatter\n\n\n\n\n\n\n2.1.2 Dados quantitativos\n\nestado_civil = [\"solteiro\", \"casado\", \"solteiro\", \"divorciado\", \"solteiro\", \"solteiro\",\n               \"casado\", \"solteiro\", \"casado\", \"solteiro\", \"divorciado\", \"solteiro\"]\n              \nsolteiro_count = estado_civil.count(\"solteiro\")\n\ncasado_count = estado_civil.count(\"casado\")\n\ndivorciado_count = estado_civil.count(\"divorciado\")\ndivorciado_count\n\n2\n\n\nrepresentação gráfica (barras)\n\n# heights of bars\nheight = [solteiro_count, casado_count, divorciado_count]\n  \n# labels for bars\ntick_label = ['solteiro', 'casado', 'divorciado']\n  \n# plotting a bar chart\nplt.bar(tick_label, height, width = 0.8, color = 'blue')\n  \n# naming the x-axis\nplt.xlabel('Estado Civil')\n# naming the y-axis\nplt.ylabel('Nº de Indivíduos')\n# plot title\nplt.title('Frequências dos Estados Civis')\n  \n# function to show the plot\nplt.show()\n\n\n\n\nA partir de um dicionário\n\nalturas_dict = { \"Teresa\": 165, \"Maria\": 169, \"Joao\": 178, \"Carlos\": 187,\n                \"Vasco\": 182, \"Joana\": 162, \"Sofia\": 165, \"Pedro\": 177,\n                \"Afonso\": 175, \"Miguel\": 177, \"Ana\": 163, \"Margarida\": 162}\n\n# converter para uma lista para depois fazerf o gráfico                \nalturas_list = list(alturas_dict.values())\nalturas_list\n\n[165, 169, 178, 187, 182, 162, 165, 177, 175, 177, 163, 162]\n\n\nagora um histograma\n\nplt.hist(alturas_list, 4)\n\nplt.show()\n\n\n\n\nestatisticas\n\n# Calcula a média usando a fórmula\nn = len(alturas_list)\nmed_alturas_list= sum(alturas_list)/n\n\nprint(\"Median: {0}\".format(med_alturas_list))\n\nMedian: 171.83333333333334\n\n\n\n# Calcula a soma dos desvios quadrados\nss_alturas_list = sum((x - med_alturas_list)**2 for x in alturas_list)\nprint(ss_alturas_list)\n\n807.6666666666666\n\n\n\n# Variância amostral corrigida (da população) com ddof = 0\n# Variância amostral não corrigida com ddof = 1\nddof = 0\nvar_alturas_list = ss_alturas_list/(n-ddof)\nprint(var_alturas_list)\n\n# Desvio Padrão (corrigido - para alterar voltar ao passo anterior da variância)\ndp_alturas_list = var_alturas_list**0.5\nprint(dp_alturas_list)\n\n67.30555555555556\n8.203996316159312\n\n\nusando uma pckage\n\nimport statistics as st\n\nprint(\"The mean is:\", st.mean(alturas_list))\nprint(\"The mode is:\", st.mode(alturas_list))\nprint(\"The median is:\", st.median(alturas_list))\nprint(\"The sample variance is:\", st.variance(alturas_list))\nprint(\"The population variance is:\",st.pvariance(alturas_list))\nprint(\"The sample standard deviation is:\",st.stdev(alturas_list))\nprint(\"The population standard deviation is:\",st.pstdev(alturas_list))\n\nprint(\"The median is:\", st.median(alturas_list))\n# N = 4 devolve os 3 quartis superiores - percentis 25 e 75\nprint(\"The first three quartiles are:\", st.quantiles(alturas_list, n = 4))\n\nprint(\"The median is:\", st.median(alturas_list))\n# N = 10 devolve  - percentil 10, 20, ... , 90\nprint(\"The percentiles are:\", st.quantiles(alturas_list, n = 10))\n\nThe mean is: 171.83333333333334\nThe mode is: 165\nThe median is: 172.0\nThe sample variance is: 73.42424242424242\nThe population variance is: 67.30555555555556\nThe sample standard deviation is: 8.568794689117158\nThe population standard deviation is: 8.203996316159312\nThe median is: 172.0\nThe first three quartiles are: [163.5, 172.0, 177.75]\nThe median is: 172.0\nThe percentiles are: [162.0, 162.6, 164.8, 165.8, 172.0, 176.6, 177.1, 179.6, 185.5]\n\n\n\n\n2.1.3 Numpy\n\nimport numpy as np\n\no vector (array) é o objecto principal no NumPy\ncriar array a partir de lista\n\nalturas_list\n\narray = np.array(alturas_list)\narray\n\narray([165, 169, 178, 187, 182, 162, 165, 177, 175, 177, 163, 162])\n\n\nOs elementos dos array têm de ser todos do mesmo tipo.\narray 2D e 3D\n\narray_2D = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\narray_2D\n\nint_list = [[[1,2,4,4,5], [5,7,7,9,3], [3,2,5,3,6], [6,8,9,5,1]],\n             [[8,9,3,4,3], [4,6,4,3,3], [2,6,3,6,6], [8,5,7,2,1]],\n             [[1,2,7,9,5], [4,8,7,7,3], [8,5,7,6,7], [2,4,4,5,4]]]\narray_3D = np.array(int_list)\narray_3D\n\narray([[[1, 2, 4, 4, 5],\n        [5, 7, 7, 9, 3],\n        [3, 2, 5, 3, 6],\n        [6, 8, 9, 5, 1]],\n\n       [[8, 9, 3, 4, 3],\n        [4, 6, 4, 3, 3],\n        [2, 6, 3, 6, 6],\n        [8, 5, 7, 2, 1]],\n\n       [[1, 2, 7, 9, 5],\n        [4, 8, 7, 7, 3],\n        [8, 5, 7, 6, 7],\n        [2, 4, 4, 5, 4]]])\n\n\nCriar um array a partir do zero\n\n# Criar um array de inteiros de tamanho 10 (length-10) preenchido a zeros\nnp.zeros(10, dtype=int)\n\n# Criar um array de 3x5 (3 linhas e 5 colunas) com dados do tipo floating-point preenchido a 1s\nnp.ones((3, 5), dtype=float)\n# criar arrau 3D\nnp.ones((2, 4,6), dtype=float)\n\n# Criar um array de 3x5 (3 linhas e 5 colunas) com 3.14\nnp.full((3, 5), 3.14)\n\n# Criar um array de -3 a 4 com espaçamento igual entre os seus elementos\n# atenção que o valor inicial está incluido mas o de stop não\nnp.arange(-3,4)\n\n# Criar um array preenchido com uma sequência de 0 até 20 saltando de 2 em 2\n# quando se passa um terceiro argumento é interpretado como o espaçamento\nnp.arange(0, 20, 2)\n\n# Criar um array de 5 valores igualmente espaçados entre 0 e 1\nnp.linspace(0, 1, 5)\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\nos array são iteráveis\ntodos os iteradores são iteráveis (o contrário não é válido)\n\nclass SquaresIterator:\n    def __init__(self, n):\n        self.n = n\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.current &gt;= self.n:\n            raise StopIteration\n        result = self.current ** 2\n        self.current += 1\n        return result\n\n# Criar o iterador\niterator = SquaresIterator(5)\n\n# Usar o iterador para mostrar os valores\nwhile True:\n    try:\n        num = next(iterator)\n        print(num)\n    except StopIteration:\n        break\n\n0\n1\n4\n9\n16\n\n\nfunção gerador\n\n# Definir o gerador\ndef squares(n):\n    current = 0\n    while current &lt; n:\n        yield current ** 2\n        current += 1\n\n# Criar o iterador\nsquares_generator = squares(5)\n\n# Usar o iterador para imprimir os valores\nfor num in squares_generator:\n    print(num)\n\n0\n1\n4\n9\n16\n\n\nfazer um contador regressivo\n\n# Definir o gerador\ndef countdown(n):\n  while n &gt;= 0:\n    yield n \n    n -= 1\n\n# Criar o iterador instanciado com o valor 3\ncountdown_generator = countdown(3)\n\n# Usar o iterador para imprimir os valores\nfor minutos in countdown_generator:\n    print(minutos)\n\n3\n2\n1\n0\n\n\ncriar amostras pseudo/aleatorias&gt;\n\n# Criar um array de 3x5 de valores aleatórios entre 0 e 1\n# a partir de uma distribuição uniforme contínua.\n# cada número gerado tem a mesma probabilidade de ocorrer dentro do intervalo\nnp.random.random((3, 5))\n\n# Criar um array de 3x3 de números aleatórios com uma distribuição normal\n# com média 0 e desvio padrão 1\nnp.random.normal(0, 1, (3, 3))\n\n# Criar um array de 3x3 de números aleatórios no intervalo de [0, 10[\n# notem que 0 pertece ao intervalo mas 10 não...\nnp.random.randint(0, 10, (3, 3))\n\narray([[0, 6, 3],\n       [2, 3, 5],\n       [8, 8, 1]])\n\n\ndefinir semente\n\n# Define a semente\nnp.random.seed(42)\n\n# Gera uma matriz 3x3 de números aleatórios duma distribuição normal\nnormal_array = np.random.normal(loc=0, scale=1, size=(3, 3))\n\nprint(normal_array)\n\n# Criar uma matriz identidade de 5x5\nnp.eye(5)\n\n[[ 0.49671415 -0.1382643   0.64768854]\n [ 1.52302986 -0.23415337 -0.23413696]\n [ 1.57921282  0.76743473 -0.46947439]]\n\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\nexercicios\n\n# Proposta de Exercicio\n# Converta a lista de sudoku num array e mostre no écran o tipo da nova variável\n# sudoku_array para demonstrar que o código funcionou correctamente\nsudoku_list = [\n  [0, 0, 4, 3, 0, 0, 2, 0, 9], [0, 0, 5, 0, 0, 9, 0, 0, 1], [0, 7, 0, 0, 6, 0, 0, 4, 3],\n  [0, 0, 6, 0, 0, 2, 0, 8, 7], [1, 9, 0, 0, 0, 7, 4, 0, 0], [0, 5, 0, 0, 8, 3, 0, 0, 0],\n  [6, 0, 0, 0, 0, 0, 1, 0, 5], [0, 0, 3, 5, 0, 8, 6, 9, 0], [0, 4, 2, 9, 1, 0, 3, 0, 0]\n               ]\n               \nprint(type(sudoku_list))\n\n&lt;class 'list'&gt;\n\n\npassar para array\n\nsudoku_array = np.array(sudoku_list)\nprint(type(sudoku_array))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nCriar um array de zeros com 4 colunas e 10 linhas\n\n# Proposta de Exercicio\n# mostrar no écran o array para demonstrar que o código funcionou correctamente\nzero_array = np.zeros((10,4))\nzero_array\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\n# criação do doubling array que será mostrado no eixo do y\ndoubling_array = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n\n# criação do one_to_ten array que será mostrado no eixo do x\none_to_ten =np.arange(1,11)\n\n# código de display do plot que terá de ser adaptado\nplt.scatter(one_to_ten, doubling_array)\nplt.show()\n\n\n\n\n\n\n2.1.4 Atributos dos arrays NumPy\nobjecto do tipo gerador para numeros aleatorios\n\nrng = np.random.default_rng(seed=2012) \ntype(rng)\n\nnumpy.random._generator.Generator\n\n\ncria array com valores até 3 com 6 elementos\n\nx1 = rng.integers(3, size = 6)  \nx1\n\narray([2, 0, 1, 0, 0, 1], dtype=int64)\n\n\n\nrng = np.random.default_rng(seed=2012)  # seed for reproducibility\n\nx2 = rng.integers(10, size=(3, 4))  # two-dimensional array\nprint('x2 tem {0} dimensões e uma shape {1} o seu conteúdo é '.format(x2.ndim, x2.shape))\nprint('{0}'.format(x2))\n\nx2 tem 2 dimensões e uma shape (3, 4) o seu conteúdo é \n[[8 2 4 0]\n [1 3 4 2]\n [4 8 7 7]]\n\n\nmultidimensoes\n\nrng = np.random.default_rng(seed=2012)  # seed for reproducibility\n\nx5 = rng.integers(10, size=(3, 4, 2, 5, 9))  # two-dimensional array\nprint('x5 tem {0} dimensões e uma shape {1} o seu conteúdo é '.format(x5.ndim, x5.shape))\n\n#print('{0}'.format(x5))\n\nx5 tem 5 dimensões e uma shape (3, 4, 2, 5, 9) o seu conteúdo é \n\n\nmudar a shape dos arrays NumPy\n\nx1=np.arange(1,6)\nprint(x1)\nprint(x1.shape)\n\n[1 2 3 4 5]\n(5,)\n\n\nflatten(), reshape()\n\narray = np.array([[1,2], [3,4], [5,6]])\n\nflat_array = array.flatten()\n\ndiff_array = flat_array.reshape(2,3)\n\nprint(array)\n\n[[1 2]\n [3 4]\n [5 6]]\n\n\n\nprint(flat_array.shape, flat_array, flat_array.size)\n\nprint(diff_array.shape, diff_array, diff_array.size, diff_array.dtype, diff_array.ndim)\n\n(6,) [1 2 3 4 5 6] 6\n(2, 3) [[1 2 3]\n [4 5 6]] 6 int32 2\n\n\n\nnp.array([1.32, 5.78, 175.55]).dtype\n\nnp.array([\"Python\", \"para\", \"Estatísticas\",\"Oficiais\", \"otorrinolaringologista\"]).dtype\n\nboolean_array = np.array([[True, False], [False, False]], dtype=np.bool_)\nprint(boolean_array.dtype)\nboolean_array.astype(np.int32)\n\nbool\n\n\narray([[1, 0],\n       [0, 0]])\n\n\n\nnp.array([True, 42, 42.42, \"Hitchikers' Guide to the Galaxy\"]).dtype\n\ndtype('&lt;U32')\n\n\nexercicios\n\nsudoku_game = np.array([[0, 0, 4, 3, 0, 0, 2, 0, 9],\n       [0, 0, 5, 0, 0, 9, 0, 0, 1],\n       [0, 7, 0, 0, 6, 0, 0, 4, 3],\n       [0, 0, 6, 0, 0, 2, 0, 8, 7],\n       [1, 9, 0, 0, 0, 7, 4, 0, 0],\n       [0, 5, 0, 0, 8, 3, 0, 0, 0],\n       [6, 0, 0, 0, 0, 0, 1, 0, 5],\n       [0, 0, 3, 5, 0, 8, 6, 9, 0],\n       [0, 4, 2, 9, 1, 0, 3, 0, 0]])\n\nsudoku_solution = np.array([[8, 6, 4, 3, 7, 1, 2, 5, 9],\n       [3, 2, 5, 8, 4, 9, 7, 6, 1],\n       [9, 7, 1, 2, 6, 5, 8, 4, 3],\n       [4, 3, 6, 1, 9, 2, 5, 8, 7],\n       [1, 9, 8, 6, 5, 7, 4, 3, 2],\n       [2, 5, 7, 4, 8, 3, 9, 1, 6],\n       [6, 8, 9, 7, 3, 4, 1, 2, 5],\n       [7, 1, 3, 5, 2, 8, 6, 9, 4],\n       [5, 4, 2, 9, 1, 6, 3, 7, 8]])\n\n\n# Usando os arrays já definidos sudoku_game e sudoku_solution \n# crie um array 3D com o jogo e a solução\ngame_and_solution = np.array([sudoku_game, sudoku_solution])\nprint(game_and_solution)\ngame_and_solution.shape\n\n[[[0 0 4 3 0 0 2 0 9]\n  [0 0 5 0 0 9 0 0 1]\n  [0 7 0 0 6 0 0 4 3]\n  [0 0 6 0 0 2 0 8 7]\n  [1 9 0 0 0 7 4 0 0]\n  [0 5 0 0 8 3 0 0 0]\n  [6 0 0 0 0 0 1 0 5]\n  [0 0 3 5 0 8 6 9 0]\n  [0 4 2 9 1 0 3 0 0]]\n\n [[8 6 4 3 7 1 2 5 9]\n  [3 2 5 8 4 9 7 6 1]\n  [9 7 1 2 6 5 8 4 3]\n  [4 3 6 1 9 2 5 8 7]\n  [1 9 8 6 5 7 4 3 2]\n  [2 5 7 4 8 3 9 1 6]\n  [6 8 9 7 3 4 1 2 5]\n  [7 1 3 5 2 8 6 9 4]\n  [5 4 2 9 1 6 3 7 8]]]\n\n\n(2, 9, 9)\n\n\n\nnew_sudoku_game = np.array([[0, 0, 4, 3, 0, 0, 2, 0, 9],\n       [0, 0, 5, 0, 0, 9, 0, 0, 1],\n       [0, 7, 0, 0, 6, 0, 0, 4, 3],\n       [0, 0, 6, 0, 0, 2, 0, 8, 7],\n       [1, 9, 0, 0, 0, 7, 4, 0, 0],\n       [0, 5, 0, 0, 8, 3, 0, 0, 0],\n       [6, 0, 0, 0, 0, 0, 1, 0, 5],\n       [0, 0, 3, 5, 0, 8, 6, 9, 0],\n       [0, 4, 2, 9, 1, 0, 3, 0, 0]])\n\nnew_sudoku_solution = np.array([[8, 6, 4, 3, 7, 1, 2, 5, 9],\n       [3, 2, 5, 8, 4, 9, 7, 6, 1],\n       [9, 7, 1, 2, 6, 5, 8, 4, 3],\n       [4, 3, 6, 1, 9, 2, 5, 8, 7],\n       [1, 9, 8, 6, 5, 7, 4, 3, 2],\n       [2, 5, 7, 4, 8, 3, 9, 1, 6],\n       [6, 8, 9, 7, 3, 4, 1, 2, 5],\n       [7, 1, 3, 5, 2, 8, 6, 9, 4],\n       [5, 4, 2, 9, 1, 6, 3, 7, 8]])\n\n\n# Usando os novos já definidos new_sudoku_game e new_sudoku_solution\n# crie um novo array 3D com o novo jogo e a nova solução.\nnew_game_and_solution = np.array([new_sudoku_game, new_sudoku_solution])\n\n# Depois usando o arrays 3D do exercicio anterior e o agora criado \n# agrupe-os num novo array 4D\ngames_and_solutions = np.array([game_and_solution, new_game_and_solution])\n\n# Verifique a sua shape\ngames_and_solutions.shape\n\n(2, 2, 9, 9)\n\n\n\nprint(games_and_solutions)\n\n[[[[0 0 4 3 0 0 2 0 9]\n   [0 0 5 0 0 9 0 0 1]\n   [0 7 0 0 6 0 0 4 3]\n   [0 0 6 0 0 2 0 8 7]\n   [1 9 0 0 0 7 4 0 0]\n   [0 5 0 0 8 3 0 0 0]\n   [6 0 0 0 0 0 1 0 5]\n   [0 0 3 5 0 8 6 9 0]\n   [0 4 2 9 1 0 3 0 0]]\n\n  [[8 6 4 3 7 1 2 5 9]\n   [3 2 5 8 4 9 7 6 1]\n   [9 7 1 2 6 5 8 4 3]\n   [4 3 6 1 9 2 5 8 7]\n   [1 9 8 6 5 7 4 3 2]\n   [2 5 7 4 8 3 9 1 6]\n   [6 8 9 7 3 4 1 2 5]\n   [7 1 3 5 2 8 6 9 4]\n   [5 4 2 9 1 6 3 7 8]]]\n\n\n [[[0 0 4 3 0 0 2 0 9]\n   [0 0 5 0 0 9 0 0 1]\n   [0 7 0 0 6 0 0 4 3]\n   [0 0 6 0 0 2 0 8 7]\n   [1 9 0 0 0 7 4 0 0]\n   [0 5 0 0 8 3 0 0 0]\n   [6 0 0 0 0 0 1 0 5]\n   [0 0 3 5 0 8 6 9 0]\n   [0 4 2 9 1 0 3 0 0]]\n\n  [[8 6 4 3 7 1 2 5 9]\n   [3 2 5 8 4 9 7 6 1]\n   [9 7 1 2 6 5 8 4 3]\n   [4 3 6 1 9 2 5 8 7]\n   [1 9 8 6 5 7 4 3 2]\n   [2 5 7 4 8 3 9 1 6]\n   [6 8 9 7 3 4 1 2 5]\n   [7 1 3 5 2 8 6 9 4]\n   [5 4 2 9 1 6 3 7 8]]]]\n\n\n\n\n2.1.5 Aceder aos elementos\n\nx1\n\n# Aceder ao primeiro elemento \n# (começa em zero)\nprint(\"O primeiro elemento é {0} e o segundo {1}\" .format(x1[0], x1[1]))\n\n# Aceder ao ante-penultimo elemento\nx1[-3]\n\n# Aceder ao 1º elemento da 2ª linha\n# (linha e coluna começam em zero)\nx2\nx2[2,3]\n\nO primeiro elemento é 1 e o segundo 2\n\n\n7\n\n\nslicing arrays\n\nx1=np.arange(0,10)\n\nx1\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nx1[:3]\n\nx1[3:]\n\nx1[1:6:2]\n\narray([1, 3, 5])\n\n\nmodificar células\n\nx1[0] = 11\nx1\n\nx2[0,2] = 5\nx2\n\nx2[-1,0] = 6\nx2\n\narray([[8, 2, 5, 0],\n       [1, 3, 4, 2],\n       [6, 8, 7, 7]], dtype=int64)\n\n\nSubarrays Multidimensionais\n\nprint(x2)\n# fatia das 2 primeiras linhas\n# as 3 primeiras colunas\nx2[:2, :3] \n\nx2[0:2, 0:2]\n\nx2[::-1, ::-1]\n\n# fatia a primeira coluna de x2\nx2[:, 0] \n\n# igual a x2[0, :]\nx2[0, 0:4]\n\n[[8 2 5 0]\n [1 3 4 2]\n [6 8 7 7]]\n\n\narray([8, 2, 5, 0], dtype=int64)\n\n\nCriar Cópias de Subarrays\n\n# x2_sub é uma view e não uma cópia!!!\nx2_sub = x2[:2, :2]\nprint(x2_sub)\n\nx2_sub[0, 0] = 99\nprint(x2_sub)\nprint(x2)\n\nx2_sub_copy = x2[:2, :2].copy()\nprint(x2_sub_copy)\n\nx2_sub_copy[0, 0] = 42\nprint(x2_sub_copy)\nprint(x2)\n\n[[8 2]\n [1 3]]\n[[99  2]\n [ 1  3]]\n[[99  2  5  0]\n [ 1  3  4  2]\n [ 6  8  7  7]]\n[[99  2]\n [ 1  3]]\n[[42  2]\n [ 1  3]]\n[[99  2  5  0]\n [ 1  3  4  2]\n [ 6  8  7  7]]\n\n\nReshape do Array\n\n# criar uma grid de 3 por 3\ngrid = np.arange(1, 10).reshape(3, 3)\nprint(grid)\n\ngrid.reshape((1, 9))\n\ngrid.reshape((9,1))\n\n# criar um vector em linha através do reshape\nx = np.array([1, 2, 3])\nx\n\n# criar um vector em linha através do newaxis\nx[np.newaxis, :] \n\n# criar um vector em coluna através do reshape\nx.reshape((3, 1))\n\n# criar um vector em coluna através do newaxis\nx[: ,np.newaxis] \n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\narray([[1],\n       [2],\n       [3]])\n\n\nConcatenação de Arrays\n\nx = np.array([1, 2, 3])\ny = np.array([3, 2, 1])\nnp.concatenate([x, y])\n\n# concatenar mais do que 2 arrays de cada vez\nz = np.array([99, 99, 99])\nprint(np.concatenate([x, y, z]))\n\n[ 1  2  3  3  2  1 99 99 99]\n\n\n\ngrid = np.array([[1, 2, 3],\n                 [4, 5, 6]])\n# concatenar ao longo do eixo 1\nnp.concatenate([grid, grid])\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [1, 2, 3],\n       [4, 5, 6]])\n\n\n\n# concatenar ao longo do 2º eixo\n# (indice dos eixos começa em zero)\nnp.concatenate([grid, grid], axis=1)\n\n# stack vertical de arrays\nnp.vstack([x, grid])\n\n# stack horizontal de arrays\ny = np.array([[99],\n              [99]])\nnp.hstack([grid, y])\n\nx_exp0 = np.expand_dims(x, axis=0)\nprint(x)\nprint(x_exp0)\n\n[1 2 3]\n[[1 2 3]]\n\n\n\n# Definindo duas matrizes bidimensionais\nx = np.array([[1, 2],\n              [3, 4]])\n\ny = np.array([[5, 6],\n              [7, 8]])\n\n# Adicionando uma dimensão extra às matrizes\nx_expandido = np.expand_dims(x, axis=2)\ny_expandido = np.expand_dims(y, axis=2)\n\n# Concatenando as matrizes ao longo da terceira dimensão usando np.dstack()\nresultado = np.dstack((x_expandido, y_expandido))\n\nprint(x)\nprint(x_expandido)\nprint(resultado)\n\nx_exp1 = np.expand_dims(x, axis=1)\nx_exp1\n\nx_exp3 = np.expand_dims(x, axis=2)\nx_exp3\n\nx_exp2 = np.expand_dims(x, axis=2)\nx_exp2\n\n[[1 2]\n [3 4]]\n[[[1]\n  [2]]\n\n [[3]\n  [4]]]\n[[[1 5]\n  [2 6]]\n\n [[3 7]\n  [4 8]]]\n\n\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]])\n\n\nSplit de Arrays\n\nx = [1, 2, 3, 99, 99, 3, 2, 1]\nx1, x2, x3 = np.split(x, [3, 5])\nprint(x1, x2, x3)\n\ngrid = np.arange(16).reshape((4, 4))\ngrid\n\nupper, lower = np.vsplit(grid, [2])\nprint(upper)\nprint(lower)\n\nleft, right = np.hsplit(grid, [2])\nprint(left)\nprint(right)\n\n[1 2 3] [99 99] [3 2 1]\n[[0 1 2 3]\n [4 5 6 7]]\n[[ 8  9 10 11]\n [12 13 14 15]]\n[[ 0  1]\n [ 4  5]\n [ 8  9]\n [12 13]]\n[[ 2  3]\n [ 6  7]\n [10 11]\n [14 15]]\n\n\nAritmética de Arrays\nos operadores aritméticos são universal functions (Ufuncs)\n\nx = np.arange(4)\nprint(\"x      =\", x)\nprint(\"x + 5  =\", x + 5)\nprint(\"x - 5  =\", x - 5)\nprint(\"x * 2  =\", x * 2)\nprint(\"x / 2  =\", x / 2)\nprint(\"x // 2 =\", x // 2)  # divisão inteira\n\nprint(\"-x     = \", -x)     # - negação\nprint(\"x ** 2 = \", x ** 2) # ** quadrado\nprint(\"x ** 3 = \", x ** 3) # ** cubo\nprint(\"x % 2  = \", x % 2)  # % resto da divisão\n\nprint(\"e^x =\", np.exp(x))       # exponencial de base e\nprint(\"2^x =\", np.exp2(x))      # exponencial de base 2\nprint(\"3^x =\", np.power(3., x)) # exponencial de base 3\n\n# as operacções inversas das exponenciais, os logaritmos\n# também estão disponíveis\nx = [1, 2, 4, 10]\nprint(\"x        =\", x)\nprint(\"ln(x)    =\", np.log(x))\nprint(\"log2(x)  =\", np.log2(x))\nprint(\"log10(x) =\", np.log10(x))\n\nx      = [0 1 2 3]\nx + 5  = [5 6 7 8]\nx - 5  = [-5 -4 -3 -2]\nx * 2  = [0 2 4 6]\nx / 2  = [0.  0.5 1.  1.5]\nx // 2 = [0 0 1 1]\n-x     =  [ 0 -1 -2 -3]\nx ** 2 =  [0 1 4 9]\nx ** 3 =  [ 0  1  8 27]\nx % 2  =  [0 1 0 1]\ne^x = [ 1.          2.71828183  7.3890561  20.08553692]\n2^x = [1. 2. 4. 8.]\n3^x = [ 1.  3.  9. 27.]\nx        = [1, 2, 4, 10]\nln(x)    = [0.         0.69314718 1.38629436 2.30258509]\nlog2(x)  = [0.         1.         2.         3.32192809]\nlog10(x) = [0.         0.30103    0.60205999 1.        ]\n\n\nProdutos Externos\n\nx = np.arange(1, 6)\nx\n\nnp.multiply.outer(x, x)\n\narray([[ 1,  2,  3,  4,  5],\n       [ 2,  4,  6,  8, 10],\n       [ 3,  6,  9, 12, 15],\n       [ 4,  8, 12, 16, 20],\n       [ 5, 10, 15, 20, 25]])\n\n\nAgregações\n\nx = np.arange(1, 6)\nx\n\nnp.add.reduce(x)\n\nnp.multiply.reduce(x)\n\nnp.add.accumulate(x)\n\nnp.multiply.accumulate(x)\n\nrng = np.random.default_rng()\nbig_array = rng.random(1000000)\n\n# %timeit sum(big_array)\n# %timeit np.sum(big_array)\n\nnp.min(big_array), np.max(big_array)\n\n(1.2519977897751389e-06, 0.999999823820084)\n\n\nEstatísticas Básicas\n\n# Alturas dos primeiros ministros portugueses\nalturas = np.array([169, 170, 159, 173, 173, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183, 182,\n                   178, 173, 174, 173, 176, 164, 170, 173, 182, 180, 183, 178, 182, 174, 175, 179,\n                   174, 173, 162, 173, 171, 165, 164, 168, 175, 165, 181, 172])\n                   \nprint(\"Média das Alturas:       \", np.mean(alturas))\nprint(\"Desvio Padrão:           \", np.std(alturas))\nprint(\"Altura Minima:    \", np.min(alturas))\nprint(\"Altura Máxima:    \", np.max(alturas))\n\nprint(\"Percentil 25:   \", np.percentile(alturas, 25))\nprint(\"Mediana:            \", np.median(alturas))\nprint(\"Percentil 75:   \", np.percentile(alturas, 75))\n\n#%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.hist(alturas,6)\nplt.title('Distribuição das Alturas dos Primeiro Ministros')\nplt.xlabel('Altura (cm)')\nplt.ylabel('Número');\n\nimport scipy.stats as st\n\n# criar um intervalo de confiança a 95% para a altura média da população\n# usando a distribuição normal\nst.norm.interval(confidence=0.95, loc=np.mean(alturas), scale=st.sem(alturas))\n\n#%pwd\n\nMédia das Alturas:        173.75\nDesvio Padrão:            6.049511625667885\nAltura Minima:     159\nAltura Máxima:     185\nPercentil 25:    170.75\nMediana:             173.0\nPercentil 75:    178.0\n\n\n(171.94185115248527, 175.55814884751473)\n\n\n\n\n\n\n# Linux\n# datadir = \"../../../../Datasets/Hospital/\"\n\n# Windows\n# datadir = \"..\\\\..\\\\..\\\\..\\\\Datasets\\\\Hospital\\\\\"\ndatadir =\"data\\\\\"\nfilename = \"D_Internamento_1.csv\"\n\n\nimport pandas as pd \n\ndf_int = pd.read_csv(f\"{datadir}{filename}\", skiprows=2)\ndf_int.head()\n\n\n\n\n\n\n\n\nAno\nNORDEM\nD010001\nD010002\nD010003\nD010004\nD010005\nD010006\nD010007\nD010008\n...\nD022097\nD022098\nD022101\nD022102\nD022103\nD022104\nD022105\nD022106\nD022107\nD022108\n\n\n\n\n0\n2012\n130\n5149.0\n4471.0\n533.0\n18.0\n2079.0\n2079.0\n127.0\n72894.0\n...\n62\n21104\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n2012\n75\n1086.0\n963.0\n85.0\n0.0\n0.0\n0.0\n38.0\n14864.0\n...\n19\n6454\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n2012\n128\n231.0\n205.0\n10.0\n0.0\n0.0\n0.0\n16.0\n6186.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n2012\n111\n421.0\n284.0\n101.0\n0.0\n0.0\n0.0\n36.0\n14813.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n2012\n14\n3737.0\n3257.0\n4.0\n272.0\n3012.0\n3012.0\n204.0\n74172.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 226 columns\n\n\n\n\nprint(df_int.D010002.min())\nprint(df_int.D010002.max())\n\nprint(df_int.D010004.min())\nprint(df_int.D010004.max())\n\n0.0\n42051.0\n0.0\n311.0\n\n\n\ndef plot_hist(x, p=5):\n # Plot the distribution and mark the mean\n plt.hist(x, alpha=.5)\n plt.axvline(x.mean())\n # 95% confidence interval \n plt.axvline(np.percentile(x, p/2.), color='red', linewidth=3)\n plt.axvline(np.percentile(x, 100-p/2.), color='red', linewidth=3)\n \ndef plot_dists(a, b, nbins, a_label='com_p', b_label='sem_p', p=5):\n # Create a single sequence of bins to be shared across both\n # distribution plots for visualization consistency.\n combined = pd.concat([a, b])\n breaks = np.linspace(\n combined.min(), \n combined.max(), \n num=nbins+1)\n plt.subplot(2, 1, 1)\n plot_hist(a)\n plt.title(a_label)\n \n plt.subplot(2, 1, 2)\n plot_hist(b)\n plt.title(b_label)\n \n plt.tight_layout()\n \nplot_dists(df_int.D010002, df_int.D010004, 20, a_label='Com Parecer', b_label='Sem parecer')\nplt.show()\n\n\n\n\n\n\n2.1.6 Introdução às bases de dados e modelos\nestabelecer ligação a base de dados\n\n# importar package \nimport cx_Oracle # cx_Oracle to access Oracle database\n\n# criar conexão\n# host = c21oradev01.int.ine.pt\n# port = 1521\n# service =FORMACAO\ndsn_tns = cx_Oracle.makedsn('c21oradev01.int.ine.pt', '1521', \n                            service_name='FORMACAO') \n\npedir user e password\n\n# importar package em vez do package todo\n# trazer só o método getpass\nfrom getpass import getpass # para ler a password sem a mostrar\n\nmy_user = \"BRUNO.LIMA\"\nmy_password = \"*******\"\n\ncriar ligação\n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\nabrir cursor\n\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\nconstruir query sql\n\nmy_sql = \"\"\"\nSELECT ano, nordem, nuts2, dtcc_cod, ent_cod\nFROM BDIFRM.TD_HOSP_10\n\"\"\"\n\nexecutar\n\nc.execute(my_sql)\n\nguardar dados\n\n# guardar os dados numa estrutura Python Pandas\nimport pandas as pd\n\ndf = pd.DataFrame(c.fetchall())\n\nfechar cursor\n\nc.close()\n\nfechar conexão à base de dados\n\nconn.close()\n\nexplorar os dados\n\ndf.head()\n\n# atribuir os nomes das colunas\nnomes_col = [\"ano\", \"nordem\", \"nuts2\", \"dtcc_cod\", \"ent_cod\"]\ndf.columns = nomes_col\n\nguardar num ficheiro .csv\n\ndf.head()\n\ndf.to_csv('data/tsee_2023.csv', index=False)\n\nexercicio\nvoltar a criar ligação\n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\nvoltar a criar cursor\n\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\ncontar nº de registos\n\nmy_sql = \"\"\"\nselect count(1) from\nBDIFRM.TD_HOSP_10\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\ncontar distritos:\n\nmy_sql= \"\"\"\nselect dtcc_cod, count(1) from BDIFRM.TD_HOSP_10\ngroup by dtcc_cod\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\ndistritos com 6:\n\nmy_sql= \"\"\"\nselect * from BDIFRM.TD_HOSP_10\nwhere dtcc_cod like '%6%'\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\nNUTS2 11 ou 17 de 2012:\n\nmy_sql= \"\"\"\nselect * from BDIFRM.TD_HOSP_10\nwhere nuts2 in ('11', '17') AND ano = '2012'\n\"\"\"\n\nexcecutar\n\nc.execute(my_sql)\n\nler dados\n\ndf = pd.DataFrame(c.fetchall())\n\ndf\n\ncontar pessoal ao serviço em tabela de hospitais\n\n# query\nmy_sql= \"\"\"\nselect ano, sum(c10001), sum(c10002), sum(c10003)\nfrom BDIFRM.TD_RECHUM1_10\nwhere ano = '2012'\ngroup by ano\n\"\"\"\n# executa\nc.execute(my_sql)\n# faz fetch\ndf = pd.DataFrame(c.fetchall())\n# mostra resultado\ndf\n\nexemplos com joins e grupos\n\nmy_sql = \"\"\"\nselect t.ano, t.nuts2, t.dtcc_cod, m.cc_dsg, \nsum(r.C21041) cardio_total, sum(r.C21042) cardio_homnes, sum(r.C21043) cardio_mulheres \nfrom BDIFRM.TD_HOSP_10 h\nleft join BDIFRM.TD_NUM_10 m \non h.dtcc_cod = m.dtcc_cod\nleft join BDIFRM.REC_HUM1_10 r\non h.nordem = r.nordem\n--using (dtcc_cod)\nwhere nuts2 like '%1'\ngroup by t.ano, t.nuts2, t.dtcc_cod, m.cc_dsg\n\"\"\"\n# executa\nc.execute(my_sql)\n# faz fetch\ndf = pd.DataFrame(c.fetchall())\n# mostra resultado\ndf\n\nfechar cursor e conexção\n\nc.close()\n\nconn.close()"
  },
  {
    "objectID": "200-mod2.html#series-e-dataframes",
    "href": "200-mod2.html#series-e-dataframes",
    "title": "2  Data Science (Basics)",
    "section": "2.2 Series e Dataframes",
    "text": "2.2 Series e Dataframes\n\n2.2.1 Series\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0])\ndata\n\n0    0.25\n1    0.50\n2    0.75\n3    1.00\ndtype: float64\n\n\n\ndata.values\n\narray([0.25, 0.5 , 0.75, 1.  ])\n\n\n\ndata.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\naceder aos elementos series\n\n# acesso ao 2º elemento\n# lembrar que começa em 0\ndata[1]\n# acesso a 2 elementos no meio da Series \n# tal como no Numpy o último não está contido\ndata[1:3]\n# acesso a todos os elementos a partir do 3º\ndata[2:]\n\n2    0.75\n3    1.00\ndtype: float64\n\n\nIndexes nas series\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n\na    0.25\nb    0.50\nc    0.75\nd    1.00\ndtype: float64\n\n\n\npopulation_dict = {'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233,\n                   'Porto': 231834, 'Cascais': 214239, 'Loures': 201349,\n                   'Braga': 193324, 'Almada': 177943}\npopulation = pd.Series(population_dict)\npopulation\n\nLisboa               544325\nSintra               385989\nVila Nova de Gaia    304233\nPorto                231834\nCascais              214239\nLoures               201349\nBraga                193324\nAlmada               177943\ndtype: int64\n\n\ncom indexes explicitos o último elemento está contido no slice\n\nserie = pd.Series({2:'a', 1:'b', 3:'c'})\n\nserie[1:3]\n\n1    b\n3    c\ndtype: object\n\n\nconstruir series\n\npd.Series([2, 4, 6])\n\npd.Series(10, index=[10, 20, 30])\n\npd.Series({2:'a', 1:'b', 3:'c'})\n\npd.Series({'a':1.2, 'b':1.5, 'c':1.7})\n\na    1.2\nb    1.5\nc    1.7\ndtype: float64\n\n\n\n\n2.2.2 DataFrames\n\narea_dict = {'Lisboa': 100.1,'Sintra': 23.8, 'Vila Nova de Gaia': 56.3,\n                   'Porto': 41.4, 'Cascais': 97.1, 'Loures': 11.8,\n                   'Braga': 41, 'Almada': 14.7}\narea = pd.Series(area_dict)\narea\n\nLisboa               100.1\nSintra                23.8\nVila Nova de Gaia     56.3\nPorto                 41.4\nCascais               97.1\nLoures                11.8\nBraga                 41.0\nAlmada                14.7\ndtype: float64\n\n\n\npopulation\n\nLisboa               544325\nSintra               385989\nVila Nova de Gaia    304233\nPorto                231834\nCascais              214239\nLoures               201349\nBraga                193324\nAlmada               177943\ndtype: int64\n\n\njuntar numa dataFrame\n\ncities = pd.DataFrame({'population': population,\n                       'area': area})\ncities\n\n\n\n\n\n\n\n\npopulation\narea\n\n\n\n\nLisboa\n544325\n100.1\n\n\nSintra\n385989\n23.8\n\n\nVila Nova de Gaia\n304233\n56.3\n\n\nPorto\n231834\n41.4\n\n\nCascais\n214239\n97.1\n\n\nLoures\n201349\n11.8\n\n\nBraga\n193324\n41.0\n\n\nAlmada\n177943\n14.7\n\n\n\n\n\n\n\n\n# atributo index \ncities.index\n\n# atributo columns \ncities.columns\n\nIndex(['population', 'area'], dtype='object')\n\n\ncriar dataFrame a partir de uma serie\n\n# a partir de um único objecto Series\npd.DataFrame(population, columns=['population'])\n\n\n\n\n\n\n\n\npopulation\n\n\n\n\nLisboa\n544325\n\n\nSintra\n385989\n\n\nVila Nova de Gaia\n304233\n\n\nPorto\n231834\n\n\nCascais\n214239\n\n\nLoures\n201349\n\n\nBraga\n193324\n\n\nAlmada\n177943\n\n\n\n\n\n\n\ncom um array\n\n# a partir de um array Numpy 2D\npd.DataFrame(np.random.rand(3, 2),\n             columns=['col1', 'col2'],\n             index=['a', 'b', 'c'])\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\na\n0.181825\n0.183405\n\n\nb\n0.304242\n0.524756\n\n\nc\n0.431945\n0.291229\n\n\n\n\n\n\n\ncom um dicionario\n\n# a partir de uma lista de dicionarios\ndata = [{'simples': i, 'dobro': 2 * i, 'triplo': 3 * i}\n        for i in range(6)]\npd.DataFrame(data)\n\n\n\n\n\n\n\n\nsimples\ndobro\ntriplo\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n2\n3\n\n\n2\n2\n4\n6\n\n\n3\n3\n6\n9\n\n\n4\n4\n8\n12\n\n\n5\n5\n10\n15\n\n\n\n\n\n\n\n\n# se algumas chaves do dicionário estiverem em falta\n# vão ser preenchidas com o valor NaN\npd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1.0\n2\nNaN\n\n\n1\nNaN\n3\n4.0\n\n\n\n\n\n\n\n\n# a partir de um dicionário\npd.DataFrame({'population': population,\n              'area': area})\n\n\n\n\n\n\n\n\npopulation\narea\n\n\n\n\nLisboa\n544325\n100.1\n\n\nSintra\n385989\n23.8\n\n\nVila Nova de Gaia\n304233\n56.3\n\n\nPorto\n231834\n41.4\n\n\nCascais\n214239\n97.1\n\n\nLoures\n201349\n11.8\n\n\nBraga\n193324\n41.0\n\n\nAlmada\n177943\n14.7\n\n\n\n\n\n\n\n\n\n2.2.3 Pandas index\nindex como array imutável\n\nind = pd.Index([2, 3, 5, 7, 11])\nind\n\n# funciona e acede-se como um array\nind[1]\n\n# podem-se obter slices\nind[::2]\n\n# tem muitos atributos iguais\nprint(ind.size, ind.shape, ind.ndim, ind.dtype)\n\n5 (5,) 1 int64\n\n\npor ser imutável\n\n# mas é imutável, i.e. não pode ser alterado\n# pelos meios habituais, por isso isto não funciona\nind[1] = 0\n\nO Index também pode ser visto como um set ordenado\n\nindA = pd.Index([1, 3, 5, 7, 9])\nindB = pd.Index([2, 3, 5, 7, 11])\n\n# interseção de conjuntos\nindA.intersection(indB)\n\n# união de conjuntos\nindA.union(indB)\n\n# diferença entre conjuntos\nindA.symmetric_difference(indB)\n\nIndex([1, 2, 9, 11], dtype='int64')\n\n\n\n\n2.2.4 Seleção de dados\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n\ndata['b']\n\n# verifica se tem esta chave (key)\n'a' in data\n\n# acede a todas as chaves (keys)\ndata.keys()\n\n# acede aos pares\ndata.items()\n\n# acede a todos os valores (items)\nlist(data.items())\n\n[('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)]\n\n\n\n# expande a series acrescentando um elemento\ndata['e'] = 1.25\ndata\n\n# altera a série mapeando a key b para 0.48 em vez de 0.5\ndata['b'] = 0.48\ndata\n\n# slicing com index explicito\ndata['a':'c']\n\n# slicing com index implicito\ndata[0:2]\n\n# masking \ndata[(data &gt; 0.3) & (data &lt; 0.8)]\n\n# fancy indexing\ndata[['a', 'e']]\n\na    0.25\ne    1.25\ndtype: float64\n\n\n\n\n2.2.5 Indexers: loc (explícito) e iloc (implícito)\n\ndata = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\ndata\n\n# indice explicito \ndata.loc[1]\n\n# indice explicito \ndata.loc[1:3]\n\n# indice implicito\ndata.iloc[1]\n\n# indice implicito \ndata.iloc[1:3]\n\n3    b\n5    c\ndtype: object\n\n\n\n\n2.2.6 DataFrames\n\npop = pd.Series({'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233,\n                   'Porto': 231834, 'Cascais': 214239, 'Loures': 201349,\n                   'Braga': 193324, 'Almada': 177943})\n\narea = pd.Series({'Lisboa': 100.1,'Sintra': 23.8, 'Vila Nova de Gaia': 56.3,\n                   'Porto': 41.4, 'Cascais': 97.1, 'Loures': 11.8,\n                   'Braga': 41, 'Almada': 14.7})\n\ndata = pd.DataFrame({'area':area, 'pop':pop})\ndata\n\n\n\n\n\n\n\n\narea\npop\n\n\n\n\nLisboa\n100.1\n544325\n\n\nSintra\n23.8\n385989\n\n\nVila Nova de Gaia\n56.3\n304233\n\n\nPorto\n41.4\n231834\n\n\nCascais\n97.1\n214239\n\n\nLoures\n11.8\n201349\n\n\nBraga\n41.0\n193324\n\n\nAlmada\n14.7\n177943\n\n\n\n\n\n\n\ntransformar os dados\n\n# criar coluna\ndata['dens'] = data['pop'] / data['area'] \ndata\n\n# ver os dados como num array 2D\ndata.values\n\n# transposicao para trocar linhas com colunas\ndata.T\n\n\n\n\n\n\n\n\nLisboa\nSintra\nVila Nova de Gaia\nPorto\nCascais\nLoures\nBraga\nAlmada\n\n\n\n\narea\n100.100000\n23.80000\n56.300000\n41.400000\n97.100000\n11.800000\n41.000000\n14.700000\n\n\npop\n544325.000000\n385989.00000\n304233.000000\n231834.000000\n214239.000000\n201349.000000\n193324.000000\n177943.000000\n\n\ndens\n5437.812188\n16218.02521\n5403.783304\n5599.855072\n2206.374871\n17063.474576\n4715.219512\n12104.965986\n\n\n\n\n\n\n\naceder aos dados\n\n# aceder a linha\ndata.values[0]\n\n# aceder a coluna \ndata['area']\n\n# aceder usando os indices implicitos inteiros\n# as primeiras 3 linhas, 0, 1 e 2\n# as primeiras 2 colunas 0 e 1\ndata.iloc[:3, :2]\n\n# aceder àos mesmos dados que anteriormente\n# agora usando os indices explicitos \ndata.loc[:'Vila Nova de Gaia', :'pop']\n\ndata.loc[data.dens &gt; 10000, ['pop', 'dens']]\n\ndata.loc[data['dens'] &gt; 10000, ['pop', 'dens']]\n\ndata.iloc[0, 2] = 5000\ndata\n\n\n\n\n\n\n\n\narea\npop\ndens\n\n\n\n\nLisboa\n100.1\n544325\n5000.000000\n\n\nSintra\n23.8\n385989\n16218.025210\n\n\nVila Nova de Gaia\n56.3\n304233\n5403.783304\n\n\nPorto\n41.4\n231834\n5599.855072\n\n\nCascais\n97.1\n214239\n2206.374871\n\n\nLoures\n11.8\n201349\n17063.474576\n\n\nBraga\n41.0\n193324\n4715.219512\n\n\nAlmada\n14.7\n177943\n12104.965986\n\n\n\n\n\n\n\n\n\n2.2.7 Operações no Pandas\nnp.random.default_rng(42) cria uma instância da classe gerador do módulo numpy.random module. Este gerador baseado no algoritmo PCG64 é um gerador de números pseudo-random que na realidade é determinado pelo valor inicial da seed, neste caso 42.\n\n# criação de Series com números aleatórios \n# entre 0 e 10 (exclusive) e 4 linhas\n# a seed está fixa a 42\nrng = np.random.default_rng(42)\nser = pd.Series(rng.integers(0, 10, 4))\nser\n\n# a função unária preserva os indices\nnp.exp(ser)\n\n# criação de dataFrame com números aleatórios entre\n# 0 e 10 (exclusive) e 3 linhas e 4 colunas\ndf = pd.DataFrame(rng.integers(0, 10, (3, 4)),\n                  columns=['A', 'B', 'C', 'D'])\ndf\n\n# a função unária preserva os indices\nnp.sin(df * np.pi / 4)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1.224647e-16\n-2.449294e-16\n0.000000\n-1.000000\n\n\n1\n1.000000e+00\n0.000000e+00\n-0.707107\n0.707107\n\n\n2\n-7.071068e-01\n-7.071068e-01\n-0.707107\n-0.707107\n\n\n\n\n\n\n\nNas operações binárias como a soma e a multiplicação o Pandas alinha os indices ao passar os objectos para as ufunc. Nos items para os quais não há uma entrada é colocado o valor NaN, “Not a Number” que é como o Pandas marca valores em falta (missing data)\n\npop_u = pd.Series({'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233}, name='pop')\n\narea_u = pd.Series({'Sintra': 23.8, 'Vila Nova de Gaia': 56.3, 'Porto': 41.4}, name='area')\n\npop_u / area_u\n\narea.index.union(pop.index)\n\nIndex(['Lisboa', 'Sintra', 'Vila Nova de Gaia', 'Porto', 'Cascais', 'Loures',\n       'Braga', 'Almada'],\n      dtype='object')\n\n\n\n# se os indices forem numéricos ficam ordenados\nA = pd.Series([2, 4, 6], index=[0, 1, 2])\nB = pd.Series([1, 3, 5], index=[1, 2, 3])\nA + B\n\n# existem várias hipóteses para lidar com missing values\n# adiciona as series mas sunstitui nan por zero\nA.add(B, fill_value=0)\n\nA = pd.DataFrame(rng.integers(0, 20, (2, 2)),\n                 columns=['a', 'b'])\nA\n\nB = pd.DataFrame(rng.integers(0, 10, (3, 3)),\n                 columns=['b', 'a', 'c'])\nB\n\n# exemplo de alinhamento de indices nas DataFrames\nA + B\n\nA.values.mean()\n\n# outra forma de lidar com missing values\nA.add(B, fill_value=A.values.mean())\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n13.00\n7.00\n10.25\n\n\n1\n23.00\n18.00\n15.25\n\n\n2\n17.25\n13.25\n14.25\n\n\n\n\n\n\n\nUfuncs: Operações entre DataFrames e Series\n\nA = rng.integers(10, size=(3, 4))\nA\n\n# subtrai a todas as linhas a primeira\nA - A[0]\n\n# igual ao exemplo anterior mas usando indices explicitos\ndf = pd.DataFrame(A, columns=['Q', 'R', 'S', 'T'])\ndf - df.iloc[0]\n\n# Subtrair uma coluna em vez de uma linha\n# não esquecer de indicar axis = 0\ndf.subtract(df['R'], axis=0)\n\n# vai buscar a linha de indice 2 e as colunas (todas) com step 2\nprint(df.head())\nmeialinha = df.iloc[1, ::2]\nmeialinha\n\n# alinha os indices antes da operação\n# por isso só vai subtrair nas colunas Q e S\ndf - meialinha\n\n   Q  R  S  T\n0  4  4  2  0\n1  5  8  0  8\n2  8  2  6  1\n\n\n\n\n\n\n\n\n\nQ\nR\nS\nT\n\n\n\n\n0\n-1.0\nNaN\n2.0\nNaN\n\n\n1\n0.0\nNaN\n0.0\nNaN\n\n\n2\n3.0\nNaN\n6.0\nNaN\n\n\n\n\n\n\n\n\n\n2.2.8 Missing values\nO tipo None do Python também pode ser usado para marcar missing values, mas não suporta operações aritméticas. Assim o uso do nan é mais vantajoso.\n\nvals1 = np.array([1, None, 2, 3])\nvals1\n\narray([1, None, 2, 3], dtype=object)\n\n\nnão conseguimos sumar\n\nvals1.sum()\n\ncom nan não dá erro\n\n# criação de um array com nan a marcar missing values\nvals2 = np.array([1, np.nan, 3, 4]) \nvals2\n\nprint(1 + np.nan)\nprint(0 * np.nan)\nprint(vals2.sum(), vals2.min(), vals2.max())\n\nnan\nnan\nnan nan nan\n\n\nO Pandas converte None em nan\n\npd.Series([1, np.nan, 2, None])\n\n0    1.0\n1    NaN\n2    2.0\n3    NaN\ndtype: float64\n\n\nTodos os tipos começados por maiuscula como Int32 podem ser nullable e por isso receber NaN, None ou NA\n\npd.Series([1, np.nan, 2, None, pd.NA], dtype='Int32')\n\n0       1\n1    &lt;NA&gt;\n2       2\n3    &lt;NA&gt;\n4    &lt;NA&gt;\ndtype: Int32\n\n\n\ndata = pd.Series([1, np.nan, 'hello', None])\ndata\n\ndata.isnull()\n\ndata.isna()\n\ndata.notnull()\n\ndata[data.notnull()]\n\ndata.dropna()\n\n0        1\n2    hello\ndtype: object\n\n\n\ndf = pd.DataFrame([[1,      np.nan, 2],\n                   [2,      3,      5],\n                   [np.nan, 4,      6]])\ndf\n\n# remove registos com missing values\ndf.dropna()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n1\n2.0\n3.0\n5\n\n\n\n\n\n\n\n\ndf = pd.DataFrame([[1,      np.nan, 2],\n                   [2,      3,      5],\n                   [np.nan, 4,      6]])\ndf\n\n# remove colunas indicando axis = 1\n# também se pode indicar axis = columns' em vez de 1\ndf.dropna(axis = 1)\n\n\n\n\n\n\n\n\n2\n\n\n\n\n0\n2\n\n\n1\n5\n\n\n2\n6\n\n\n\n\n\n\n\n\ndf.loc[:,3] = np.nan\ndf\n\n# excluir aopenas quando todos são nulos\ndf.dropna(axis='columns', how='all')\n\ndf.fillna(0)\n# podemos fazer forward fill\ndf.ffill()\n# backward fill\ndf.bfill()\n# Ou amobos\ndf.bfill().ffill()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n3.0\n2\nNaN\n\n\n1\n2.0\n3.0\n5\nNaN\n\n\n2\n2.0\n4.0\n6\nNaN\n\n\n\n\n\n\n\nestas experiências não alteram mesmo a DataFrame se não usarmos o parâmetro inplace\n\ndf.dropna(axis='columns', how='all', inplace = True)\n\ndf.ffill(inplace = True)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1.0\nNaN\n2\n\n\n1\n2.0\n3.0\n5\n\n\n2\n2.0\n4.0\n6\n\n\n\n\n\n\n\n\ndf.isna()\n\n# proposta testa por partes a expressão\n# estamos a examinar só a 2ª coluna\ndf.iloc[:,1].isna().sum()\n\ndf.iloc[:,1].fillna(df.iloc[:,1].mean())\n\ndf.head()\n\ndf.describe()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       3 non-null      float64\n 1   1       2 non-null      float64\n 2   2       3 non-null      int64  \ndtypes: float64(2), int64(1)\nmemory usage: 204.0 bytes\n\n\nquando estamos a examinar uma coluna tb pode ser útil saber quantos valores unicos tem\n\n# porque é que usamos o len\n# e não o .sum()\nlen(df.iloc[:,0].unique())\n\nsum(df.iloc[:,0].unique())\n\ndf.iloc[:,0].unique() \n\nvalor, contador = np.unique(df.iloc[:,2], return_counts = True)\nprint(valor)\nprint(contador)\n\nfor valor, contador in zip(valor, contador):\n    print(f\"{valor} aparece {contador} vezes\")\n\n[2 5 6]\n[1 1 1]\n2 aparece 1 vezes\n5 aparece 1 vezes\n6 aparece 1 vezes\n\n\n\n# a função zip transforma 2 iteráveis num único iterável\n# em que cada elemento é um par\ncities = ['Elvas', 'Evora', 'Estremoz']\npop = [21750, 81127, 12750]\n \nnew_dict = {cities: pop for cities, pop in zip(cities, pop)}\nprint(new_dict)\n\n{'Elvas': 21750, 'Evora': 81127, 'Estremoz': 12750}\n\n\n\n\n2.2.9 exemplos extra formação\n\nimport pandas as pd\nimport altair as alt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\npenguins = pd.read_csv(\"https://pos.it/palmer-penguins-github-csv\")\n\n\npenguins.groupby(\"species\").size().reset_index(name = \"count\")\n\n\n\n\n\n\n\n\nspecies\ncount\n\n\n\n\n0\nAdelie\n152\n\n\n1\nChinstrap\n68\n\n\n2\nGentoo\n124\n\n\n\n\n\n\n\n\ncolors = [\"#FF8C00\", \"#A020F0\", \"#008B8B\"]\nsns.set_palette(colors, n_colors = 3)\n\n\npenguins[\"bill_ratio\"] = (\n   penguins[\"bill_length_mm\"] / penguins[\"bill_depth_mm\"] \n)\nsns.displot(penguins, \n            x = \"bill_ratio\", \n            hue = \"species\", \n            kind = \"kde\", fill = True, aspect = 2, height = 3)\nplt.show()\n\n\n\n\n\ndef collatz(num):\n    if num % 2 == 0:\n        return num // 2\n    else:\n        return 3 * num + 1\n\nnumber = 5\n\nwhile number != 1:\n    number = collatz(int(number))\n    print(number)\n\n16\n8\n4\n2\n1"
  },
  {
    "objectID": "300-mod3.html#jupyter-e-markdown",
    "href": "300-mod3.html#jupyter-e-markdown",
    "title": "3  Jupyter e Visualizações (Basics)",
    "section": "3.1 Jupyter e Markdown",
    "text": "3.1 Jupyter e Markdown\nUm JUPYTER Notebook é composto de uma combinação de células de código e documentação.\nAbrir através do prompt: Anaconda prompt -&gt; jupyter notebook\nFicheiros do tipo .ipynb\n\nShortcuts \nThe Zen of Python, by Tim Peters\n\n\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n3.1.1 Data Exploration and Visualization (Basics)\n\n3.1.1.1 Importar Dados Externos para Pandas Dataframe\nExemplo .csv Exemplo .xlsx\n\n# Obter dados a partir do Link Direto ao dados\n# Importar Bibliotecas\n# No ambiente formação devem-se utilizar os ficheiros locais (poderá dar erro \"http error 404\")\n\nimport pandas as pd\nimport requests \n# Link direto para ficheiro CSV: Despesa com Medicamentos nos Hospitais do SNS\nficheiro = r'https://dados.gov.pt/pt/datasets/r/8803343f-6e1b-47de-87ac-26432adb45f0'\n\n# Importar num Pandas Dataframe\ndf = pd.read_csv(ficheiro, sep=';')\n\n# Informação\n# print(df.head(5))\n# print(df.info())\n# print(df.describe())\n\n\n# Import Packages openxyl para poder trabalhar com XLSX\nimport pandas as pd\nimport openpyxl\nimport numpy as np\n\n# Link Direto para ficheiro XLSX\nficheiro = r'https://dados.gov.pt/s/resources/classificacao-etaria-de-teatro/20240331-230417/classificacaoetariateatro.xlsx'\n# Importar num DataFrame\n# Argumentos uteis: skiprows=[0,1], usecols='A', nrows=2, header=None, sheet_name='Sheet1'\n\n# Mostrar primeiras 3 linhas e ultimas 3 linhas (head e tail em conjunto)\n# Utilizar iloc e numpy\n# print (df.iloc[np.r_[0:3, -3:0]])  # head e tail\n# print(df.info())\n# print(df.describe())\n\n\n\n3.1.1.2 Importar tabela da BD\n\n# Obter Password e Utilizador para Ligacao SQL\nfrom getpass import getpass # para ler a password sem a mostrar\nmy_user = '\"BRUNO.LIMA\"[BDIFRM]' \nmy_password = '*******'\n\n\n# Ler Dados da BD\n# criar conexão\nimport cx_Oracle \nimport pandas as pd\nhost = 'c21oradev01.int.ine.pt'\nport = '1521'\nservice = 'FORMACAO'\ndsn_tns = cx_Oracle.makedsn(host, port, service_name=service) \n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\n# Cursor:\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\nLer Diferentes Views para Pandas DF\n\n# Dados por Municipio:\n# SQL String\nmy_sql = \"\"\"\nselect *\nfrom V_BGRI2021_DTMN_PT \n\"\"\"\n# Executar o cursor c com a string como parâmetro\nc.execute(my_sql)\n\n# Ober Nomes Colunas: (c.description devolva listagem dos atributos, nome atributo é 1º elemento - x[0])\n# No Modulo Intermédio devem discutir mais este tipo de metodo para criar Listagem\nnames = [x[0] for x in c.description]\n#print(names)\n\n# Input tabela dentro DataFrame, atribuir nomes colunas\ndf_mn_c2021 = pd.DataFrame(c.fetchall(), columns = names)\n\n# Dados por NUTS3:\n\n# SQL String\nmy_sql = \"\"\"\nselect *\nfrom V_BGRI2021_N3_PT \n\"\"\"\n\n# Executar o cursor c com a string como parâmetro\nc.execute(my_sql)\n# Criar Nomes colunas\nnames = [ x[0] for x in c.description]\ndf_n3_c2021 = pd.DataFrame(c.fetchall(), columns = names)\n\nler a partir de .xlsx\n\n# Solução de recurso caso existem problemas da BD \nimport pandas as pd\ndf_n3_c2021 = pd.read_excel(r'data\\N3_C2021.xlsx')\ndf_mn_c2021 = pd.read_excel(r'data\\DTMN_C2021.xlsx')\n# Mostrar informação inicial:\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n\n\n# Criar DF a partir de Ficheiros EXCEL\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\DTMN_C2021.xlsx\"\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\N3_C2021.xlsx\"\n# Import Packages openxyl para poder trabalhar com XLSX\nimport pandas as pd\nimport numpy as np\n\n# Link Direto para ficheiro XLSX\nficheiro_dtmn = r'data\\DTMN_C2021.xlsx'\nficheiro_n3 = r'data\\N3_C2021.xlsx'\n# Importar num DataFrame\ndf_n3_c2021 = pd.read_excel(ficheiro_n3)\nficheiro_dtmn = pd.read_excel(ficheiro_dtmn) \n# Argumentos uteis: skiprows=[0,1], usecols='A', nrows=2, header=None\n\n# Mostrar primeiras 3 linhas e ultimas 3 linhas (head e tail em conjunto)\n# Utilizar iloc e numpy\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n# print(df_mn_c2021.info())\n\n\n\n3.1.1.3 importar dados da DGS\nImportar DGS (Certificados de Obitos por Dia)\n\n# Ver referencia: https://dados.gov.pt/pt/datasets/registos-de-certificados-de-obito/\n# Atributos: Ano, Mês, Dia do Mês, Dia da Semana, Nº Certificados de Óbito Diários\n\nimport pandas as pd\n\n# Link DGS (Certificados de Obitos por Dia)  \nficheiro = r'http://dados.gov.pt/pt/datasets/r/dde8a843-d6a8-4a3f-82ff-b0e9c6743a3a'\n# Ler ficheiro do computador:\n#ficheiro = r'data\\evolucao-diaria-de-certificados-de-obito.csv'\n\n# Importar em DataFrame\ndf_obitos = pd.read_csv(ficheiro, sep=';')\n\n# Mostrar informação df\n# print(df_obitos.head(5))\n# print(df_obitos.info())\n# print(df_obitos.describe())"
  },
  {
    "objectID": "300-mod3.html#gráficos-em-matplotlib-e-seaborn",
    "href": "300-mod3.html#gráficos-em-matplotlib-e-seaborn",
    "title": "3  Jupyter e Visualizações (Basics)",
    "section": "3.2 Gráficos em MATPLOTLIB e SEABORN",
    "text": "3.2 Gráficos em MATPLOTLIB e SEABORN\nobter dados\n\n# Solução de recurso caso existem problemas da BD \nimport pandas as pd\n\ndf_n3_c2021 = pd.read_excel(r'data\\N3_C2021.xlsx')\ndf_mn_c2021 = pd.read_excel(r'data\\DTMN_C2021.xlsx')\n# Mostrar informação inicial:\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n\n\n# Criar DF a partir de Ficheiros EXCEL\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\DTMN_C2021.xlsx\"\n# \"C:\\Users\\bart.schoen\\OneDrive - ineportugal\\Documents\\2024_FormacaoPython\\Dados\\N3_C2021.xlsx\"\n# Import Packages openxyl para poder trabalhar com XLSX\nimport pandas as pd\nimport numpy as np\n\n# Link Direto para ficheiro XLSX\nficheiro_dtmn = r'data\\DTMN_C2021.xlsx'\nficheiro_n3 = r'data\\N3_C2021.xlsx'\n# Importar num DataFrame\ndf_n3_c2021 = pd.read_excel(ficheiro_n3)\nficheiro_dtmn = pd.read_excel(ficheiro_dtmn) \n# Argumentos uteis: skiprows=[0,1], usecols='A', nrows=2, header=None\n\n# Mostrar primeiras 3 linhas e ultimas 3 linhas (head e tail em conjunto)\n# Utilizar iloc e numpy\n# print(df_n3_c2021.head(5))\n# print(df_mn_c2021.head(5))\n# print(df_mn_c2021.info())\n\n\n# Importar DGS (Certificados de Obitos por Dia)\n# Ver referencia: https://dados.gov.pt/pt/datasets/registos-de-certificados-de-obito/\n# Atributos: Ano, Mês, Dia do Mês, Dia da Semana, Nº Certificados de Óbito Diários\n\nimport pandas as pd\n\n# Link DGS (Certificados de Obitos por Dia)  \n#ficheiro = r'http://dados.gov.pt/pt/datasets/r/dde8a843-d6a8-4a3f-82ff-b0e9c6743a3a'\n# Ler ficheiro do computador:\nficheiro = r'data\\evolucao-diaria-de-certificados-de-obito.csv'\n\n# Importar em DataFrame\ndf_obitos = pd.read_csv(ficheiro, sep=';')\n\n# Mostrar informação df\n# print(df_obitos.head(5))\n# print(df_obitos.info())\n# print(df_obitos.describe())\n\n\n\n3.2.1 Gráfico de Dispersão (Scatterplot) em MATPLOTLIB\n\n# Importar pyplot do matplotlib \nfrom matplotlib import pyplot as plt\n\n\n# Scatterplot que conssiste de 2 plots\n# 1º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade superior a 65 \n# 2º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade inferior a 14  \n\n\n# Marker: circle ('o'), point ('.'), diamond('d'), or square ('s')\n# s - tamamnho\n# label - rótulo a colocar\n# alpha - transparência\n# Plot 1\nplt.scatter(df_mn_c2021.N_INDIVIDUOS, df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS,\n                color = 'Red', \n                marker='.', \n                s=12,\n                label = '65 ou mais', \n                alpha = 0.5)\n# Plot 2\nplt.scatter(df_mn_c2021.N_INDIVIDUOS, df_mn_c2021.N_INDIVIDUOS_0_14,\n                color = 'Green', \n                marker='s', \n                s=12,\n                label = '15 ou menos', \n                alpha = 0.5) \n# Definir Labels x e y áxis\nplt.xlabel('Nº Individuos')\nplt.ylabel('Nº Individuos por grupo')\n\n# Adicionar GRID\nplt.grid(True)\n\n# Definir Título\nplt.title('Relação Individuos por grupo idade ao total \\n por municipio',size=12,fontweight=\"bold\")\n\n# Mostrar Legenda\nplt.legend()\nplt.show()\n\n\n\n\n\n\n3.2.2 Gráfico Relação Grupos Etários com Tamanho Municipio (valores relativos)\n\n# É possivel dividir as variáveis!\n# Permita tirar conclusões dos dados\nplt.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS,\n               color = 'Red', s=12, marker='.', label = '65 ou mais', alpha = 0.5)\n# 2º plot com \nplt.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_0_14/df_mn_c2021.N_INDIVIDUOS,\n               color = 'Green', s=12, marker='s', label = '15 ou menos', alpha = 0.5) # \n#'''\n# Labels x e y áxis\nplt.xlabel('Nº Individuos (por 1000)')\nplt.ylabel('Rácio nº Individuos')\n# Adicionar GRID\nplt.grid(True)\nplt.title('Relação Individuos por grupo idade ao total \\n por municipio',size=12,fontweight=\"bold\")\n# Mostrar Legenda\nplt.legend()\nplt.style.use('fivethirtyeight')\n#plt.title\nplt.show()\n\n\n\n\nCriar a mesma Figura utilizando os objectos fig e ax\n\n# Import matplotlib\nfrom matplotlib import pyplot as plt\nfig,ax = plt.subplots()\n\n# Scatterplot que conssite de 2 plots\n# 1º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade superior a 65 \n# 2º plot: Mostrar nº de indivíduos no áxis X, Y áxis: nº de indivíduos com idade inferior a 14  \n\nax.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS,\n            color = 'Red', \n           s=12, \n           marker='.', \n           label = '65 ou mais', \n           alpha = 0.5)\n# 2º plot com \nax.scatter(df_mn_c2021.N_INDIVIDUOS/1000, df_mn_c2021.N_INDIVIDUOS_0_14/df_mn_c2021.N_INDIVIDUOS,\n               color = 'Green', \n               s=12, \n               marker='s', \n               label = '15 ou menos', \n               alpha = 0.5) # \n\n\n\n# Definir Labels x e y áxis\nax.set_xlabel('Nº Individuos')\nax.set_ylabel('Nº Individuos por grupo')\n\n# Adicionar GRID\nax.grid(True)\n\n# Definir Título\nax.set_title('Relação Individuos por grupo idade ao total \\n por municipio',size=12,fontweight=\"bold\")\n\n\n# Mostrar Legenda\nplt.legend()\nplt.show()\n\n\n\n\n\n\n3.2.3 ScatterPlot em SEABORN\n\n# Grafico Scatterplot SEABORN - Inicial\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Criar Gráfico\nsns.scatterplot(x=df_mn_c2021.N_INDIVIDUOS, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS, \n                data = df_mn_c2021\n               )\nplt.show()\n\n\n\n\nSeaborn adicionar tamanho\n\n# Grafico Scatterplot SEABORN - Incluir cor\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.set_palette('Accent')\n\n# Criar Gráfico\n# size - variavel para tamanho de cada ponto\nsns.scatterplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                hue = \"NUTS2\",\n                hue_order = ['11','15','16','17','18','20','30'],\n                # Relação entre edificios até 1980 no total de edificios\n                size = (df_mn_c2021.N_EDIFICIOS_CONSTR_ANTES_1945 + \n                        df_mn_c2021.N_EDIFICIOS_CONSTR_1946_1980) /\n                    df_mn_c2021.N_EDIFICIOS_CLASSICOS\n               )\nplt.show()\n\n\n\n\n\n\n3.2.4 plotnine\n\nfrom plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap\n\n(\n    ggplot(df_mn_c2021, aes(x=df_mn_c2021.N_INDIVIDUOS, \n    y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS))\n    + geom_point()\n    + stat_smooth(method=\"lm\")\n)\n\n\n\n\n\n\n3.2.5 SEABORN RELPLOT\nSEABORN REPLOT Inicial\n\nsns.set_palette('Accent')\n\n# Criar Gráfico\nsns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                kind = \"scatter\",\n                col = \"NUTS2\",\n                col_order = ['11','15','16','17','18','20','30'],\n                # Numero colunas\n                col_wrap = 3,\n                size = (df_mn_c2021.N_EDIFICIOS_CONSTR_ANTES_1945 + \n                        df_mn_c2021.N_EDIFICIOS_CONSTR_1946_1980) /\n                    df_mn_c2021.N_EDIFICIOS_CLASSICOS,\n                # Tamanho minimo e máximo\n                sizes=(10, 150),\n                alpha = 0.7\n               )\nplt.show()\n\n\n\n\n\n# Outro Exemplo com Municipios com menos de 50000 e mais que 50000\nsns.set_style('whitegrid')\n\n# Criar Nova Coluna para indicar tamanho do municipio (2 classes)\ndf_mn_c2021[\"Tipo Municipio\"] = np.where(df_mn_c2021.N_INDIVIDUOS&lt;50000, 'Menos 50.000', 'Mais 50.000')\n\n# Mostra população com mais de 65 anos por tipo de municipio\nsns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS,\n                data = df_mn_c2021,\n           kind=\"scatter\",\n           col=df_mn_c2021[\"Tipo Municipio\"]           \n           )\nplt.show()\n\n\n\n\nMesmo Figura Utilizando MATPLOTLIB\n\n# Create subplots\nfig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n\n# Mudar o array de 2 dimensõe para listagem, facilitando a definição do subplot\naxes = axes.flatten()  \n\n# Define NUTS2 order\nnut2_order = ['11', '15', '16', '17', '18', '20', '30']\n\n\n# Percorrer cada NUTS2 e criar um SUBPLOT\nfor i, nut2 in enumerate(nut2_order[:7]):\n    \n    # Definir ax para subplot (0...n)\n    ax = axes[i]\n    # Seleção dos dados NUTS2\n    df_nut2 = df_mn_c2021[df_mn_c2021['NUTS2'] == nut2]\n    # Criar SUBPLOT\n    ax.scatter(\n        df_nut2['N_INDIVIDUOS'] / 1000,\n        df_nut2['N_INDIVIDUOS_65_OU_MAIS'] / df_nut2['N_INDIVIDUOS'],\n        s=(df_nut2['N_EDIFICIOS_CONSTR_ANTES_1945'] +\n           df_nut2['N_EDIFICIOS_CONSTR_1946_1980']) / df_nut2['N_EDIFICIOS_CLASSICOS'] * 1000,  \n        alpha=0.7,\n        label=nut2\n    )\n    ax.set_xlabel('N_INDIVIDUOS (thousands)')\n    ax.set_ylabel('N_INDIVIDUOS_65_OU_MAIS / N_INDIVIDUOS')\n    ax.set_title(f'NUTS2: {nut2}')\n    ax.legend()\n\nplt.tight_layout(pad=1.5)\nplt.show()\n\n\n\n\n\n\n3.2.6 SeaBorn CATPLOTs\nGráfico de contagens (CountPlot)\n\n# Criar COUNTPLOT - Contagens de registos por ano\nsns.set_palette('Accent')\n\n# Ordenar no Notebook de demonstração\ncategory_order = sorted(df_obitos['Ano'].unique(),reverse=True)\n\n# Criar Gráfico\nsns.catplot(x=\"Ano\", # x ou y\n            data = df_obitos,\n            kind = \"count\",\n            order = category_order\n            )\nplt.show()\n\n\n\n\nGráfico de barras (Bar plot)\n\n# Criar Barplot - Contagens de nº de óbitos diarios por ano\n\nsns.set_palette('Accent')\n\n# Gráfico com nº de óbitos diários por ano\nsns.catplot(x=\"Ano\",\n            y = 'Nº Certificados de Óbito Diários',\n            data = df_obitos,\n            kind = \"bar\",\n            # Error bar mostra Interval de confiança (ci) \n            errorbar = 'ci' # None\\'ci\\sd\\se\\pi\\metodo definido'\n            )\nplt.show()\n\n\n\n\nBox plot\n\n# Criar BOXPLOT - Distribuição dos ceritficados Óbito Diários\n\nsns.set_palette('tab10')\n# Mudar o aspecto do output\n#sns.set_context('notebook')\n\n# Gráfico mostra a distribuição de nº de óbitos diários\nsns.catplot(x=\"Dia da Semana\", # Dia da Semana\n            y = 'Nº Certificados de Óbito Diários',\n            data = df_obitos,\n            kind = \"box\",\n            whis = [5,95] # 2.0\n            # sym = '' # - controlar visualização outliers\n            )\nplt.show()\n\n\n\n\n\n\n3.2.7 Criar plots com ou sem RELPLOT\n\n# Mostrar Percentagem pop65 no total\n\nsns.set_palette('Accent')\n\n# Nova Coluna com Ano e Mes\n# Criar Nova Coluna para indicar tamanho do municipio\n# Converter para String e Juntar\ndf_obitos[\"ANO_MES\"] = df_obitos['Ano'].astype(str) + df_obitos['Mês'].astype(str)\n\n# Criar Gráfico nº de óbitos por ano e mes\ng1 = sns.relplot(x='ANO_MES',\n                y= 'Nº Certificados de Óbito Diários', \n                data = df_obitos,\n                kind = 'line',\n                color = 'green',\n                errorbar = 'ci' # Mudar de ci para sd\n                )\n\nplt.show()\n\n# Grafico com Lineplot com seleção dos anos 2015 até 2016\n\ndf_obitos2015_2016 = df_obitos[df_obitos['Ano'].isin([2015, 2016, 2017])]\ng2 = sns.lineplot(x='ANO_MES',\n                  y= 'Nº Certificados de Óbito Diários', \n                  data = df_obitos2015_2016,\n                  color = 'red',\n                  errorbar = 'sd' # Mudar de ci para sd\n                  )\n\n# Get the x-axis tick positions and labels\nxtick_positions = g2.get_xticks()\nxtick_labels = df_obitos2015_2016['ANO_MES'].iloc[xtick_positions].tolist()\n\n# Set x-axis tick labels to show only every 12th label\nfiltered_xtick_labels = [label if i % 12 == 0 else '' for i, label in enumerate(xtick_labels)]\ng2.set_xticks(xtick_positions)\ng2.set_xticklabels(filtered_xtick_labels, rotation=45)  # You can adjust rotation as needed\n    \ng1    \nplt.show()\n\n\n\n\n\n\n\n\n\n3.2.8 Histograma (Histogram)\n\n# histograma que mostra a distribuição do rácio de população com mais de 65 anos\n\nsns.set_palette('Accent')\n\n# Alternativa Order List\nhue_order_list = sorted(df_mn_c2021['NUTS2'].unique())\n\n# Criar Gráfico com sns.histplot\nsns.histplot(x=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n            data = df_mn_c2021,\n             bins = 30,\n             #binwidth = 0.02, - Alternativa definir tamanho de cada bin\n            hue = \"NUTS2\",\n            hue_order = ['11','15','16','17','18','20','30']\n               )\nplt.show()\n\n\n\n\n\n\n3.2.9 Customização dos gráficos em SEABORN\n\n# Exemplo diferenças na visualização com estes 3 paramentros\n\n# Categorical Color Brewer palettes: 'tab10' (default matplotlib palette), 'Dark2', 'Pastel1', 'Set2', 'Paired'\n# Seaborn has six variations of matplotlib’s palette, called: deep, muted, pastel, bright, dark, and colorblind\nsns.set_palette('colorblind')\n# 'paper',  'notebook', 'talk', 'poster'\nsns.set_context('paper')\n# Atenção - estilo continua valido para o resto do codigo\n# darkgrid, whitegrid, dark, white, ticks\nsns.set_style('darkgrid')\n# Mudar o aspecto do output\n\n\n# Gráfico mostra a distribuição de nº de óbitos diários\nsns.catplot(x=\"Dia da Semana\", # Dia da Semana\n            y = 'Nº Certificados de Óbito Diários',\n            data = df_obitos,\n            kind = \"box\",\n            whis = [5,95]# 2.0\n            #sym = '' # - controlar mostrar outliers\n            )\nplt.show()\n\n\n\n\n\n# Demonstração diferentes STYLES\n\n# darkgrid, whitegrid, dark, white, ticks\nfor estilo in ['darkgrid', 'whitegrid', 'dark', 'white', 'ticks']:\n    sns.set_style(estilo)\n    sns.set_palette('Accent')\n\n    # Criar Gráfico\n    sns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                    y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                    data = df_mn_c2021,\n                    kind = \"scatter\",\n                    hue = 'NUTS2')\n    print(estilo.upper())\n    plt.show()\n\n#plt.show()\n\nDARKGRID\n\n\n\n\n\nWHITEGRID\n\n\n\n\n\nDARK\n\n\n\n\n\nWHITE\n\n\n\n\n\nTICKS\n\n\n\n\n\ntipos de plots em Seaborn\n\n# Diferentes tipos de plots em Seaborn\n# \ng1 = sns.scatterplot(x=df_mn_c2021.N_INDIVIDUOS,y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS, data = df_mn_c2021)\ng2 = sns.relplot(x=df_mn_c2021.N_INDIVIDUOS,y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS, data = df_mn_c2021,kind='scatter')\nprint(type(g1))\nprint(type(g2))\n\n&lt;class 'matplotlib.axes._axes.Axes'&gt;\n&lt;class 'seaborn.axisgrid.FacetGrid'&gt;\n\n\n\n\n\n\n\n\nAdicionar Titulos\n\n# Definições globais:\nsns.set_style('ticks')\nsns.set_palette('colorblind')\nsns.set_context('notebook')\n\n# # Criar Gráfico\ng = sns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                kind = \"scatter\",\n                hue = 'NUTS2',\n               col='NUTS1')\n\n# Definir Titulo e font do titulo\ng.fig.suptitle('Relacao população 65+ no total de população',\n               y = 1.05,\n              fontdict={'size': 20, 'color': 'black','name': 'Arial'})\n\ng.set_titles(\"NUTS1 {col_name}\")\n\nplt.show()\n\n\n\n\nMudar Rotulos dos Eixos\n\ng = sns.relplot(x=df_mn_c2021.N_INDIVIDUOS/1000, \n                y=df_mn_c2021.N_INDIVIDUOS_65_OU_MAIS/df_mn_c2021.N_INDIVIDUOS, \n                data = df_mn_c2021,\n                kind = \"scatter\",\n                hue = 'NUTS2',\n               col='NUTS1')\n\ng.fig.suptitle('Relacao população 65+ no total de população',\n               y = 1.05,\n              fontdict={'color': 'black','name': 'Arial'})\n\ng.set_titles(\"NUTS1 {col_name}\")\n\n# Rotulos Eixos\n# .set - permite definir atributos para cada eixo de um FacetGrid\ng.set(xlabel=\"População por municipio (em 1000)\",\n     ylabel=\"Rácio nº Individuos superior a 65 anos\")\n\nplt.show()\n\n'''\n# Gravar Ficheiro:\noutputfile = r'c:\\temp\\omeuplot.png'\ng.savefig(outputfile, format='png')\n\n'''\n\n&lt;&gt;:21: SyntaxWarning: invalid escape sequence '\\o'\n&lt;&gt;:21: SyntaxWarning: invalid escape sequence '\\o'\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_12056\\235803542.py:21: SyntaxWarning: invalid escape sequence '\\o'\n\n\n\n\n\n\"\\n# Gravar Ficheiro:\\noutputfile = r'c:\\temp\\\\omeuplot.png'\\ng.savefig(outputfile, format='png')\\n\\n\""
  },
  {
    "objectID": "300-mod3.html#exercícios",
    "href": "300-mod3.html#exercícios",
    "title": "3  Jupyter e Visualizações (Basics)",
    "section": "3.3 Exercícios",
    "text": "3.3 Exercícios\nLer dados\n\nimport pandas as pd\nimport requests \n# Link direto para ficheiro CSV: Despesa com Medicamentos nos Hospitais do SNS\nficheiro = r'data\\utentes-atendidos-nos-centros-de-saude-no-ambito-da-soep.csv'\n\n# Importar num Pandas Dataframe\ndf = pd.read_csv(ficheiro, sep=';')\n\ndf.head\ndf.info\ndf.describe\n\n&lt;bound method NDFrame.describe of        Período      ARS                                    ACES  \\\n0      2019-07    Norte                            ULS Nordeste   \n1      2019-07    Norte  Trás-os-Montes - Alto Tâmega e Barroso   \n2      2019-07    Norte           Douro I - Marão e Douro Norte   \n3      2019-07    Norte           Douro I - Marão e Douro Norte   \n4      2019-07    Norte                    Douro II - Douro Sul   \n...        ...      ...                                     ...   \n27347  2023-09  Algarve                     Algarve I - Central   \n27348  2023-09  Algarve                 Algarve II - Barlavento   \n27349  2023-09  Algarve                 Algarve II - Barlavento   \n27350  2023-09  Algarve                  AlgarveIII - Sotavento   \n27351  2023-09  Algarve                  AlgarveIII - Sotavento   \n\n      Localização Geográfica       Sexo Faixa Etária  Nº Utentes  \\\n0      41.8069684,-6.7587977  Masculino       65 e +          57   \n1       41.741781,-7.4731648   Feminino        50-64          30   \n2      41.2968711,-7.7483727  Masculino       65 e +          54   \n3      41.2968711,-7.7483727   Feminino        35-49          42   \n4      41.0953745,-7.8123805  Masculino          &lt;20           9   \n...                      ...        ...          ...         ...   \n27347  37.0274264,-7.9395984   Feminino        50-64          84   \n27348  37.1387554,-8.5445093  Masculino        20-34          24   \n27349  37.1387554,-8.5445093  Masculino       65 e +          77   \n27350   37.383008,-7.7293275  Masculino          &lt;20          23   \n27351   37.383008,-7.7293275   Feminino        20-34          25   \n\n                                                      ID  \n0                   2019-7/65 e +/Masculino/ULS Nordeste  \n1      2019-7/50-64/Feminino/Trás-os-Montes - Alto Tâ...  \n2      2019-7/65 e +/Masculino/Douro I - Marão e Dour...  \n3      2019-7/35-49/Feminino/Douro I - Marão e Douro ...  \n4              2019-7/&lt;20/Masculino/Douro II - Douro Sul  \n...                                                  ...  \n27347          2023-9/50-64/Feminino/Algarve I - Central  \n27348     2023-9/20-34/Masculino/Algarve II - Barlavento  \n27349    2023-9/65 e +/Masculino/Algarve II - Barlavento  \n27350        2023-9/&lt;20/Masculino/AlgarveIII - Sotavento  \n27351       2023-9/20-34/Feminino/AlgarveIII - Sotavento  \n\n[27352 rows x 8 columns]&gt;\n\n\nBOXPLOT com a distribuíção por ARS dos diferentes nº de utentes por Sexo\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Gráfico mostra a distribuição de nº de óbitos diários\nsns.catplot(x=\"ARS\", # Dia da Semana\n            y = 'Nº Utentes',\n            data = df,\n            kind = \"box\",\n            hue = \"Sexo\",\n            whis = [5,95] # 2.0\n            # sym = '' # - controlar visualização outliers\n            )\nplt.show()\n\n\n\n\nMostrar num LINEPLOT o nº de utentos por Ano por ARS\n\n# Criar novo variável ano no Dataframe:\n# Converter \"Período\" para formato datetime\ndf['Período'] = pd.to_datetime(df['Período'], format='%Y-%m')\n\n# Extrair o ano e criar nova coluna ANO\ndf['ANO'] = df['Período'].dt.year\n\ncol_order= sorted(df['ARS'].unique(),reverse=False)\n\nsns.relplot(x=df.ANO, \n                y=df['Nº Utentes'], \n                data = df,\n                kind = \"line\",\n                col = \"ARS\",\n                color='red',\n                col_wrap = 3, \n                errorbar = 'ci') \nplt.show()\n\n\n\n\nDesafio utilizar o package CUTECHARTS para nº de observações por ARS\n\nimport pandas as pd\nimport cutecharts.charts as ctc\n\ndf2 = df['ARS'].value_counts().to_frame(name=\"count\")\n\n# pie chart \npie = ctc.Pie('ARS', # title\n              width='720px',height='720px')\n\n# set the chart options\npie.set_options(labels=list(df2.index), # ARS names as labels\n                inner_radius=0)\n\n# label to be shown on graph\npie.add_series(list(df2['count'])) \n\n# display the charts\npie.render_notebook()"
  },
  {
    "objectID": "400-mod4.html#conda",
    "href": "400-mod4.html#conda",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.1 CONDA",
    "text": "4.1 CONDA\nMotor para gestão de packages, dependencias e enviroments para qualquer linguagem.\n\nabrir o terminal: Anaconda prompt\n\nC:\\users\\bruno.lima&gt;conda -V\nC:\\users\\bruno.lima&gt;conda info\nC:\\users\\bruno.lima&gt;conda update -n base -c defaults conda (atualiza conda no env base)"
  },
  {
    "objectID": "400-mod4.html#ambientes-virtuais",
    "href": "400-mod4.html#ambientes-virtuais",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.2 Ambientes virtuais",
    "text": "4.2 Ambientes virtuais\nEspaços isolados e independentes que têm código e as dependências de um projecto.\nFerramentas de criação de ambientes virtuais:\nvenv, virtualenv, pipenv, conda, poetry\n\n4.2.1 Exercícios\ncriar ambiente virtual ‘exercicio_env’:\n&gt; conda create --name exercicio_env\ncriar ambiente dentro duma subdirectoria:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda create --prefix ./envs\nlistar os ambientes existentes:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda env list\napagar o ambiente ‘exercicio_env’:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda remove -n exercicio_env --all\ninstalar o pip:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda install pip\nexportar o ambiente activo para um ficheiro enviroment.ymal:\n(base) C:\\Users\\bruno.lima\\Documents&gt;conda env export &gt; enviroment.yml\nexportar o ambiente activo sem as dependencias para um ficheiro from_history.yml:\n(base) C:\\Users\\bruno.lima\\Documents&gt;conda env export --from-history &gt; from_history.yml\ncriar um spec-file.txt com os detalhes do ambiente activo:\n(base) C:\\Users\\bruno.lima\\Documents&gt;conda list --explicit &gt; spec-file.txt\nQuando iniciamos um novo projecto é recomendável iniciar um novo ambiente:\n(base) C:\\Users\\bruno.lima\\Documents\\projecto_novo&gt;conda create -p ./conda numpy\npara sair de um ambiente:\n(teste) C:\\Users\\bruno.lima\\Documents\\projecto_novo&gt;conda deactivate"
  },
  {
    "objectID": "400-mod4.html#instalar-packages",
    "href": "400-mod4.html#instalar-packages",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.3 Instalar packages",
    "text": "4.3 Instalar packages\n\n\n4.3.1 Exercícios\ncriar ambiente e activá-lo na nossa pasta de projecto:\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda create --prefix ./envs\n(base) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda activate ./envs\ninstalar package pip:\n(C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj\\envs) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda install pip\nprocurar versoes numpy entre 1.19 e 1.23:\n(C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj\\envs) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;conda search \"numpy &gt;=1.19, &lt;=1.23\"\ninstalar package python-twitter-v2:\n(C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj\\envs) C:\\Users\\bruno.lima\\Documents\\Python\\exercicio_proj&gt;pip install python-twitter-v2"
  },
  {
    "objectID": "400-mod4.html#ides",
    "href": "400-mod4.html#ides",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.4 IDEs",
    "text": "4.4 IDEs\n\n4.4.1 Jupyter notebook\npode usar-se a partir do ANACONDA\nCom o miniconda temos de activar um ambiente primeiro e depois instalar:\nconda install ipykernel conda install jupyter\n\n\n4.4.2 VsCode\nInstalar extensões:\n\nPython\nJupyter\n\nEm settings (Ctrl + ,), pesquisar CONDA e definir o path: C:\\ProgramData\\anaconda3\\Scripts\\connda.exe\npesquisar format e activar formatação automáticaao gravar\nactivar verificação de tipos typecheck -&gt; escolher basic\npodemos também defenir terminal integrated default como Command Prompt\nsettings -&gt; themes -&gt; color theme (Ctrl K Ctrl T)\nmais atalhos:\n\n\n\n4.4.3 Cloud\nGoogle Colab"
  },
  {
    "objectID": "400-mod4.html#classes-e-módulos",
    "href": "400-mod4.html#classes-e-módulos",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.5 Classes e Módulos",
    "text": "4.5 Classes e Módulos\n\n\n4.5.1 Módulos\n\nfrom modules import conversor\n\nconversor.metros_em_milhas(100)\n\n0.06213727366498068"
  },
  {
    "objectID": "400-mod4.html#programação-orientada-a-objectos-oop",
    "href": "400-mod4.html#programação-orientada-a-objectos-oop",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.6 Programação Orientada a Objectos (OOP)",
    "text": "4.6 Programação Orientada a Objectos (OOP)\n\n4.6.1 Classes\nclasses simples\n\nclass Gato:\n    \"\"\"Modelo de um gato\"\"\"\n\n    def mia(self):\n        print(\"Miau...\")\n\n\ntom = Gato()\n\ntom.mia()\n\ntom.__doc__\n\nMiau...\n\n\n'Modelo de um gato'\n\n\n\n4.6.1.1 Instanciação\n\nclass Pessoa:\n\n    # método constutor\n    # executado sempre que se cria um obejcto do tipo Pessoa\n    def __init__(self, nome_da_pessoa):\n        self.nome = nome_da_pessoa  # cada instância tem a seu atributo (property)\n\n\nesta_pessoa = Pessoa(\"Fernando\")\n\nesta_pessoa.nome\n\n'Fernando'\n\n\napagar instancia\n\npessoa_errada = Pessoa(\"Errada\")\nprint(pessoa_errada.nome)\nprint(pessoa_errada)\n\ndel pessoa_errada\n#print(pessoa_errada.nome)\n#print(pessoa_errada)\n\nErrada\n&lt;__main__.Pessoa object at 0x000001CE9BDAE990&gt;\n\n\natributos de classe\n\nclass Funcionario:\n\n    funcionarios_count =0\n\n    def __init__(self, numero_de_funcionario):\n        self.numero_de_funcionario = numero_de_funcionario\n        type(self).funcionarios_count += 1\n\n\nfor num_func in range(1, 10):\n   Funcionario(num_func)\n\nprint(f\"Foram criados {Funcionario.funcionarios_count} funcionários.\")\n\nForam criados 9 funcionários.\n\n\n\n\n4.6.1.2 Atributos Property (Getter e Setter)\n\nclass PessoaReal:\n    def __init__(self, nome_de_entrada):\n        self.nome = nome_de_entrada\n\n    @property\n    def nome(self):\n        return f\"Sua alteza {self._nome_privado}\"\n\n    @nome.setter\n    def nome(self, nome_de_entrada2):\n        if (len(nome_de_entrada2) == 0):\n            raise ValueError(\"O nome não pode ser vazio\")\n        self._nome_privado = nome_de_entrada2\n\n\numa_pessoa = PessoaReal(\"Artur\")\n\nprint(uma_pessoa.nome)\n\numa_pessoa.nome = \"Clara\"\nprint(uma_pessoa.nome)\n\nSua alteza Artur\nSua alteza Clara\n\n\n\n\n\n4.6.2 exercícios\nDefine uma class carro com os atributos: - marca - deposito - nivel\ne com os métodos: - andar (Km) - abastecer (litros)\n\nclass Carro:\n    def __init__(self, marca, deposito, nivel):\n        self.marca = marca\n        self.deposito = deposito\n        self.nivel = nivel\n\n    def andar(self, km):\n        self.nivel -= km * 7 / 100\n        if self.nivel &lt; 0:\n            self.nivel = 0\n        print(f'Andou {km} km')\n        \n        \n    # recebe o numero de litros a abastecer; so pode abastecer ate ao maximo do deposito\n    def abastecer(self, litros):\n        self.nivel += litros\n        if self.nivel &gt; self.deposito:\n            self.nivel = self.deposito\n        print(f'Abasteceu {self.nivel} litros')\n\n    def __str__(self):\n        return f'{self.marca} - {self.nivel:.2f}'\n\n\ncarro1 = Carro('Fiat', 50, 20)\n\ncarro2 = Carro('Renault', 60, 30)\n\nprint(carro1)\n\ncarro1.abastecer(100)\ncarro1.andar(10)\ncarro1.nivel\n\nFiat - 20.00\nAbasteceu 50 litros\nAndou 10 km\n\n\n49.3"
  },
  {
    "objectID": "400-mod4.html#listas-bidimensionais",
    "href": "400-mod4.html#listas-bidimensionais",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.7 Listas bidimensionais",
    "text": "4.7 Listas bidimensionais\ndefinição de uma lista bidimensional\n\nlista_bi = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\nlista_bi[1][2]\n\n6\n\n\ndois loops percorrendo a lista bidimensional usando iteradores\n\nfor linha in lista_bi:\n    for coluna in linha:\n        print(coluna, end = '')\n\n123456789\n\n\nAdicionar ou remover\n\nlista = [[1,2,3],[4,5,6]]\nlista.append([7,8,9])\nlista\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\ndel lista[1]\nlista\n\n[[1, 2, 3], [7, 8, 9]]\n\n\n\nlista = [[1,2,3],[4,5,6]]\nlista.extend([['a','b']])\nlista\n\n[[1, 2, 3], [4, 5, 6], ['a', 'b']]\n\n\n\nlista[:2]\n\n[[1, 2, 3], [4, 5, 6]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ndel lista[1][2]\nlista\n\n[[1, 2, 3], [4, 5], [7, 8, 9]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 66,6,6], [7, 8, 9]]\nlista[1].remove(6)\nlista\n\n[[1, 2, 3], [4, 5, 66, 6], [7, 8, 9]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nlista[1].pop(2)\nlista\n\n[[1, 2, 3], [4, 5], [7, 8, 9]]\n\n\n\nlista = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nlista.insert(1,['a','b','c'])\nlista\n\n[[1, 2, 3], ['a', 'b', 'c'], [4, 5, 6], [7, 8, 9]]\n\n\n\n4.7.1 exercicio\n\n# criar uma lista bidimensional com colunas A a J e linhas 1 a 10\nlista_bi = [[f'{chr(65 + col)}{linha}' for col in range(10)] for linha in range(1, 11)]\n\nlista_bi\n\n[['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1', 'I1', 'J1'],\n ['A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2', 'I2', 'J2'],\n ['A3', 'B3', 'C3', 'D3', 'E3', 'F3', 'G3', 'H3', 'I3', 'J3'],\n ['A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4', 'I4', 'J4'],\n ['A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5', 'H5', 'I5', 'J5'],\n ['A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6', 'I6', 'J6'],\n ['A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7', 'I7', 'J7'],\n ['A8', 'B8', 'C8', 'D8', 'E8', 'F8', 'G8', 'H8', 'I8', 'J8'],\n ['A9', 'B9', 'C9', 'D9', 'E9', 'F9', 'G9', 'H9', 'I9', 'J9'],\n ['A10', 'B10', 'C10', 'D10', 'E10', 'F10', 'G10', 'H10', 'I10', 'J10']]\n\n\n\n# solução alternativa\nlista_bi2 = [[0 for coluna in range(10)] for linha in range(10)]\n\nfor linha in range(10):\n    for coluna in range(10):\n        lista_bi2[linha][coluna] = f'{chr(65 + coluna)}{linha + 1}'\n        \nlista_bi2\n\n[['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1', 'I1', 'J1'],\n ['A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2', 'I2', 'J2'],\n ['A3', 'B3', 'C3', 'D3', 'E3', 'F3', 'G3', 'H3', 'I3', 'J3'],\n ['A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4', 'I4', 'J4'],\n ['A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5', 'H5', 'I5', 'J5'],\n ['A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6', 'I6', 'J6'],\n ['A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7', 'I7', 'J7'],\n ['A8', 'B8', 'C8', 'D8', 'E8', 'F8', 'G8', 'H8', 'I8', 'J8'],\n ['A9', 'B9', 'C9', 'D9', 'E9', 'F9', 'G9', 'H9', 'I9', 'J9'],\n ['A10', 'B10', 'C10', 'D10', 'E10', 'F10', 'G10', 'H10', 'I10', 'J10']]"
  },
  {
    "objectID": "400-mod4.html#list-comprehension",
    "href": "400-mod4.html#list-comprehension",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.8 List Comprehension",
    "text": "4.8 List Comprehension\n\n\nnum = [1, 2, 3, 4, 5]\n\ndobro = [i*2 for i in num]\n\nprint(\"\"\"- a expressão a aplicar é: i*2\n- a sequência de entrada é a lista num:\"\"\",num,\"\"\"\n- a variável que representa o elemento é: i\n- o resultado é:\"\"\",\n      dobro, sep='\\n')\n\n- a expressão a aplicar é: i*2\n- a sequência de entrada é a lista num:\n[1, 2, 3, 4, 5]\n\n- a variável que representa o elemento é: i\n- o resultado é:\n[2, 4, 6, 8, 10]\n\n\n\nlista  =  [x ** 2  for x in range (1, 11)   if  x % 2 == 1]\n\nprint(\"\"\"- a expressão a aplicar é: x ** 2\n- a sequência de entrada é: range (1, 11)\n- a variável é: x\n- a condição predicado é: if x % 2 == 1\"\"\",\n      lista, sep='\\n\\n')\n\n- a expressão a aplicar é: x ** 2\n- a sequência de entrada é: range (1, 11)\n- a variável é: x\n- a condição predicado é: if x % 2 == 1\n\n[1, 9, 25, 49, 81]\n\n\n\n# outra forma de fazer uma lista bidimensional\nlista_bi1 = []\nfor valor_linha in range(10, 40, 10):\n  linha = []\n  for valor_col in range(1,5):\n    linha.append(valor_linha + valor_col)\n  lista_bi1.append(linha)\nlista_bi1\n\n[[11, 12, 13, 14], [21, 22, 23, 24], [31, 32, 33, 34]]\n\n\n\nlista_bi2 = [[linha+col for col in range(1,5)] for linha in range(10, 40, 10)]\nlista_bi2\n\n[[11, 12, 13, 14], [21, 22, 23, 24], [31, 32, 33, 34]]\n\n\n\nlista_bi3 = [[ (linha+1)*10 + (col+1) for col in range(4)] for linha in range(3)]\nlista_bi3\n\n[[11, 12, 13, 14], [21, 22, 23, 24], [31, 32, 33, 34]]\n\n\n\n# matriz toda a zeros com n linhas e m colunas\nn = 3\nm = 4\nmatriz = [[0 for _ in range(m)] for _ in range(n)]\nmatriz\n\n[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n\n\n\n# matriz toda a zeros com n linhas e m colunas\nn = 3\nm = 4\nmatriz = [[0] * m for _ in range(n)]\nmatriz\n\n[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n\n\n\n3*[4*[0]]\n\n[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n\n\n\n# @title exemplo\nnumeros = [\"um\", \"dois\", \"três\", \"quatro\"]\n\niniciais = [numero[0] for numero in numeros]\n\nprint( numeros, iniciais, sep='\\n')\n\n['um', 'dois', 'três', 'quatro']\n['u', 'd', 't', 'q']\n\n\n\n# @title exemplo\n\nnaturais_5 = [1, 2, 3, 4, 5]\n\nimpares_5 = [i for i in naturais_5 if i%2!=0]\n\nprint(naturais_5, impares_5, sep = '\\n')\n\n[1, 2, 3, 4, 5]\n[1, 3, 5]\n\n\n\n# @title exemplo\nprint ([x.lower() for x in [\"A\",\"B\",\"C\"]], sep=\"\\n\")\n\n['a', 'b', 'c']"
  },
  {
    "objectID": "400-mod4.html#dictionary-comprehension",
    "href": "400-mod4.html#dictionary-comprehension",
    "title": "4  Programming Tecnhiques (Intermediate)",
    "section": "4.9 Dictionary Comprehension",
    "text": "4.9 Dictionary Comprehension\n\n\nextenso = [\"Um\", \"Dois\", \"Três\"]\nromanos = ['I', 'II', 'III']\n\nnumeros = {numero_por_extenso:numero_romano for (numero_por_extenso, numero_romano) in zip(extenso, romanos)}\nprint(numeros)\n\n{'Um': 'I', 'Dois': 'II', 'Três': 'III'}\n\n\n\n# trocando k,v por v,k\nnumeros = {numero_romano:numero_por_extenso for (numero_por_extenso, numero_romano) in zip(extenso, romanos)}\n\nprint(numeros)\n\n{'I': 'Um', 'II': 'Dois', 'III': 'Três'}\n\n\n\nquadrados = {x: x**2 for x in range(1, 6)}\nprint(quadrados)\n\n{1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n\nusando funções na expressão\n\nfrom math import sqrt\n\ndef is_prime(numero):\n    if numero == 1:\n        return \"Não primo\"\n    for i in range(2, int(sqrt(numero))+1):\n        if numero % i == 0:\n            return \"Não primo\"\n    return \"Primo\"\n\nnumeros_candidatos = [21, 43, 53, 87, 99, 101]\n\nmarca_primos = {candidato:is_prime(candidato) for candidato in numeros_candidatos}\nprint(marca_primos)\n\n{21: 'Não primo', 43: 'Primo', 53: 'Primo', 87: 'Não primo', 99: 'Não primo', 101: 'Primo'}\n\n\n\n4.9.1 exercício\n\n# considerando o dicionario\ndicionario = {'hidrogenio': 'H', 'oxigenio': 'O', 'sodio': 's'}\n\n# usa uma compreehnsion list para obter o dicionario invertido\ndicionario_invertido = {v:k for k,v in dicionario.items()}\n\nprint(dicionario_invertido)\n\n{'H': 'hidrogenio', 'O': 'oxigenio', 's': 'sodio'}"
  },
  {
    "objectID": "500-mod5.html#exploração-com-pandas",
    "href": "500-mod5.html#exploração-com-pandas",
    "title": "5  Data Science (Intermediate)",
    "section": "5.1 Exploração com Pandas",
    "text": "5.1 Exploração com Pandas\n\n# usamos por convenção np para Numpy\n# usamos por convenção pd para Pandas\nimport numpy as np\nimport pandas as pd\n\n\n5.1.1 Series\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0])\ndata\n\n0    0.25\n1    0.50\n2    0.75\n3    1.00\ndtype: float64\n\n\n\ndata.values\n\ndata.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\n\ndel data[2]\n\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n\na    0.25\nb    0.50\nc    0.75\nd    1.00\ndtype: float64\n\n\n\npopulation_dict = {'Lisboa': 544325,'Sintra': 385989, 'Vila Nova de Gaia': 304233,\n                   'Porto': 231834, 'Cascais': 214239, 'Loures': 201349,\n                   'Braga': 193324, 'Almada': 177943}\n                   \npopulation = pd.Series(population_dict)\npopulation\n\nLisboa               544325\nSintra               385989\nVila Nova de Gaia    304233\nPorto                231834\nCascais              214239\nLoures               201349\nBraga                193324\nAlmada               177943\ndtype: int64\n\n\n\npopulation['Braga']\n\npopulation['Braga'] = 201000\n\npopulation['Braga']\n\n201000\n\n\n\n\n5.1.2 DataFrames\n\narea_dict = {'Lisboa': 100.1,'Sintra': 23.8, 'Vila Nova de Gaia': 56.3,\n                   'Porto': 41.4, 'Cascais': 97.1, 'Loures': 11.8,\n                   'Braga': 41, 'Almada': 14.7}\narea = pd.Series(area_dict)\narea\n\nLisboa               100.1\nSintra                23.8\nVila Nova de Gaia     56.3\nPorto                 41.4\nCascais               97.1\nLoures                11.8\nBraga                 41.0\nAlmada                14.7\ndtype: float64\n\n\ncriar uma fataframe a partir de series:\n\ncities = pd.DataFrame({'population': population,\n                       'area': area})\ncities\n\n\n\n\n\n\n\n\npopulation\narea\n\n\n\n\nLisboa\n544325\n100.1\n\n\nSintra\n385989\n23.8\n\n\nVila Nova de Gaia\n304233\n56.3\n\n\nPorto\n231834\n41.4\n\n\nCascais\n214239\n97.1\n\n\nLoures\n201349\n11.8\n\n\nBraga\n201000\n41.0\n\n\nAlmada\n177943\n14.7\n\n\n\n\n\n\n\n\n# reset index com o nome cidade\ncities.reset_index()\n\ncities\n\n\n\n5.1.3 Index\n\nindA = pd.Index([1, 3, 5, 7, 9])\nindA\n\nindB = pd.Index([2, 3, 5, 7, 11])\nindA.intersection(indB)\n\nIndex([3, 5, 7], dtype='int64')\n\n\na intersecção é muito útil para descobrirmos registos com a mesma identificação em vários conjuntos\n\naerod_dict = {'Lisboa': 3, 'Porto': 4, 'Cascais': 1, 'Braga': 7, 'Viseu': 2}\naerod = pd.Series(aerod_dict)\naerod.index\n\naerod.index.intersection(cities.index)\n\nIndex(['Lisboa', 'Porto', 'Cascais', 'Braga'], dtype='object')\n\n\n\n\n5.1.4 Reorganizar as DataFrames\nPara juntar duas dataframes podemos usar os métodos: + Concatenate (pd.concat()) + Append (df.append): As of pandas 2.0, append (previously deprecated) was removed + Merge (pd.merge())\nPor conveniência vamos definir uma função para criar dataframes\n\ndef make_df(cols, ind):\n    \"\"\"Quickly make a DataFrame\"\"\"\n    data = {c: [str(c) + str(i) for i in ind]\n            for c in cols}\n    return pd.DataFrame(data, ind)\n  \n# exemplo de DataFrame\nmake_df('ABC', range(3))\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\nA0\nB0\nC0\n\n\n1\nA1\nB1\nC1\n\n\n2\nA2\nB2\nC2\n\n\n\n\n\n\n\n\n5.1.4.1 Método concatenate\n\ndf1 = make_df('AB', [1, 2])\ndf2 = make_df('AB', np.arange(3,5))\n\ndf1\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n3\nA3\nB3\n\n\n4\nA4\nB4\n\n\n\n\n\n\n\n\npd.concat([df1, df2])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\nA1\nB1\n\n\n2\nA2\nB2\n\n\n3\nA3\nB3\n\n\n4\nA4\nB4\n\n\n\n\n\n\n\n\ndf3 = make_df('AB', range(2))\ndf4 = make_df('CD', range(2))\n\ndf3 \ndf4\n\npd.concat([df3, df4], axis='columns')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nA0\nB0\nC0\nD0\n\n\n1\nA1\nB1\nC1\nD1\n\n\n\n\n\n\n\nDuplicação de indexes\n(Uma diferença importante entre np.concatenate e pd.concat é que a concatenação do Pandas preserva os índices, mesmo que o resultado tenha índices duplicados.)\n\nx = make_df('AB', [0, 1])\ny = make_df('AB', [2, 3])\ny.index = x.index  # fazer o match dos indices\n\nx\ny\n\npd.concat([x, y])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n0\nA2\nB2\n\n\n1\nA3\nB3\n\n\n\n\n\n\n\n\n# Tratar índices repetidos como um erro, fazendo apenas a verificação\ntry:\n    pd.concat([x, y], verify_integrity=True)\nexcept ValueError as e:\n    print(\"ValueError:\", e)\n\nValueError: Indexes have overlapping values: Index([0, 1], dtype='int64')\n\n\n\n# Ignorando o index das dataframes de origem, e refazendo na nova dataframe o index\nx\ny\n\npd.concat([x, y], ignore_index=True)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n2\nA2\nB2\n\n\n3\nA3\nB3\n\n\n\n\n\n\n\n\n# Adicionando chaves MultiIndex para especificar um rótulo para as fontes de dados\nx\ny\n\npd.concat([x, y], keys=['x', 'y'])\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\nx\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\ny\n0\nA2\nB2\n\n\n1\nA3\nB3\n\n\n\n\n\n\n\n\nteste = pd.concat([x, y], keys=['x', 'y'])\n\nteste.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nA\nB\n\n\n\n\n0\nx\n0\nA0\nB0\n\n\n1\nx\n1\nA1\nB1\n\n\n2\ny\n0\nA2\nB2\n\n\n3\ny\n1\nA3\nB3\n\n\n\n\n\n\n\nConcatenação com joins\n\ndf5 = make_df('ABC', [1, 2])\ndf6 = make_df('BCD', [3, 4])\n\ndf5\ndf6\npd.concat([df5, df6])\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n1\nA1\nB1\nC1\nNaN\n\n\n2\nA2\nB2\nC2\nNaN\n\n\n3\nNaN\nB3\nC3\nD3\n\n\n4\nNaN\nB4\nC4\nD4\n\n\n\n\n\n\n\npara juntar fazendo a união das colunas de entrada usamos join=‘outer’, que é o valor por omissão\npara juntar fazendo a interseção das colunas de entrada usamos join=‘inner’\n\npd.concat([df5, df6], join='inner')\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n1\nB1\nC1\n\n\n2\nB2\nC2\n\n\n3\nB3\nC3\n\n\n4\nB4\nC4\n\n\n\n\n\n\n\nse quisermos preservar todas as colunas de uma das dataframes devemos fazer reindex das colunas a preservar na outra dataframe axis=1\n\npd.concat([df6, df5.reindex(df6.columns, axis=1)])\n\n\n\n\n\n\n\n\nB\nC\nD\n\n\n\n\n3\nB3\nC3\nD3\n\n\n4\nB4\nC4\nD4\n\n\n1\nB1\nC1\nNaN\n\n\n2\nB2\nC2\nNaN\n\n\n\n\n\n\n\n\n\n5.1.4.2 Método merge\n\ndf1 = pd.DataFrame({'cidade': [ 'Braga','Lisboa','Sintra', 'Vila Nova de Gaia','Porto', 'Cascais', \n                               'Loures', 'Almada'],\n                    'populacao': [ 193324, 544325, 385989, 304233, 231834, 214239, 201349, 177943]})\ndf2 = pd.DataFrame({'cidade': ['Lisboa','Sintra', 'Vila Nova de Gaia',\n                   'Porto', 'Cascais', 'Loures', 'Braga', 'Almada'],\n                    'area': [ 100.1, 23.8, 56.3, 41.4, 97.1, 11.8, 41,  14.7],\n                   'nuts3': [ '170', '170', '11A', '11A', '170', '170', '112', '170']})\ndf1\ndf2\n\n\n\n\n\n\n\n\ncidade\narea\nnuts3\n\n\n\n\n0\nLisboa\n100.1\n170\n\n\n1\nSintra\n23.8\n170\n\n\n2\nVila Nova de Gaia\n56.3\n11A\n\n\n3\nPorto\n41.4\n11A\n\n\n4\nCascais\n97.1\n170\n\n\n5\nLoures\n11.8\n170\n\n\n6\nBraga\n41.0\n112\n\n\n7\nAlmada\n14.7\n170\n\n\n\n\n\n\n\none-to-one join\n\ndf3 = pd.merge(df1, df2)\n\ndf3\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\n\n\n\n\n0\nBraga\n193324\n41.0\n112\n\n\n1\nLisboa\n544325\n100.1\n170\n\n\n2\nSintra\n385989\n23.8\n170\n\n\n3\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\n4\nPorto\n231834\n41.4\n11A\n\n\n5\nCascais\n214239\n97.1\n170\n\n\n6\nLoures\n201349\n11.8\n170\n\n\n7\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\none-to-many join\n\ndf4 = pd.DataFrame({'nuts3': [ '112', '11A', '170'],\n                   'nuts3_dsg': ['Cávado', 'Área Met. Porto', 'Área Met. Lisboa']})\n                   \ndf5 = pd.merge(df3, df4)\n\ndf4\ndf5\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\n\n\n0\nBraga\n193324\n41.0\n112\nCávado\n\n\n1\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\n\n\n2\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\n\n\n3\nVila Nova de Gaia\n304233\n56.3\n11A\nÁrea Met. Porto\n\n\n4\nPorto\n231834\n41.4\n11A\nÁrea Met. Porto\n\n\n5\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\n\n\n6\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\n\n\n7\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\nmany-to-many join\n\ndf6 = pd.DataFrame({'nuts3': [ '112', '112','11A', '170'],\n                   'class': ['Urbano', 'Rural','Urbano', 'Urbano']})\ndf6\npd.merge(df5, df6)\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\nnuts3_dsg\nclass\n\n\n\n\n0\nBraga\n193324\n41.0\n112\nCávado\nUrbano\n\n\n1\nBraga\n193324\n41.0\n112\nCávado\nRural\n\n\n2\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\nUrbano\n\n\n3\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\nUrbano\n\n\n4\nVila Nova de Gaia\n304233\n56.3\n11A\nÁrea Met. Porto\nUrbano\n\n\n5\nPorto\n231834\n41.4\n11A\nÁrea Met. Porto\nUrbano\n\n\n6\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\nUrbano\n\n\n7\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\nUrbano\n\n\n8\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\nUrbano\n\n\n\n\n\n\n\nmerge key\npodemos indicar a chave para ligar, o primeiro exemplo é equivalente a display(‘df1’, ‘df2’, “pd.merge(df1, df2, on=‘cidade’)”)\nmas nem sempre as colunas por onde queremos fazer o join têm o mesmo nome, nesse caso podemos usar o left_on e o right_on\n\ndf1a = pd.DataFrame({'cidade': ['Lisboa','Sintra', 'Vila Nova de Gaia','Porto', 'Cascais', \n                               'Loures', 'Braga', 'Almada'],\n                    'populacao': [544325, 385989, 304233, 231834, 214239, 201349, 193324, 177943]})\ndf2a = pd.DataFrame({'cidade+100khab': ['Lisboa','Sintra', 'Vila Nova de Gaia',\n                   'Porto', 'Cascais', 'Loures', 'Braga', 'Almada'],\n                    'area': [ 100.1, 23.8, 56.3, 41.4, 97.1, 11.8, 41,  14.7],\n                   'nuts3': [ '170', '170', '11A', '11A', '170', '170', '112', '170']})\n\ndf1a\nprint()\ndf2a\nprint()\npd.merge(df1a, df2a, left_on=\"cidade\", right_on=\"cidade+100khab\")\n\n\n\n\n\n\n\n\n\n\n\n\ncidade\npopulacao\ncidade+100khab\narea\nnuts3\n\n\n\n\n0\nLisboa\n544325\nLisboa\n100.1\n170\n\n\n1\nSintra\n385989\nSintra\n23.8\n170\n\n\n2\nVila Nova de Gaia\n304233\nVila Nova de Gaia\n56.3\n11A\n\n\n3\nPorto\n231834\nPorto\n41.4\n11A\n\n\n4\nCascais\n214239\nCascais\n97.1\n170\n\n\n5\nLoures\n201349\nLoures\n11.8\n170\n\n\n6\nBraga\n193324\nBraga\n41.0\n112\n\n\n7\nAlmada\n177943\nAlmada\n14.7\n170\n\n\n\n\n\n\n\npodemos fazer drop da coluna repetida\n\npd.merge(df1a, df2a, left_on=\"cidade\", right_on=\"cidade+100khab\").drop('cidade+100khab', axis=1)\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\nnuts3\n\n\n\n\n0\nLisboa\n544325\n100.1\n170\n\n\n1\nSintra\n385989\n23.8\n170\n\n\n2\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\n3\nPorto\n231834\n41.4\n11A\n\n\n4\nCascais\n214239\n97.1\n170\n\n\n5\nLoures\n201349\n11.8\n170\n\n\n6\nBraga\n193324\n41.0\n112\n\n\n7\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\nLeft_index e Right_index Keywords\n\ndf1i = df1.set_index('cidade')\ndf2i = df2.set_index('cidade')\n\ndf1i\ndf2i\n\n\n\n\n\n\n\n\narea\nnuts3\n\n\ncidade\n\n\n\n\n\n\nLisboa\n100.1\n170\n\n\nSintra\n23.8\n170\n\n\nVila Nova de Gaia\n56.3\n11A\n\n\nPorto\n41.4\n11A\n\n\nCascais\n97.1\n170\n\n\nLoures\n11.8\n170\n\n\nBraga\n41.0\n112\n\n\nAlmada\n14.7\n170\n\n\n\n\n\n\n\n\npd.merge(df1i, df2i, left_index=True, right_index=True)\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\n\n\ncidade\n\n\n\n\n\n\n\nBraga\n193324\n41.0\n112\n\n\nLisboa\n544325\n100.1\n170\n\n\nSintra\n385989\n23.8\n170\n\n\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\nPorto\n231834\n41.4\n11A\n\n\nCascais\n214239\n97.1\n170\n\n\nLoures\n201349\n11.8\n170\n\n\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\nquando temos os indices dos dois lados podemos usar apenas o join\n\n# método antigo\ndf1i.join(df2i)\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\n\n\ncidade\n\n\n\n\n\n\n\nBraga\n193324\n41.0\n112\n\n\nLisboa\n544325\n100.1\n170\n\n\nSintra\n385989\n23.8\n170\n\n\nVila Nova de Gaia\n304233\n56.3\n11A\n\n\nPorto\n231834\n41.4\n11A\n\n\nCascais\n214239\n97.1\n170\n\n\nLoures\n201349\n11.8\n170\n\n\nAlmada\n177943\n14.7\n170\n\n\n\n\n\n\n\nas keywords left_index e right_index são mais úteis quando pretendemos misturar index e colunas\n\npd.merge(df1i, df2a, left_index=True, right_on='cidade+100khab')\n\n\n\n\n\n\n\n\npopulacao\ncidade+100khab\narea\nnuts3\n\n\n\n\n6\n193324\nBraga\n41.0\n112\n\n\n0\n544325\nLisboa\n100.1\n170\n\n\n1\n385989\nSintra\n23.8\n170\n\n\n2\n304233\nVila Nova de Gaia\n56.3\n11A\n\n\n3\n231834\nPorto\n41.4\n11A\n\n\n4\n214239\nCascais\n97.1\n170\n\n\n5\n201349\nLoures\n11.8\n170\n\n\n7\n177943\nAlmada\n14.7\n170\n\n\n\n\n\n\n\n\n# para fazer reset de um index\ndf1i\ndf1i.reset_index()\n\n\n\n\n\n\n\n\ncidade\npopulacao\n\n\n\n\n0\nBraga\n193324\n\n\n1\nLisboa\n544325\n\n\n2\nSintra\n385989\n\n\n3\nVila Nova de Gaia\n304233\n\n\n4\nPorto\n231834\n\n\n5\nCascais\n214239\n\n\n6\nLoures\n201349\n\n\n7\nAlmada\n177943\n\n\n\n\n\n\n\n\n# posso continuar a fazer reset do index\n# isso irá acrescentando colunas\ndf1i.reset_index(inplace = True) # o 'inplace = True' altera o dataframe original \ndf1i\ndf1i.reset_index()\n\n\n\n\n\n\n\n\nindex\ncidade\npopulacao\n\n\n\n\n0\n0\nBraga\n193324\n\n\n1\n1\nLisboa\n544325\n\n\n2\n2\nSintra\n385989\n\n\n3\n3\nVila Nova de Gaia\n304233\n\n\n4\n4\nPorto\n231834\n\n\n5\n5\nCascais\n214239\n\n\n6\n6\nLoures\n201349\n\n\n7\n7\nAlmada\n177943\n\n\n\n\n\n\n\nInner e Outer Joins\n\ndf11 = pd.DataFrame({'cidade': ['Lisboa','Sintra'],\n                    'populacao': [544325, 385989]})\ndf12 = pd.DataFrame({'cidade': ['Lisboa','Porto', ],\n                    'area': [ 100.1, 97.1]})\ndf11\nprint()\ndf12\nprint()\npd.merge(df11, df12)\n\n\n\n\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\n\n\n\n\n0\nLisboa\n544325\n100.1\n\n\n\n\n\n\n\npor omissão é realizado o inner join mas podemos especificar o tipo de join\n\npd.merge(df11, df12, how='outer')\nprint() # paenas para acrescentr uma linha vazia\npd.merge(df11, df12, how='left')\n\n\n\n\n\n\n\n\n\n\n\ncidade\npopulacao\narea\n\n\n\n\n0\nLisboa\n544325\n100.1\n\n\n1\nSintra\n385989\nNaN\n\n\n\n\n\n\n\nSobreposição de Nomes de Colunas\n\ndf13 = pd.DataFrame({'cidade': ['Lisboa','Porto'],\n                    'area': [ 100, 97.5]})\ndf12\ndf13\nprint()\npd.merge(df12, df13, on='cidade')\n\n\n\n\n\n\n\n\n\n\n\ncidade\narea_x\narea_y\n\n\n\n\n0\nLisboa\n100.1\n100.0\n\n\n1\nPorto\n97.1\n97.5\n\n\n\n\n\n\n\npodemos indicar os sufixos que prentedemos para conhecermos a origem\n\npd.merge(df12, df13, on=\"cidade\", suffixes=[\"_12\", \"_13\"])\n\n\n\n\n\n\n\n\ncidade\narea_12\narea_13\n\n\n\n\n0\nLisboa\n100.1\n100.0\n\n\n1\nPorto\n97.1\n97.5\n\n\n\n\n\n\n\n\n\n5.1.4.3 Agregar e Agrupar\n\ndf5\n\ndf5.describe()\n\n\n\n\n\n\n\n\npopulacao\narea\n\n\n\n\ncount\n8.000000\n8.000000\n\n\nmean\n281654.500000\n48.275000\n\n\nstd\n126731.169791\n34.415601\n\n\nmin\n177943.000000\n11.800000\n\n\n25%\n199342.750000\n21.525000\n\n\n50%\n223036.500000\n41.200000\n\n\n75%\n324672.000000\n66.500000\n\n\nmax\n544325.000000\n100.100000\n\n\n\n\n\n\n\n\nprint(df5['populacao'].sum(), df5['populacao'].mean())\n\n2253236 281654.5\n\n\nvalores agrupados\n\ndf5.groupby('nuts3').populacao.mean()\n\n# ou de forma equivalente\ndf5.groupby('nuts3')['populacao'].mean()\n\nnuts3\n112    193324.0\n11A    268033.5\n170    304769.0\nName: populacao, dtype: float64\n\n\no object groupby suporta iteração sobre os grupos, isto pode ser útil para inspeccionarmos manualmente os grupos\n\n# inspecao da estrutura\nfor (group_name, group_data) in df5.groupby('nuts3'):\n    print(\"{0} shape={1}\".format(group_name, group_data.shape))\n\n112 shape=(1, 5)\n11A shape=(2, 5)\n170 shape=(5, 5)\n\n\n\n# summary statistics por grupo\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Nuts3:\", group_name)\n    print(\"Mean value:\", group_data['populacao'].mean())\n    print(\"Median value:\", group_data['populacao'].median())\n    print(\"Standard deviation:\", group_data['populacao'].std())\n    print()\n\nNuts3: 112\nMean value: 193324.0\nMedian value: 193324.0\nStandard deviation: nan\n\nNuts3: 11A\nMean value: 268033.5\nMedian value: 268033.5\nStandard deviation: 51193.82385112486\n\nNuts3: 170\nMean value: 304769.0\nMedian value: 214239.0\nStandard deviation: 157289.52373886824\n\n\n\n\n# inspeccionar valores unicos\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(\"Unique values:\", group_data['cidade'].count())\n    print(\"Unique values:\", group_data['cidade'].nunique())\n    print(\"Unique values:\", group_data['cidade'].unique())\n    print()\n\nCategory: 112\nUnique values: 1\nUnique values: 1\nUnique values: ['Braga']\n\nCategory: 11A\nUnique values: 2\nUnique values: 2\nUnique values: ['Vila Nova de Gaia' 'Porto']\n\nCategory: 170\nUnique values: 5\nUnique values: 5\nUnique values: ['Lisboa' 'Sintra' 'Cascais' 'Loures' 'Almada']\n\n\n\n\n# inspeccionar os tops\nN = 1\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(group_data.nlargest(N, 'area'))\n    print()\n\nCategory: 112\n  cidade  populacao  area nuts3 nuts3_dsg\n0  Braga     193324  41.0   112    Cávado\n\nCategory: 11A\n              cidade  populacao  area nuts3        nuts3_dsg\n3  Vila Nova de Gaia     304233  56.3   11A  Área Met. Porto\n\nCategory: 170\n   cidade  populacao   area nuts3         nuts3_dsg\n1  Lisboa     544325  100.1   170  Área Met. Lisboa\n\n\n\n\n# inspecao visual\nfor group_name, group_data in df5.groupby('nuts3'):\n    print(\"Category:\", group_name)\n    print(group_data.head())\n    print()\n\nCategory: 112\n  cidade  populacao  area nuts3 nuts3_dsg\n0  Braga     193324  41.0   112    Cávado\n\nCategory: 11A\n              cidade  populacao  area nuts3        nuts3_dsg\n3  Vila Nova de Gaia     304233  56.3   11A  Área Met. Porto\n4              Porto     231834  41.4   11A  Área Met. Porto\n\nCategory: 170\n    cidade  populacao   area nuts3         nuts3_dsg\n1   Lisboa     544325  100.1   170  Área Met. Lisboa\n2   Sintra     385989   23.8   170  Área Met. Lisboa\n5  Cascais     214239   97.1   170  Área Met. Lisboa\n6   Loures     201349   11.8   170  Área Met. Lisboa\n7   Almada     177943   14.7   170  Área Met. Lisboa\n\n\n\n\n# filtrar grupos\nfor group_name, group_data in df5.groupby('nuts3'):\n    if group_data['area'].max() &gt; 100:\n        print(\"Categorias com area &gt; 100:\", group_name)\n\nCategorias com area &gt; 100: 170\n\n\n\n# Criar visualizacoes por grupo\nimport matplotlib.pyplot as plt\n\ndf5.set_index('cidade', inplace = True )\nfor group_name, group_data in df5.groupby('nuts3'):\n    group_data['populacao'].plot(kind='bar', title=group_name)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n# para aplicar funcao dentro do grupo\n# Function to calculate percentage change within each group\ndef calculate_relative_percentage(group,col_name):\n    total_sum = group[col_name].sum()\n    group['relative_percentage'] = (group[col_name] / total_sum) * 100\n    return group\n\n# Apply the custom analysis to each group\nresult_df = pd.DataFrame()\nfor name, group in df5.groupby('nuts3'):\n    group = calculate_relative_percentage(group, 'populacao')\n    result_df = pd.concat([result_df, group])\n\nresult_df\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\nrelative_percentage\n\n\ncidade\n\n\n\n\n\n\n\n\n\nBraga\n193324\n41.0\n112\nCávado\n100.000000\n\n\nVila Nova de Gaia\n304233\n56.3\n11A\nÁrea Met. Porto\n56.752794\n\n\nPorto\n231834\n41.4\n11A\nÁrea Met. Porto\n43.247206\n\n\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\n35.720497\n\n\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\n25.329938\n\n\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\n14.059107\n\n\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\n13.213221\n\n\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\n11.677238\n\n\n\n\n\n\n\n\ndf5.groupby('nuts3')['populacao'].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nnuts3\n\n\n\n\n\n\n\n\n\n\n\n\n112\n1.0\n193324.0\nNaN\n193324.0\n193324.00\n193324.0\n193324.00\n193324.0\n\n\n11A\n2.0\n268033.5\n51193.823851\n231834.0\n249933.75\n268033.5\n286133.25\n304233.0\n\n\n170\n5.0\n304769.0\n157289.523739\n177943.0\n201349.00\n214239.0\n385989.00\n544325.0\n\n\n\n\n\n\n\nFunções de Agregação\nvarias funções de agregação podem ser aplicadas em simultâneo\n\ndf5.groupby('nuts3')['populacao'].aggregate([\"min\", \"median\", \"mean\", \"max\"])\n\n\n\n\n\n\n\n\nmin\nmedian\nmean\nmax\n\n\nnuts3\n\n\n\n\n\n\n\n\n112\n193324\n193324.0\n193324.0\n193324\n\n\n11A\n231834\n268033.5\n268033.5\n304233\n\n\n170\n177943\n214239.0\n304769.0\n544325\n\n\n\n\n\n\n\nUtilização de filtros\n\ndef filter_func(x):\n    \"\"\"Defino a função de filtro\"\"\"\n    return x['populacao'].std() &gt; 100000\n\n# a função de filtro é aplicado ao grupo\ndf5.groupby('nuts3').filter(filter_func)\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\ncidade\n\n\n\n\n\n\n\n\nLisboa\n544325\n100.1\n170\nÁrea Met. Lisboa\n\n\nSintra\n385989\n23.8\n170\nÁrea Met. Lisboa\n\n\nCascais\n214239\n97.1\n170\nÁrea Met. Lisboa\n\n\nLoures\n201349\n11.8\n170\nÁrea Met. Lisboa\n\n\nAlmada\n177943\n14.7\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\n\ndf5.groupby('nuts3')['populacao'].std()\n\nnuts3\n112              NaN\n11A     51193.823851\n170    157289.523739\nName: populacao, dtype: float64\n\n\nTambém é comum passar as colunas de mapeamento dum dicionário para operações a serem aplicadas nessa coluna\n\ndf5.groupby('nuts3').aggregate({'populacao': 'min', 'area': 'max'})\n\n\n\n\n\n\n\n\npopulacao\narea\n\n\nnuts3\n\n\n\n\n\n\n112\n193324\n41.0\n\n\n11A\n231834\n56.3\n\n\n170\n177943\n100.1\n\n\n\n\n\n\n\nMétodo Transform (conserva o nr de linhas original)\nNa transformação, a saída tem o mesmo formato da entrada\n\ndef center(x):\n    return x - x.mean()\n\ndf5\nprint()\ndf5.groupby('nuts3')['populacao'].transform(center)\n\n\n\n\ncidade\nBraga                     0.0\nLisboa               239556.0\nSintra                81220.0\nVila Nova de Gaia     36199.5\nPorto                -36199.5\nCascais              -90530.0\nLoures              -103420.0\nAlmada              -126826.0\nName: populacao, dtype: float64\n\n\nMétodo Apply\n\ndef norm_by_area(x):\n    # x is a DataFrame of group values\n    x['populacao'] /= x['area'].sum()\n    return x\n\ndf5.groupby('nuts3').apply(norm_by_area)\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\2164890277.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df5.groupby('nuts3').apply(norm_by_area)\n\n\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\nnuts3\ncidade\n\n\n\n\n\n\n\n\n112\nBraga\n4715.219512\n41.0\n112\nCávado\n\n\n11A\nVila Nova de Gaia\n3113.950870\n56.3\n11A\nÁrea Met. Porto\n\n\nPorto\n2372.917093\n41.4\n11A\nÁrea Met. Porto\n\n\n170\nLisboa\n2199.292929\n100.1\n170\nÁrea Met. Lisboa\n\n\nSintra\n1559.551515\n23.8\n170\nÁrea Met. Lisboa\n\n\nCascais\n865.612121\n97.1\n170\nÁrea Met. Lisboa\n\n\nLoures\n813.531313\n11.8\n170\nÁrea Met. Lisboa\n\n\nAlmada\n718.961616\n14.7\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\n\n# com o group by por nuts3 somam-se as áreas da nuts3 \n# por exemplo na 11A será 56.3 + 41.4 = 97.7\n# como pop VNGaia = 304233 Porto = 231834 \nprint(\"Pop normalizada por nuts3 de {0} é {1}\". format(\"VNGaia\", 304233/97.7))\nprint(\"Pop normalizada por nuts3 de {0} é {1}\". format(\"Porto\", 231834/97.7))\n\nPop normalizada por nuts3 de VNGaia é 3113.950870010235\nPop normalizada por nuts3 de Porto é 2372.9170931422723\n\n\n\ndf5.groupby('cidade').apply(norm_by_area)\n\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\ncidade\ncidade\n\n\n\n\n\n\n\n\nAlmada\nAlmada\n12104.965986\n14.7\n170\nÁrea Met. Lisboa\n\n\nBraga\nBraga\n4715.219512\n41.0\n112\nCávado\n\n\nCascais\nCascais\n2206.374871\n97.1\n170\nÁrea Met. Lisboa\n\n\nLisboa\nLisboa\n5437.812188\n100.1\n170\nÁrea Met. Lisboa\n\n\nLoures\nLoures\n17063.474576\n11.8\n170\nÁrea Met. Lisboa\n\n\nPorto\nPorto\n5599.855072\n41.4\n11A\nÁrea Met. Porto\n\n\nSintra\nSintra\n16218.025210\n23.8\n170\nÁrea Met. Lisboa\n\n\nVila Nova de Gaia\nVila Nova de Gaia\n5403.783304\n56.3\n11A\nÁrea Met. Porto\n\n\n\n\n\n\n\n\n# com o group by por cidade não há lugar a somas... \n# as áreas são VNGaia = 56.3 Porto = 41.4 \n# como pop VNGaia = 304233 Porto = 231834 \nprint(\"Pop normalizada por cidade de{0} é {1}\". format(\"VNGaia\", 304233/56.3))\nprint(\"Pop normalizada por cidade de{0} é {1}\". format(\"Porto\", 231834/41.4))\n\nPop normalizada por cidade deVNGaia é 5403.783303730018\nPop normalizada por cidade dePorto é 5599.855072463768\n\n\nDiferenças entre Apply e Transform\n\ntransform() pode receber uma função, uma função de string, uma lista de funções e um dicionário. No entanto, apply() só é pode receber uma função.\n\ntransform() não pode produzir resultados agregados\n\napply() funciona com várias séries (várias colunas) ao mesmo tempo. No entanto, transform() só pode funcionar com uma série de cada vez.\n\n\n# Função de string\ndf5['populacao'].transform('sqrt')\n\n# lista de funções\ndf5['area'].transform([np.sqrt, np.exp])\n\n# Dicionário\ndf5.transform({\n    'populacao': np.sqrt,\n    'area': np.exp,\n})\n\n\n\n\n\n\n\n\npopulacao\narea\n\n\ncidade\n\n\n\n\n\n\nBraga\n439.686252\n6.398435e+17\n\n\nLisboa\n737.783844\n2.970829e+43\n\n\nSintra\n621.280130\n2.168746e+10\n\n\nVila Nova de Gaia\n551.573205\n2.823445e+24\n\n\nPorto\n481.491433\n9.545343e+17\n\n\nCascais\n462.859590\n1.479089e+42\n\n\nLoures\n448.719289\n1.332524e+05\n\n\nAlmada\n421.832905\n2.421748e+06\n\n\n\n\n\n\n\n\n# Apply consegue produzir agregados\ndf5.apply(lambda x:x.sum())\n\npopulacao                                              2253236\narea                                                     386.2\nnuts3                                 11217017011A11A170170170\nnuts3_dsg    CávadoÁrea Met. LisboaÁrea Met. LisboaÁrea Met...\ndtype: object\n\n\n\n## mas não funciona com o transform\ndf5.transform(lambda x:x.sum())\n\n\ndef subtract_two(x):\n    return x['populacao'] - x['area']\n  \n# apply funciona com várias séries em simultâneo\ndf5.apply(subtract_two, axis=1)\n\ncidade\nBraga                193283.0\nLisboa               544224.9\nSintra               385965.2\nVila Nova de Gaia    304176.7\nPorto                231792.6\nCascais              214141.9\nLoures               201337.2\nAlmada               177928.3\ndtype: float64\n\n\n\n# mas o transform não\ndf5.transform(subtract_two, axis=1)\n\nEspecificar as Split Keys para os grupos\nPodemos fazer grupos com uma lista, série ou index a especificar as keys pelas quais se faz o agrupamento. A key pode ser uma série ou lista com o comprimento da DataFrame.\n\nL = [0, 1, 0, 1, 2, 0, 3, 1]\ndf5.groupby(L).sum()\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3\nnuts3_dsg\n\n\n\n\n0\n793552\n161.9\n112170170\nCávadoÁrea Met. LisboaÁrea Met. Lisboa\n\n\n1\n1026501\n171.1\n17011A170\nÁrea Met. LisboaÁrea Met. PortoÁrea Met. Lisboa\n\n\n2\n231834\n41.4\n11A\nÁrea Met. Porto\n\n\n3\n201349\n11.8\n170\nÁrea Met. Lisboa\n\n\n\n\n\n\n\n\n# forma mais verbosa equivalente ao que temos usado até agora\n# aqui explictamos que a key é df5['nuts3'] e não apenas 'nuts3'\ndf5.groupby(df5['nuts3']).sum()\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3_dsg\n\n\nnuts3\n\n\n\n\n\n\n\n112\n193324\n41.0\nCávado\n\n\n11A\n536067\n97.7\nÁrea Met. PortoÁrea Met. Porto\n\n\n170\n1523845\n247.5\nÁrea Met. LisboaÁrea Met. LisboaÁrea Met. Lisb...\n\n\n\n\n\n\n\n\n# com um dicionário\n\ndf2g = df5.set_index('nuts3')\nmapping = {'11A': 'norte', '112': 'norte', '170': 'centro'}\ndf2g\nprint()\ndf2g.groupby(mapping).sum()\n\n\n\n\n\n\n\n\n\n\n\npopulacao\narea\nnuts3_dsg\n\n\nnuts3\n\n\n\n\n\n\n\ncentro\n1523845\n247.5\nÁrea Met. LisboaÁrea Met. LisboaÁrea Met. Lisb...\n\n\nnorte\n729391\n138.7\nCávadoÁrea Met. PortoÁrea Met. Porto\n\n\n\n\n\n\n\n\n\n5.1.4.4 Pivot Tables\n\nimport seaborn as sns # importamos esta package para termos acesso a um dataset\n\ntitanic = sns.load_dataset('titanic')\n\ntitanic.head()\nprint()\ntitanic.describe()\n\n\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\nPreparar manualmente a Pivot Table\n\ntitanic.groupby('sex')[['survived']].mean()\nprint()\ntitanic.groupby(['sex', 'class'], observed=True)['survived'].mean().unstack() # unstack para ter a pivot table\n\n\n\n\n\n\n\n\n\n\nclass\nFirst\nSecond\nThird\n\n\nsex\n\n\n\n\n\n\n\nfemale\n0.968085\n0.921053\n0.500000\n\n\nmale\n0.368852\n0.157407\n0.135447\n\n\n\n\n\n\n\n\nprint()\ntitanic.pivot_table(index='sex', columns='class',\n                    values='survived', aggfunc='mean', observed=True)\n\n\n\n\n\n\n\n\n\n\nclass\nFirst\nSecond\nThird\n\n\nsex\n\n\n\n\n\n\n\nfemale\n0.968085\n0.921053\n0.500000\n\n\nmale\n0.368852\n0.157407\n0.135447\n\n\n\n\n\n\n\nSyntax Pivot Table\n\ntitanic.pivot_table('survived', index='sex', columns='class', aggfunc='mean')\nprint()\nage_group = pd.cut(titanic['age'], [0, 18, 80])\nage_group\nprint()\n# multilevel pivot table\ntitanic.pivot_table('survived', index=['sex', age_group], \n                    columns='class', aggfunc='mean')\n\n\n\n\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\4189409504.py:1: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('survived', index='sex', columns='class', aggfunc='mean')\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\4189409504.py:7: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('survived', index=['sex', age_group],\n\n\n\n\n\n\n\n\n\nclass\nFirst\nSecond\nThird\n\n\nsex\nage\n\n\n\n\n\n\n\nfemale\n(0, 18]\n0.909091\n1.000000\n0.511628\n\n\n(18, 80]\n0.972973\n0.900000\n0.423729\n\n\nmale\n(0, 18]\n0.800000\n0.600000\n0.215686\n\n\n(18, 80]\n0.375000\n0.071429\n0.133663\n\n\n\n\n\n\n\nSummary Statistics na DataFrame\n\ntitanic.describe()\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\ntodas estas funções estão disponíveis como fomos vendo nos exemplos anteriores\n\ntitanic.pivot_table('fare', index=['sex'], \n                    columns='class', aggfunc='mean')\n                    \nprint()\n\ntitanic.pivot_table('survived', index=['class'], aggfunc='count')\n\n\n\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\4083826376.py:1: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('fare', index=['sex'],\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\4083826376.py:6: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  titanic.pivot_table('survived', index=['class'], aggfunc='count')\n\n\n\n\n\n\n\n\n\nsurvived\n\n\nclass\n\n\n\n\n\nFirst\n216\n\n\nSecond\n184\n\n\nThird\n491"
  },
  {
    "objectID": "500-mod5.html#estatisticas-oficiais",
    "href": "500-mod5.html#estatisticas-oficiais",
    "title": "5  Data Science (Intermediate)",
    "section": "5.2 Estatisticas Oficiais",
    "text": "5.2 Estatisticas Oficiais\n\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"PT_2012_Hosp.csv\"\n\nler os dados:\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nprint()\ndf_hosp.describe()\n\ndf_hosp = df_hosp.reset_index() # passar o index para uma coluna; podia ser feito com inplace = True\nprint()\ndf_hosp.head()\n\n\n\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\281557973.py:1: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nDTCC_COD\nCC_DSG\nC10001\nC20001\nC21001\nC21011\nC21021\n...\nC30001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\n\n\n\n\n0\n2012\n229\n17\n1504\nBarreiro\n1458.0\n247.0\n159.0\n2.0\n11.0\n...\n493.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n\n\n1\n2012\n206\n17\n1507\nMontijo\n144.0\n0.0\n0.0\n0.0\n0.0\n...\n46.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n\n\n2\n2012\n65\n16\n502\nCastelo Branco\n894.0\n111.0\n89.0\n0.0\n8.0\n...\n337.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n\n\n3\n2012\n106\n17\n1114\nVila Franca de Xira\n801.0\n166.0\n108.0\n0.0\n12.0\n...\n264.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n\n\n4\n2012\n209\n11\n1315\nValongo\n221.0\n13.0\n13.0\n0.0\n0.0\n...\n88.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n\n\n\n\n5 rows × 64 columns\n\n\n\n\n5.2.1 Pre-processamento\n\n5.2.1.1 Exclusão de colunas:\n\n# exclusão da coluna nordem\ndf_hosp = df_hosp.drop(columns=['NORDEM'])\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNUTS2\nDTCC_COD\nCC_DSG\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\n...\nC30001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\n\n\n\n\n0\n2012\n17\n1504\nBarreiro\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n...\n493.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n\n\n1\n2012\n17\n1507\nMontijo\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n46.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n\n\n2\n2012\n16\n502\nCastelo Branco\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n...\n337.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n\n\n3\n2012\n17\n1114\nVila Franca de Xira\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n...\n264.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n\n\n4\n2012\n11\n1315\nValongo\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n...\n88.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n\n\n\n\n5 rows × 63 columns\n\n\n\n\n\n5.2.1.2 Variáveis com demasiadas categorias:\n\ndf_hosp['NUTS2'].value_counts().sort_values()\nprint()\ndf_hosp['DTCC_COD'].value_counts()\nprint()\ndf_hosp['DTCC_COD'].value_counts(normalize=True) # resultados percentuais\n\n\n\n\n\nDTCC_COD\n1106    0.158416\n603     0.099010\n1312    0.069307\n602     0.019802\n1110    0.019802\n          ...   \n1503    0.009901\n1419    0.009901\n1401    0.009901\n1418    0.009901\n1408    0.009901\nName: proportion, Length: 63, dtype: float64\n\n\nAgregar todos os municipios com menos de dez por cento das ocorrências na amostra:\n\n# verifica contagens para a coluna DTCC_COD\ndf_hosp['DTCC_COD_COUNT']= df_hosp.DTCC_COD.map(df_hosp.DTCC_COD.value_counts(normalize=True)) # Constroi a nova coluna\n\n# Correr sem criar a coluna primeiro\ndf_hosp.loc[df_hosp['DTCC_COD_COUNT'] &lt; 0.1, 'DTCC_COD_NEW'] = 'outro' # Constroi a nova coluna\ndf_hosp.loc[df_hosp['DTCC_COD_COUNT'] &gt;= 0.1, 'DTCC_COD_NEW'] = df_hosp['CC_DSG'] # atualiza os valores em falta\n\nprint()\ndf_hosp.loc[:,['DTCC_COD_NEW','DTCC_COD', 'CC_DSG']] # todas as linhas e as colunas selecionadas\n\n\n\n\n\n\n\n\n\n\n\nDTCC_COD_NEW\nDTCC_COD\nCC_DSG\n\n\n\n\n0\noutro\n1504\nBarreiro\n\n\n1\noutro\n1507\nMontijo\n\n\n2\noutro\n502\nCastelo Branco\n\n\n3\noutro\n1114\nVila Franca de Xira\n\n\n4\noutro\n1315\nValongo\n\n\n...\n...\n...\n...\n\n\n96\noutro\n1110\nOeiras\n\n\n97\nLisboa\n1106\nLisboa\n\n\n98\noutro\n1312\nPorto\n\n\n99\noutro\n1312\nPorto\n\n\n100\noutro\n1408\nConstância\n\n\n\n\n101 rows × 3 columns\n\n\n\n\n# exclusão das colunas já tratadas\ndf_hosp = df_hosp.drop(columns=['DTCC_COD', 'CC_DSG','DTCC_COD_COUNT'])\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n4\n2012\n11\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\noutro\n\n\n\n\n5 rows × 62 columns\n\n\n\n\n\n5.2.1.3 Missing data\nsem o scikit\nverificar onde temos dados em falta o método isna()\n\n#Check missing data\ndf_hosp.isna().sum()\n\nANO              0\nNUTS2            0\nC10001          15\nC20001          15\nC21001          15\n                ..\nC31051          15\nC31061          15\nC31071          15\nC32001          15\nDTCC_COD_NEW     0\nLength: 62, dtype: int64\n\n\npor haver linhas com todos os valores missing então há colunas que devem ser excluídos.\n\ndf_hosp[df_hosp.isna().any(axis=1)]\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n11\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n12\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n13\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n14\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n15\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n16\n2012\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n54\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n55\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n56\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n57\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n58\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n59\n2012\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nLisboa\n\n\n71\n2012\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n72\n2012\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n73\n2012\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\noutro\n\n\n\n\n15 rows × 62 columns\n\n\n\n\n# dropna pode ser parametrizado para linhas ou colunas, e até thresholds\ndf_hosp1 = df_hosp.dropna()\ndf_hosp1\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n4\n2012\n11\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\noutro\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\n2012\n17\n96.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n9.0\n0.0\n0.0\n0.0\n3.0\n1.0\n4.0\n1.0\n20.0\noutro\n\n\n97\n2012\n17\n191.0\n42.0\n42.0\n0.0\n1.0\n0.0\n2.0\n0.0\n...\n5.0\n0.0\n0.0\n3.0\n0.0\n0.0\n2.0\n0.0\n27.0\nLisboa\n\n\n98\n2012\n11\n66.0\n21.0\n16.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n8.0\noutro\n\n\n99\n2012\n11\n491.0\n132.0\n76.0\n0.0\n4.0\n1.0\n3.0\n0.0\n...\n6.0\n0.0\n1.0\n3.0\n0.0\n1.0\n1.0\n0.0\n93.0\noutro\n\n\n100\n2012\n16\n59.0\n6.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n16.0\noutro\n\n\n\n\n86 rows × 62 columns\n\n\n\n\n# vamos fazer uma cópia para testes\ndf1 = df_hosp1.copy()\ndf1\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n89.0\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n4\n2012\n11\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\noutro\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\n2012\n17\n96.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n9.0\n0.0\n0.0\n0.0\n3.0\n1.0\n4.0\n1.0\n20.0\noutro\n\n\n97\n2012\n17\n191.0\n42.0\n42.0\n0.0\n1.0\n0.0\n2.0\n0.0\n...\n5.0\n0.0\n0.0\n3.0\n0.0\n0.0\n2.0\n0.0\n27.0\nLisboa\n\n\n98\n2012\n11\n66.0\n21.0\n16.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n8.0\noutro\n\n\n99\n2012\n11\n491.0\n132.0\n76.0\n0.0\n4.0\n1.0\n3.0\n0.0\n...\n6.0\n0.0\n1.0\n3.0\n0.0\n1.0\n1.0\n0.0\n93.0\noutro\n\n\n100\n2012\n16\n59.0\n6.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n16.0\noutro\n\n\n\n\n86 rows × 62 columns\n\n\n\nVamos introduzir um NaN num registo para experimentarmos outras formas de tratamento\n\ndf1.loc[0:3, :]\n\ndf1.loc[2,'C21001'] = np.nan\n\ndf1.loc[0:3, :]\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\nNaN\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n\n\n4 rows × 62 columns\n\n\n\nvamos inserir o valor médio da C21001 (Médicos - Especialistas - Total) para a nuts 16, mas de uma forma genérica para qualquer que fosse a coluna numérica:\n\n# passo 1 seleccionar a/as colunas onde há valores missing\ncols_missing = df1.isna().sum().index[df1.isna().sum().values &gt;0].tolist()\nprint(cols_missing)\n\n# passo 2 seleccionar o conjunto de colunas da dataframe que são numéricas\ncols_numerical = df1.select_dtypes(include=['number']).columns.tolist()\nprint(cols_numerical)\n\n# passo 3 imputar a mediana na for missing data\ncols_numerical_missing = [ col for col in cols_missing if (col in cols_numerical)] # loop for condicional\ndf1[cols_numerical_missing] = df1[cols_numerical_missing].fillna(df1.groupby('NUTS2')[cols_numerical_missing].transform('mean'))\n\ndf1.loc[0:3, :]\n\n['C21001']\n['ANO', 'NUTS2', 'C10001', 'C20001', 'C21001', 'C21011', 'C21021', 'C21031', 'C21041', 'C21061', 'C21071', 'C21081', 'C21091', 'C21101', 'C21111', 'C21121', 'C21131', 'C21141', 'C21151', 'C21161', 'C21171', 'C21181', 'C21191', 'C21201', 'C21211', 'C21221', 'C21231', 'C21241', 'C21251', 'C21261', 'C21271', 'C21281', 'C21291', 'C21301', 'C21311', 'C21321', 'C21331', 'C21341', 'C21351', 'C21361', 'C21371', 'C21381', 'C21391', 'C21401', 'C21411', 'C21421', 'C21431', 'C21441', 'C22001', 'C23001', 'C24001', 'C30001', 'C31001', 'C31011', 'C31021', 'C31031', 'C31041', 'C31051', 'C31061', 'C31071', 'C32001']\n\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nDTCC_COD_NEW\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\noutro\n\n\n1\n2012\n17\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\noutro\n\n\n2\n2012\n16\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\noutro\n\n\n3\n2012\n17\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\noutro\n\n\n\n\n4 rows × 62 columns\n\n\n\nquando não aplicamos o transform temos resultados diferentes:\n\n# obtemos uma dataframe com 7 linhas e index na nuts2\ndf1.groupby('NUTS2')[cols_numerical_missing].mean()\nprint()\n\n# obtemos o mesmo nº de linhas que a dataframe tinha com a média calculada pelo group by\n# sem index\ndf1.groupby('NUTS2')[cols_numerical_missing].transform('mean')\nprint()\n\n# verificação das médias\ndf1.groupby('NUTS2')['C21001'].mean()\n\n\n\n\n\nNUTS2\n11     57.933333\n15     61.500000\n16     38.250000\n17    152.300000\n18     68.500000\n20     30.000000\n30     29.000000\nName: C21001, dtype: float64\n\n\nusando o scikit\n\ndf1.loc[2,'C21001'] = np.nan\ndf1.loc[0:3, :]\n\n# o simple imputer não pode ser usado em colunas categóricas \n# com a estratégia de média, só em colunas numéricas\ndf1 = df1.drop(columns = ['DTCC_COD_NEW'])\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(df1)\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SimpleImputer?Documentation for SimpleImputeriFittedSimpleImputer() \n\n\ne agora atua sobre a dataframe:\n\nx = pd.DataFrame(imputer.transform(df1))\nx\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n\n0\n2012.0\n17.0\n1458.0\n247.0\n159.0\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n493.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n\n\n1\n2012.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n46.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n\n\n2\n2012.0\n16.0\n894.0\n111.0\n74.4\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n337.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n\n\n3\n2012.0\n17.0\n801.0\n166.0\n108.0\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n264.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n\n\n4\n2012.0\n11.0\n221.0\n13.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n88.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n81\n2012.0\n17.0\n96.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n29.0\n9.0\n0.0\n0.0\n0.0\n3.0\n1.0\n4.0\n1.0\n20.0\n\n\n82\n2012.0\n17.0\n191.0\n42.0\n42.0\n0.0\n1.0\n0.0\n2.0\n0.0\n...\n32.0\n5.0\n0.0\n0.0\n3.0\n0.0\n0.0\n2.0\n0.0\n27.0\n\n\n83\n2012.0\n11.0\n66.0\n21.0\n16.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n10.0\n2.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n8.0\n\n\n84\n2012.0\n11.0\n491.0\n132.0\n76.0\n0.0\n4.0\n1.0\n3.0\n0.0\n...\n99.0\n6.0\n0.0\n1.0\n3.0\n0.0\n1.0\n1.0\n0.0\n93.0\n\n\n85\n2012.0\n16.0\n59.0\n6.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n17.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n16.0\n\n\n\n\n86 rows × 61 columns\n\n\n\no método SimpleImputer do scikit.impute não suporta o groupby directamente mas é muito útil e suporta categorias quando a estratégia selecionada é ‘most_frequent’ ou ‘constant’ além disso pode ser usado indirectamente o groupby recorrendo a uma função lambda aplicada nas colunas dentro da função transform\n\nx.iloc[:4,:6]\n\ndf1.iloc[:4,:6]\n\ndf1['C21001'] = df1.groupby('NUTS2')['C21001'].transform(\n    lambda col: imputer.fit_transform(col.to_frame()).flatten(),)\nprint()\n\ndf1.iloc[:4,:6]\n\n\n\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.00\n2.0\n\n\n1\n2012\n17\n144.0\n0.0\n0.00\n0.0\n\n\n2\n2012\n16\n894.0\n111.0\n38.25\n0.0\n\n\n3\n2012\n17\n801.0\n166.0\n108.00\n0.0\n\n\n\n\n\n\n\nImputação de Variáveis Categóricas\n\n# Seleciona todas as colunas categóricas e atribui-lhes \n# um novo nível criado 'missing'\ncols_categorical = df_hosp.select_dtypes(include=['object']).columns.tolist()\n\ndf_hosp[cols_categorical] = df_hosp[cols_categorical].fillna('missing')\nprint(cols_categorical)\n\n['DTCC_COD_NEW']\n\n\nexemplo em que usamos vários imputadores em simultâneo, seria possível usando o ColumnTransformer do módulo sklearn.compose e especificando as colunas a fectar é apresentado abaixo:\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nA = [[np.nan,2,'Jose'],[4,np.nan,'Maria'],[7,8,'Joao'],[10,5,np.nan]]\n\ncol_trans = ColumnTransformer(\n[('imp0', SimpleImputer(strategy='constant', fill_value=1), [0]),\n ('imp1', SimpleImputer(strategy='mean'), [1]),\n ('imp2', SimpleImputer(strategy='constant', fill_value='desconhecido'), [2])],\nremainder='passthrough')\n\ncol_trans.fit_transform(A)\n\narray([[1, 2.0, 'Jose'],\n       [4, 5.0, 'Maria'],\n       [7, 8.0, 'Joao'],\n       [10, 5.0, 'desconhecido']], dtype=object)\n\n\nImputação com o vizinho mais próximo\n\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputer.fit_transform(X)\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\n\n\n5.2.1.4 Variáveis Categóricas\n\n## Typecast da coluna para categoria em usando o pandas\ndf1['NUTS2'] = pd.Categorical(df1.NUTS2)\n\ndf1.dtypes\n\nprint()\n## Typecast da coluna para categoria em python\ndf_hosp11 = df_hosp1.copy()\ndf_hosp11['NUTS2']= df_hosp1.NUTS2.astype('category')\ndf_hosp11.dtypes\n\n\n\n\nANO                int64\nNUTS2           category\nC10001           float64\nC20001           float64\nC21001           float64\n                  ...   \nC31051           float64\nC31061           float64\nC31071           float64\nC32001           float64\nDTCC_COD_NEW      object\nLength: 62, dtype: object\n\n\nCriação de Variáveis Categóricas\ncriar colunas à custa de outras colunas directamente usando o método map:\n\ndf_hosp2 = df1.copy()\ndf_hosp2\n\ndef label(value): # função para criar uma variável dummy\n    if value == 0:\n        return \"no\"\n    if value &gt; 0:\n        return \"yes\"\n\ndf_hosp2['t_cirurgia'] = df_hosp2['C21071'].map(label)\ndf_hosp2.head()\n\n\n\n\n\n\n\n\nANO\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\n\n\n0\n2012\n17\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\nyes\n\n\n1\n2012\n17\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\nno\n\n\n2\n2012\n16\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\nyes\n\n\n3\n2012\n17\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\nyes\n\n\n4\n2012\n11\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\nyes\n\n\n\n\n5 rows × 62 columns\n\n\n\nVariáveis Dummy\n\n# Definimos y como o nosso target\nX = df_hosp2.drop(columns=['t_cirurgia'])\ny = df_hosp2['t_cirurgia'].values\n\n#Transforma as variáveis categoricas em dummies com drop da baseline\ndf_x = pd.get_dummies(X, drop_first = True)\n\ndf_x.head()\n\n\n\n\n\n\n\n\nANO\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\nC21071\n...\nC31051\nC31061\nC31071\nC32001\nNUTS2_15\nNUTS2_16\nNUTS2_17\nNUTS2_18\nNUTS2_20\nNUTS2_30\n\n\n\n\n0\n2012\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n19.0\n...\n3.0\n5.0\n0.0\n454.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n2012\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n46.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\n2012\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n10.0\n...\n0.0\n0.0\n0.0\n337.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n12.0\n...\n2.0\n7.0\n0.0\n228.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n2012\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n...\n1.0\n6.0\n0.0\n80.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 66 columns\n\n\n\nVariáveis Dummy com Scikit\nScikit tem vários encoders os mais comuns são o OrdinalEncoder e o HotEncoder.\n\nOrdinalEncoder é usado para transformar variáveis categóricas em numéricas.\nOneHotEncoder é usado para transformar variáveis categóricas em variáveis dummy.\n\n(criar encoder, fazer o fit e efectuar a transformação)\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')\n\n# Definimos X_sk como a coluna para criar dummies nos exemplos scikit\n# atenção df_hosp2[['NUTS2']] é uma dataframe, df_hosp2['NUTS2'] é uma serie\nX_sk = df_hosp2[['NUTS2']]\n\nprint(type(df_hosp2[['NUTS2']]))\nprint()\nencoder = encoder.fit(X_sk)\n\nX_encoded = pd.DataFrame(encoder.transform(X_sk).toarray()) # transforma a matriz em dataframe\nX_encoded.head()\nprint()\n\nX_encoded = X_encoded.rename(columns = {0 : \"Nut0\", 1 : \"Nut1\", 2 : \"Nut2\", 3 : \"Nut3\"\n                                       , 4 : \"Nut4\", 5 : \"Nut5\", 6 : \"Nut6\"}) # renomeia as colunas\nX_encoded\nprint()\n\n# merge com a dataframe dos hospitais\ndf_hosp2 = df_hosp2.join(X_encoded)\ndf_hosp2 = df_hosp2.drop(columns = 'NUTS2') # drop da coluna original\ndf_hosp2.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nANO\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\nC21071\n...\nC31071\nC32001\nt_cirurgia\nNut0\nNut1\nNut2\nNut3\nNut4\nNut5\nNut6\n\n\n\n\n0\n2012\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n19.0\n...\n0.0\n454.0\nyes\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n2012\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n46.0\nno\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n2012\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n10.0\n...\n0.0\n337.0\nyes\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n2012\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n12.0\n...\n0.0\n228.0\nyes\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n2012\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n...\n0.0\n80.0\nyes\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 68 columns\n\n\n\n\n\n5.2.1.5 Variáveis Correlacionadas\nem modelos de regressão podemos ter problemas com variáveis correlacionadas (multicolinearidade)\n\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = df_x.corr().abs()\ncor_matrix\nprint()\n\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\nprint(upper_tri)\nprint()\n\n\n          ANO  C10001    C20001    C21001    C21011    C21021    C21031  \\\nANO       NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nC10001    NaN     NaN  0.982999  0.983593  0.783939  0.926770  0.662279   \nC20001    NaN     NaN       NaN  0.996023  0.758603  0.937420  0.689197   \nC21001    NaN     NaN       NaN       NaN  0.780568  0.944305  0.689538   \nC21011    NaN     NaN       NaN       NaN       NaN  0.708871  0.501019   \n...       ...     ...       ...       ...       ...       ...       ...   \nNUTS2_16  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_17  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_18  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_20  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \nNUTS2_30  NaN     NaN       NaN       NaN       NaN       NaN       NaN   \n\n            C21041    C21061    C21071  ...    C31051    C31061    C31071  \\\nANO            NaN       NaN       NaN  ...       NaN       NaN       NaN   \nC10001    0.856563  0.550499  0.900382  ...  0.285700  0.405187  0.334934   \nC20001    0.864047  0.560911  0.880763  ...  0.259565  0.412341  0.336556   \nC21001    0.860977  0.563814  0.891950  ...  0.250113  0.405222  0.341762   \nC21011    0.638383  0.576456  0.691786  ...  0.256874  0.146949  0.135031   \n...            ...       ...       ...  ...       ...       ...       ...   \nNUTS2_16       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_17       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_18       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_20       NaN       NaN       NaN  ...       NaN       NaN       NaN   \nNUTS2_30       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n\n            C32001  NUTS2_15  NUTS2_16  NUTS2_17  NUTS2_18  NUTS2_20  NUTS2_30  \nANO            NaN       NaN       NaN       NaN       NaN       NaN       NaN  \nC10001    0.993045  0.027644  0.205653  0.380013  0.035742  0.020845  0.057632  \nC20001    0.966742  0.022840  0.205238  0.370057  0.001802  0.042949  0.064700  \nC21001    0.968224  0.017892  0.212531  0.400595  0.013943  0.044323  0.064488  \nC21011    0.757519  0.022719  0.112272  0.370731  0.002489  0.041404  0.058901  \n...            ...       ...       ...       ...       ...       ...       ...  \nNUTS2_16       NaN       NaN       NaN  0.352410  0.175322  0.069438  0.098783  \nNUTS2_17       NaN       NaN       NaN       NaN  0.150756  0.059708  0.084941  \nNUTS2_18       NaN       NaN       NaN       NaN       NaN  0.029704  0.042258  \nNUTS2_20       NaN       NaN       NaN       NaN       NaN       NaN  0.016737  \nNUTS2_30       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n\n[66 rows x 66 columns]\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# o heatmap é uma boa forma de visualizar as matrizes de correlações\nplt.figure(figsize = (20,20))\nsns.heatmap(cor_matrix)\n\nplt.show()\n\n\n\n\n\n# seleciona para remover as colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] &gt; 0.9)]\nprint(to_drop)\n\n['C20001', 'C21001', 'C21021', 'C21071', 'C21251', 'C21421', 'C21431', 'C23001', 'C24001', 'C30001', 'C32001']\n\n\n\ndf_hosp3 = df_x.drop(columns=to_drop, axis=1)\ndf_hosp3.head()\n\n\n\n\n\n\n\n\nANO\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\nC21101\nC21111\n...\nC31041\nC31051\nC31061\nC31071\nNUTS2_15\nNUTS2_16\nNUTS2_17\nNUTS2_18\nNUTS2_20\nNUTS2_30\n\n\n\n\n0\n2012\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n2.0\n1.0\n...\n2.0\n3.0\n5.0\n0.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n2012\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\n2012\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n2.0\n2.0\n7.0\n0.0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n2012\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n6.0\n0.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 55 columns\n\n\n\n\n\n5.2.1.6 Class Imbalance\n\nfrom sklearn.linear_model import LogisticRegression\n\n# exemplo de declaração do regressor balanceando as classes\nLr = LogisticRegression(class_weight='balanced') # declarar uma regressão logística com classes balanceadas\n\n\n\n5.2.1.7 Normalização e Standardização\n\nshape = (4, 2)\nshape\n\nimport numpy.random as rd\n\n# *shape faz unpack do tuplo shape\nrd.rand(*shape) # gerador de samples de uma distribuição uniforme\n\narray([[0.95569962, 0.94993731],\n       [0.68944862, 0.12672182],\n       [0.66743279, 0.91807003],\n       [0.16408514, 0.83971314]])\n\n\n\n# random.RandomState.lognormal(mean=0.0, sigma=1.0, size=None) \n# gerador de samples de uma distribuição log-normal\nrd.rand(*shape) * rd.lognormal(0, 1, shape) \n\narray([[3.07867567, 0.29107532],\n       [0.32893935, 0.25393341],\n       [1.13404802, 1.4326565 ],\n       [0.38529293, 2.77981218]])\n\n\n\nimport numpy.random as rd\nimport pandas as pd\nfrom sklearn.preprocessing import Normalizer\nimport seaborn as sns\n\nimport warnings # para suprimir warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nshape = (100, 2)\n# *shape faz unpack do tuplo shape\ndf = pd.DataFrame(rd.rand(*shape) * rd.lognormal(1, 0.4, shape)\n                  , columns=[\"weight\", \"age\"]) # gerar um dataframe com 100 linhas e 2 colunas\nndf = pd.DataFrame(Normalizer(norm=\"l2\").fit_transform(df),\n                   columns=[\"norm_weight\", \"norm_age\"]) # normalizar os dados\n\n# kernel density estimate (KDE) plot é um método para \n# visualizar a distribuição de observações num dataset\nsns.kdeplot(data=pd.concat([df, ndf], axis=1), fill=True, \n            common_norm=False, palette=\"crest\",alpha=.5, linewidth=1,)\n            \nplt.show()\n            \ndf\n\n\n\n\n\n\n\n\n\n\n\nweight\nage\n\n\n\n\n0\n0.558178\n0.301456\n\n\n1\n1.108466\n2.647193\n\n\n2\n3.675595\n0.501199\n\n\n3\n1.056189\n3.962007\n\n\n4\n2.272442\n0.045789\n\n\n...\n...\n...\n\n\n95\n2.162981\n0.937799\n\n\n96\n2.416412\n0.363348\n\n\n97\n0.691724\n1.134979\n\n\n98\n0.237128\n1.090808\n\n\n99\n3.202288\n2.941029\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nfor d in [df]:\n    sns.pairplot(d.reset_index(), hue=\"index\", diag_kind=None)\n    \nplt.show()\n\n\n\n\n\n# repete o exemplo com shape(50,2) faz standardization\nimport numpy.random as rd\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\nshape = (50, 2)\n# *shape faz unpack do tuplo shape\ndf = pd.DataFrame(rd.rand(*shape) * rd.normal(10, 6, shape),\n                  columns=[\"weight\", \"age\"])\nndf = pd.DataFrame(StandardScaler().fit_transform(df),\n                   columns=[\"norm_weight\", \"norm_age\"])\n\n# kernel density estimate (KDE) plot é um método para \n# visualizar a distribuição de observações num dataset\nsns.kdeplot(data=pd.concat([df, ndf], axis=1), \n            fill=True, common_norm=False, palette=\"crest\",\n            alpha=.5, linewidth=1,)\n            \nplt.show()\n\n\n\n\n\nfor d in [df]:\n    sns.pairplot(d.reset_index(), hue=\"index\", palette=\"crest\", diag_kind=None)\n\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Define a pre-processing pipeline\npipeline = Pipeline([\n('selector', VarianceThreshold()),\n('scaler', StandardScaler()),\n('KNN', KNeighborsClassifier())])\n\n\n#Simple pipeline execution with default parameters\npipeline.fit(X, y)\nprint('Training set score: ' + str(pipeline.score(X,y)))\n\npipeline.predict(X)\n\ny\n\npipeline.predict_proba(X)\n\n\n\n\n5.2.2 Gravar os dados\n\ndf_prep = pd.concat([X, pd.DataFrame(y, columns=[\"t_cirurgia\"])], axis=1)\n\ndf_prep.head()\n\n# Exportação da dataframe\nfileout = 'df_prep.csv'\ndf_prep.to_csv(f\"{datadir}{fileout}\",  index=False)"
  },
  {
    "objectID": "500-mod5.html#introdução-a-machine-learning",
    "href": "500-mod5.html#introdução-a-machine-learning",
    "title": "5  Data Science (Intermediate)",
    "section": "5.3 Introdução a Machine Learning",
    "text": "5.3 Introdução a Machine Learning\n\n5.3.1 Regressão\nQUESTÃO:\nCom este conjunto de dados que inclui o número de médicos e enfermeiros em várias especialidades será que consigo estimar o nº de total de Enfermeiros - Especialistas - Em Saúde Infantil e Pediátrica em cada hospital?\n\nColuna C31011\n\n\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"df_prep.csv\"\n\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nprint()\ndf_hosp.describe()\n\n\n\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\390302264.py:1: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC30001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\n\n\n\n\ncount\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n...\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n\n\nmean\n118.081395\n14.976744\n642.744186\n113.872093\n73.979651\n0.813953\n6.941860\n0.360465\n2.953488\n0.302326\n...\n212.290698\n22.941860\n3.127907\n6.220930\n4.476744\n3.465116\n0.965116\n3.465116\n1.220930\n189.348837\n\n\nstd\n68.264230\n3.620129\n809.928453\n172.413511\n108.256016\n2.144825\n10.384779\n1.146994\n5.346592\n1.701693\n...\n255.792649\n26.911775\n4.752204\n10.678622\n5.740930\n8.559409\n1.482825\n4.421066\n2.783993\n240.396475\n\n\nmin\n3.000000\n11.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n57.500000\n11.000000\n120.250000\n7.750000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n34.250000\n3.250000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n30.000000\n\n\n50%\n119.000000\n16.000000\n338.500000\n42.500000\n33.000000\n0.000000\n2.000000\n0.000000\n0.000000\n0.000000\n...\n102.500000\n10.000000\n0.000000\n0.000000\n3.000000\n1.000000\n0.000000\n2.000000\n0.000000\n85.500000\n\n\n75%\n175.750000\n17.000000\n872.500000\n162.500000\n115.500000\n0.000000\n10.750000\n0.000000\n3.750000\n0.000000\n...\n321.250000\n35.750000\n4.000000\n11.000000\n6.000000\n3.000000\n1.000000\n5.000000\n1.000000\n282.250000\n\n\nmax\n229.000000\n30.000000\n5325.000000\n1161.000000\n719.000000\n13.000000\n51.000000\n6.000000\n31.000000\n13.000000\n...\n1515.000000\n103.000000\n21.000000\n50.000000\n28.000000\n72.000000\n6.000000\n19.000000\n15.000000\n1515.000000\n\n\n\n\n8 rows × 61 columns\n\n\n\n\ndf_hosp = df_hosp.reset_index() # passar o index para uma coluna; podia ser feito com inplace = True\n\n\n# Definimos y como o nosso target\nX = df_hosp.drop(columns=['t_cirurgia'])\ny = df_hosp['t_cirurgia'].values\n\n\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = X.corr().abs()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n\n\n# seleciona para remover as colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] &gt; 0.9)]\nprint(to_drop)\n\n['C20001', 'C21001', 'C21021', 'C21071', 'C21251', 'C21421', 'C21431', 'C23001', 'C24001', 'C30001', 'C32001']\n\n\n\ndf_hosp = df_hosp.drop(columns=to_drop, axis=1) # drop das colunas altamente correlacionadas\n\n\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC22001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n4.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n7.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n\n\n\n\n5 rows × 52 columns\n\n\n\nprimeiro modelo de regressão linear com uma variavél explicativa:\n\nimport statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001',data = df_hosp).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.679\nModel:                            OLS   Adj. R-squared:                  0.675\nMethod:                 Least Squares   F-statistic:                     177.9\nDate:                Thu, 30 May 2024   Prob (F-statistic):           1.89e-22\nTime:                        11:56:49   Log-Likelihood:                -206.66\nNo. Observations:                  86   AIC:                             417.3\nDf Residuals:                      84   BIC:                             422.2\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.2111      0.385     -0.549      0.585      -0.976       0.554\nC31001         0.1455      0.011     13.339      0.000       0.124       0.167\n==============================================================================\nOmnibus:                       27.749   Durbin-Watson:                   2.131\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              128.206\nSkew:                          -0.795   Prob(JB):                     1.45e-28\nKurtosis:                       8.766   Cond. No.                         46.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Este é o R entre as 2 variáveis\nr = cor_matrix.loc['C31001','C31011']\nr\n\n0.8242029193194886\n\n\n\n# E este é o r quadrado\nr2 = r**2\nr2\n\n0.6793104522147675\n\n\nmodelos com duas variáveis explicativas:\n\nimport statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001 + C21011',data = df_hosp).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.681\nModel:                            OLS   Adj. R-squared:                  0.673\nMethod:                 Least Squares   F-statistic:                     88.60\nDate:                Thu, 30 May 2024   Prob (F-statistic):           2.55e-21\nTime:                        11:56:49   Log-Likelihood:                -206.43\nNo. Observations:                  86   AIC:                             418.9\nDf Residuals:                      83   BIC:                             426.2\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.2542      0.391     -0.650      0.518      -1.032       0.524\nC31001         0.1441      0.011     12.916      0.000       0.122       0.166\nC21011         0.0935      0.140      0.668      0.506      -0.185       0.372\n==============================================================================\nOmnibus:                       25.855   Durbin-Watson:                   2.130\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              118.604\nSkew:                          -0.715   Prob(JB):                     1.76e-26\nKurtosis:                       8.573   Cond. No.                         47.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# import statsmodels.formula.api as smf\n\nest = smf.ols('C31011 ~ C31001 + C21361',data = df_hosp).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.697\nModel:                            OLS   Adj. R-squared:                  0.689\nMethod:                 Least Squares   F-statistic:                     95.36\nDate:                Thu, 30 May 2024   Prob (F-statistic):           3.12e-22\nTime:                        11:56:49   Log-Likelihood:                -204.26\nNo. Observations:                  86   AIC:                             414.5\nDf Residuals:                      83   BIC:                             421.9\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.2519      0.377     -0.669      0.505      -1.001       0.497\nC31001         0.1263      0.014      9.138      0.000       0.099       0.154\nC21361         0.0760      0.035      2.186      0.032       0.007       0.145\n==============================================================================\nOmnibus:                       23.605   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               93.192\nSkew:                          -0.684   Prob(JB):                     5.80e-21\nKurtosis:                       7.913   Cond. No.                         48.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nmodelo com todas as variáveis explicativas:\n\n# reorganiza as colunas para colocar a coluna target no fim\nlast_cols = ['C31011']\nfirst_cols = [col for col in df_hosp.columns if col not in last_cols]\n\ndf = df_hosp[first_cols+last_cols]\ndf.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC22001\nC31001\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\nC31011\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n4.0\n39.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n8.0\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n0.0\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n0.0\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n7.0\n36.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n4.0\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n8.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n0.0\n\n\n\n\n5 rows × 52 columns\n\n\n\n\nstring_cols = ' + '.join(df.columns[:-1])\nest = smf.ols('C31011 ~ {}'.format(string_cols),data = df).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       1.000\nModel:                            OLS   Adj. R-squared:                  1.000\nMethod:                 Least Squares   F-statistic:                 1.313e+25\nDate:                Thu, 30 May 2024   Prob (F-statistic):          4.89e-260\nTime:                        11:56:49   Log-Likelihood:                 1869.0\nNo. Observations:                  71   AIC:                            -3638.\nDf Residuals:                      21   BIC:                            -3525.\nDf Model:                          49                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept         -2.744e-17    4.8e-19    -57.221      0.000   -2.84e-17   -2.64e-17\nt_cirurgia[T.yes] -3.011e-15    6.7e-13     -0.004      0.996    -1.4e-12    1.39e-12\nANO                -5.21e-16   9.65e-16     -0.540      0.595   -2.53e-15    1.49e-15\nNORDEM            -3.381e-16   6.81e-15     -0.050      0.961   -1.45e-14    1.38e-14\nNUTS2              1.128e-15   9.23e-14      0.012      0.990   -1.91e-13    1.93e-13\nC10001              2.89e-16   5.08e-15      0.057      0.955   -1.03e-14    1.09e-14\nC21011              4.33e-15   2.34e-12      0.002      0.999   -4.86e-12    4.87e-12\nC21031            -3.053e-15    1.8e-12     -0.002      0.999   -3.76e-12    3.75e-12\nC21041             1.887e-15   3.77e-13      0.005      0.996   -7.81e-13    7.85e-13\nC21061            -6.883e-15    1.4e-12     -0.005      0.996   -2.92e-12    2.91e-12\nC21081             -2.22e-14   8.34e-12     -0.003      0.998   -1.74e-11    1.73e-11\nC21091            -9.267e-15   3.65e-12     -0.003      0.998    -7.6e-12    7.58e-12\nC21101            -3.553e-15   1.75e-12     -0.002      0.998   -3.65e-12    3.64e-12\nC21111            -1.443e-15   8.72e-13     -0.002      0.999   -1.82e-12    1.81e-12\nC21121            -3.372e-16    1.7e-13     -0.002      0.998   -3.54e-13    3.53e-13\nC21131             3.997e-15   1.87e-12      0.002      0.998   -3.89e-12    3.89e-12\nC21141            -5.107e-15   7.91e-13     -0.006      0.995   -1.65e-12    1.64e-12\nC21151            -2.446e-15   1.18e-12     -0.002      0.998   -2.46e-12    2.46e-12\nC21161             1.593e-14   9.24e-12      0.002      0.999   -1.92e-11    1.92e-11\nC21171            -1.943e-15   2.47e-13     -0.008      0.994   -5.15e-13    5.11e-13\nC21181              1.21e-14   8.42e-13      0.014      0.989   -1.74e-12    1.76e-12\nC21191              1.36e-15   7.98e-13      0.002      0.999   -1.66e-12    1.66e-12\nC21201             -1.11e-15   8.42e-13     -0.001      0.999   -1.75e-12    1.75e-12\nC21211             7.461e-14   6.04e-12      0.012      0.990   -1.25e-11    1.26e-11\nC21221            -1.421e-14    2.1e-12     -0.007      0.995   -4.37e-12    4.34e-12\nC21231            -2.956e-15   4.92e-13     -0.006      0.995   -1.03e-12    1.02e-12\nC21241             8.882e-16    4.3e-13      0.002      0.998   -8.94e-13    8.95e-13\nC21261            -7.994e-15   4.99e-12     -0.002      0.999   -1.04e-11    1.04e-11\nC21271             1.443e-15   5.72e-13      0.003      0.998   -1.19e-12    1.19e-12\nC21281             1.943e-15    2.4e-12      0.001      0.999   -4.98e-12    4.98e-12\nC21291             4.219e-15      7e-13      0.006      0.995   -1.45e-12    1.46e-12\nC21301            -1.332e-15   1.27e-12     -0.001      0.999   -2.65e-12    2.65e-12\nC21311             -2.22e-16   4.36e-13     -0.001      1.000   -9.07e-13    9.07e-13\nC21321            -1.915e-15   1.14e-12     -0.002      0.999   -2.37e-12    2.36e-12\nC21331              1.11e-15   1.62e-13      0.007      0.995   -3.36e-13    3.38e-13\nC21341             1.887e-15   6.94e-13      0.003      0.998   -1.44e-12    1.44e-12\nC21351            -5.024e-15    4.3e-13     -0.012      0.991      -9e-13     8.9e-13\nC21361             1.832e-15   3.55e-13      0.005      0.996   -7.36e-13     7.4e-13\nC21371             5.412e-16   2.02e-13      0.003      0.998   -4.19e-13     4.2e-13\nC21381              3.86e-16   1.22e-13      0.003      0.998   -2.53e-13    2.54e-13\nC21391            -4.441e-15   1.01e-12     -0.004      0.997   -2.11e-12    2.11e-12\nC21401            -8.604e-16   3.76e-13     -0.002      0.998   -7.83e-13    7.81e-13\nC21411            -5.773e-15   2.11e-12     -0.003      0.998    -4.4e-12    4.39e-12\nC21441              1.45e-16   3.21e-27   4.51e+10      0.000    1.45e-16    1.45e-16\nC22001            -7.633e-16   1.57e-13     -0.005      0.996   -3.28e-13    3.26e-13\nC31001                1.0000   3.68e-13   2.72e+12      0.000       1.000       1.000\nC31021               -1.0000   4.24e-13  -2.36e+12      0.000      -1.000      -1.000\nC31031               -1.0000    2.4e-13  -4.16e+12      0.000      -1.000      -1.000\nC31041               -1.0000   3.75e-13  -2.67e+12      0.000      -1.000      -1.000\nC31051               -1.0000   5.72e-13  -1.75e+12      0.000      -1.000      -1.000\nC31061               -1.0000   4.48e-13  -2.23e+12      0.000      -1.000      -1.000\nC31071               -1.0000   5.96e-13  -1.68e+12      0.000      -1.000      -1.000\n==============================================================================\nOmnibus:                       59.367   Durbin-Watson:                   0.132\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              330.141\nSkew:                          -2.462   Prob(JB):                     2.05e-72\nKurtosis:                      12.346   Cond. No.                     3.45e+19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.8e-31. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n\n\n\n\n\nWarning\n\n\n\na variável C31001 permite construir Y directamente!\n\n\n\ndf = df.drop(columns='C31001', axis=1)\ndf.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC21441\nC22001\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\nC31011\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n4.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n8.0\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n0.0\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n0.0\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n7.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n4.0\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n0.0\n\n\n\n\n5 rows × 51 columns\n\n\n\ne voltamos a fazer o modelo\n\nstring_cols = ' + '.join(df.columns[:-1])\nest = smf.ols('C31011 ~ {}'.format(string_cols),data = df).fit()\nprint(est.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 C31011   R-squared:                       0.988\nModel:                            OLS   Adj. R-squared:                  0.963\nMethod:                 Least Squares   F-statistic:                     39.38\nDate:                Thu, 30 May 2024   Prob (F-statistic):           1.05e-13\nTime:                        11:56:49   Log-Likelihood:                -55.748\nNo. Observations:                  71   AIC:                             209.5\nDf Residuals:                      22   BIC:                             320.4\nDf Model:                          48                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept           6.22e-07   2.44e-07      2.544      0.018    1.15e-07    1.13e-06\nt_cirurgia[T.yes]    -0.4189      0.378     -1.108      0.280      -1.203       0.365\nANO                   0.0013      0.000      2.544      0.018       0.000       0.002\nNORDEM               -0.0108      0.003     -3.364      0.003      -0.017      -0.004\nNUTS2                -0.0718      0.051     -1.399      0.176      -0.178       0.035\nC10001                0.0082      0.002      3.480      0.002       0.003       0.013\nC21011                4.9274      0.857      5.750      0.000       3.150       6.705\nC21031               -2.0528      0.951     -2.159      0.042      -4.024      -0.081\nC21041                0.2306      0.213      1.084      0.290      -0.211       0.672\nC21061               -1.6767      0.730     -2.298      0.031      -3.190      -0.164\nC21081                8.8434      4.456      1.985      0.060      -0.397      18.084\nC21091               -7.3059      1.433     -5.097      0.000     -10.278      -4.333\nC21101               -1.4882      0.965     -1.542      0.137      -3.490       0.514\nC21111               -1.0788      0.451     -2.394      0.026      -2.013      -0.144\nC21121               -0.1928      0.090     -2.150      0.043      -0.379      -0.007\nC21131                1.5267      1.035      1.475      0.154      -0.619       3.673\nC21141                0.2519      0.456      0.553      0.586      -0.693       1.197\nC21151               -2.7579      0.354     -7.788      0.000      -3.492      -2.023\nC21161                6.1177      5.197      1.177      0.252      -4.660      16.895\nC21171               -0.3612      0.120     -2.998      0.007      -0.611      -0.111\nC21181                0.4058      0.480      0.845      0.407      -0.590       1.402\nC21191               -0.3214      0.457     -0.703      0.490      -1.270       0.627\nC21201                0.3619      0.482      0.750      0.461      -0.638       1.362\nC21211                5.3379      3.315      1.610      0.122      -1.537      12.213\nC21221               -2.4970      1.093     -2.285      0.032      -4.763      -0.231\nC21231               -0.8779      0.215     -4.077      0.000      -1.324      -0.431\nC21241                0.5824      0.216      2.692      0.013       0.134       1.031\nC21261               -2.2529      2.856     -0.789      0.439      -8.176       3.670\nC21271                0.4035      0.320      1.259      0.221      -0.261       1.068\nC21281                0.1880      1.389      0.135      0.894      -2.692       3.068\nC21291                1.1723      0.320      3.668      0.001       0.510       1.835\nC21301               -0.0261      0.739     -0.035      0.972      -1.558       1.506\nC21311               -0.5173      0.228     -2.272      0.033      -0.989      -0.045\nC21321                0.1573      0.659      0.239      0.814      -1.209       1.524\nC21331                0.1142      0.091      1.259      0.221      -0.074       0.302\nC21341                0.6779      0.375      1.806      0.085      -0.101       1.457\nC21351               -0.4512      0.230     -1.959      0.063      -0.929       0.026\nC21361                0.8000      0.115      6.943      0.000       0.561       1.039\nC21371                0.1325      0.114      1.167      0.256      -0.103       0.368\nC21381               -0.2458      0.047     -5.195      0.000      -0.344      -0.148\nC21391                0.0751      0.588      0.128      0.899      -1.145       1.295\nC21401               -0.2867      0.209     -1.370      0.185      -0.721       0.147\nC21411               -4.0923      0.862     -4.748      0.000      -5.880      -2.305\nC21441            -1.082e-15   8.85e-16     -1.223      0.234   -2.92e-15    7.52e-16\nC22001               -0.2374      0.076     -3.129      0.005      -0.395      -0.080\nC31021                0.0500      0.102      0.490      0.629      -0.162       0.262\nC31031               -0.4693      0.082     -5.756      0.000      -0.638      -0.300\nC31041                0.0147      0.020      0.731      0.473      -0.027       0.057\nC31051                0.2098      0.209      1.006      0.326      -0.223       0.642\nC31061                0.1607      0.079      2.047      0.053      -0.002       0.324\nC31071                0.4654      0.147      3.158      0.005       0.160       0.771\n==============================================================================\nOmnibus:                        3.460   Durbin-Watson:                   2.030\nProb(Omnibus):                  0.177   Jarque-Bera (JB):                3.515\nSkew:                          -0.073   Prob(JB):                        0.172\nKurtosis:                       4.080   Cond. No.                     4.42e+19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.7e-31. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n5.3.1.1 Regressão com Scikit\n\ndf_hosp.head()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC21011\nC21031\nC21041\nC21061\nC21081\nC21091\n...\nC22001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nt_cirurgia\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n2.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n4.0\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\nyes\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nno\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nyes\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n7.0\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\nyes\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\nyes\n\n\n\n\n5 rows × 52 columns\n\n\n\n\n# seleciona para remover as colunas ano e ordem\nto_drop = ['ANO','NORDEM']\ndf = df_hosp.drop(columns=to_drop, axis=1) # drop das colunas ano e ordem\n\n\ndf1 = df.dropna() # drop das linhas com valores missing\n\n\n# define a variável target e as features\nX = df1.drop(columns=['C31011'])\ny = df1['C31011'].values\n\n\n## Typecast da coluna para categoria em pandas\nX['NUTS2'] = pd.Categorical(X.NUTS2)\n#X.dtypes\n\nX.shape\n\n(71, 49)\n\n\n\n# cria variáveis dummy e faz drop da baseline\nX = pd.get_dummies(X, drop_first = True)\n\nX.shape\n\n(71, 54)\n\n\n\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\n\nthresholder = VarianceThreshold(threshold=.2) # define o threshold de variância \nX = thresholder.fit_transform(X) # aplica o threshold para excluir variáveis com baixa variância\n\nX.shape\n\n(71, 45)\n\n\n\nfrom sklearn.model_selection import train_test_split \n\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, test_size = 0.2 ,random_state = 2002)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(56, 45)\n(15, 45)\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train) # escalar os dados em pre-processamento\n\nscaler.mean_\nprint()\nscaler.scale_\n\n\n\n\narray([8.74428584e+02, 2.52133751e+00, 1.33618689e+00, 6.04110072e+00,\n       2.07849779e+00, 9.99840549e-01, 1.31852964e+00, 2.49463711e+00,\n       3.02772057e+00, 1.94667430e+00, 2.03155904e+00, 3.17756806e+00,\n       9.61303258e+00, 2.59414569e+00, 1.79275398e+00, 2.14991398e+00,\n       6.67914436e-01, 1.98198198e+00, 3.08634253e+00, 7.65777916e-01,\n       3.85710152e+00, 2.14575697e+00, 3.19473219e+00, 1.56817846e+00,\n       4.09205302e+00, 2.73115061e+00, 5.38442458e+00, 2.92197646e+00,\n       5.15425325e+00, 1.15123603e+01, 6.52663304e+00, 8.26658429e+00,\n       9.99362041e-01, 4.31888207e+00, 2.01777813e+00, 7.27395322e+00,\n       2.81339215e+01, 1.19658848e+01, 6.17888108e+00, 4.58535824e+00,\n       1.65205114e+00, 4.30397894e+00, 2.98160816e+00, 4.42842742e-01,\n       4.79157424e-01])\n\n\n\nX_scaled = scaler.transform(X_train)\n\nX_train\n\nprint()\n\nX_scaled\n\n\n\n\narray([[ 5.24932601,  4.75937868,  4.12955491, ..., -0.48511692,\n        -0.60485838, -1.34164079],\n       [-0.4012212 , -0.39661489, -0.3608349 , ..., -0.14972744,\n        -0.60485838,  0.74535599],\n       [ 1.30961034,  1.18984467, -0.3608349 , ...,  1.52721994,\n        -0.60485838,  0.74535599],\n       ...,\n       [-0.52930484, -0.39661489, -0.3608349 , ..., -0.48511692,\n        -0.60485838,  0.74535599],\n       [-0.1702132 , -0.39661489, -0.3608349 , ...,  0.18566203,\n        -0.60485838,  0.74535599],\n       [-0.70427625, -0.39661489, -0.3608349 , ..., -0.14972744,\n         1.65327957, -1.34164079]])\n\n\n\nfrom sklearn.linear_model import LinearRegression \n\nlr = LinearRegression()\nlr.fit(X_scaled,y_train) # treina o modelo\n\nlr.coef_\nprint()\ny_pred = lr.predict(X_test) # faz a previsão mas não com os dados escalados\n\n\n\n\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n-45066.18373373444\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred)\nplt.xlim(0, 10)\nplt.ylim(0, 1000)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nfizemos o scale dos dados de train mas não dos dados de teste.\n\n\nVamos tentar de outra forma\n\n# define a variável target e os predictors\nX_2nd = df1.drop(columns=['C31011'])\ny_2nd = df1['C31011'].values\n\n\n## Typecast da coluna para categoria em pandas\nX_2nd['NUTS2'] = pd.Categorical(X_2nd.NUTS2)\n# cria variáveis dummy e faz drop da baseline\nX_2nd = pd.get_dummies(X_2nd, drop_first = True)\n\n\n#Split data for machine learning\nX_2nd_train, X_2nd_test, y_2nd_train, y_2nd_test = train_test_split(X_2nd,  y_2nd, test_size = 0.2 ,random_state = 2002)\nprint(X_2nd_train.shape)\nprint(X_2nd_test.shape)\n\n(56, 54)\n(15, 54)\n\n\n\nlr2 = LinearRegression()\nlr2.fit(X_2nd_train,y_2nd_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nlr2.coef_\n\narray([ 1.12385995e-16, -1.45716772e-15, -1.75554016e-14,  9.52016244e-15,\n       -7.43849426e-15, -1.13381526e-13,  2.98538971e-13,  9.90596494e-14,\n       -4.58522109e-14, -1.13797860e-15, -1.36890499e-13,  8.04911693e-16,\n        9.93649607e-15,  6.64468480e-14, -3.02535774e-15, -7.33657926e-15,\n       -2.55351296e-15, -3.55271368e-15,  5.83977311e-14,  4.89712437e-14,\n       -5.42621503e-15,  3.33066907e-15,  1.65534253e-13, -1.52100554e-14,\n       -1.09690035e-13,  8.93729535e-15,  6.70297151e-14, -2.44249065e-15,\n        1.72362125e-14, -3.02535774e-15,  2.22044605e-16, -4.88498131e-15,\n        1.55431223e-15, -3.94129174e-15,  1.77635684e-15, -7.53563878e-15,\n       -1.02140518e-14, -3.44724249e-14,  2.77555756e-16,  0.00000000e+00,\n        1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.39055434e-14,\n        2.06779038e-15, -8.94423424e-15,  7.41073869e-15, -5.88973315e-14,\n        6.56259769e-13,  2.09728068e-15])\n\n\n\ny_2nd_pred = lr2.predict(X_2nd_test)\n\n\n# we choose the x axis as index, chossing year will give a discrete plot\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_2nd_test, y= y_2nd_pred)\nplt.xlim(-2, 20)\nplt.ylim(-2, 20)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\n\ny_2nd_test\n\ny_2nd_pred\n\nr2_score(y_2nd_test, y_2nd_pred)\n\n1.0\n\n\ntemos de excluir C31001!!\n\n  # define a variável target e os predictors\nX_3rd = df1.drop(columns=['C31011', 'C31001'])\ny_3rd = df1['C31011'].values\n\n\n## Typecast da coluna para categoria em pandas\nX_3rd['NUTS2'] = pd.Categorical(X_3rd.NUTS2)\n# cria variáveis dummy e faz drop da baseline\nX_3rd = pd.get_dummies(X_3rd, drop_first = True)\n\n\n#Split data for machine learning\nX_3rd_train, X_3rd_test, y_3rd_train, y_3rd_test = train_test_split(X_3rd,  y_3rd, test_size = 0.2 ,random_state = 2002)\nprint(X_3rd_train.shape)\nprint(X_3rd_test.shape)\n\n(56, 53)\n(15, 53)\n\n\n\nlr3 = LinearRegression()\nlr3.fit(X_3rd_train,y_3rd_train)\nlr3.coef_\n\narray([-2.89065165e-03,  5.41320479e+00,  7.52508894e+00, -2.51991314e+00,\n       -1.38806027e+00,  3.89513026e+01, -6.38321113e+01, -1.85914497e+01,\n        1.39598080e+01,  1.13122965e-01,  2.48179866e+01,  2.76124915e+00,\n       -9.72914071e+00,  1.57918396e+01,  8.85392279e-01,  1.35181584e+00,\n        1.49772845e-01,  1.77607406e+00, -2.73519188e+01, -1.50593912e+01,\n        1.73988830e-01,  1.28398713e+00, -1.93473069e+01,  3.98023055e+00,\n        1.29523386e+01, -3.31543616e+00, -1.49145538e+01, -3.28300461e-02,\n       -2.80431048e+00,  7.84716044e-01,  1.85229891e+00,  2.71964178e+00,\n       -2.34258012e-01,  9.26484042e-01, -4.03718916e-01, -8.32495907e-01,\n        1.32607411e+00, -1.49737685e+00, -1.67066361e-12,  5.15923351e-01,\n       -8.48951953e-01, -1.25860009e-01,  1.97142581e-01, -2.58385488e-01,\n       -4.37086520e-02, -1.17080763e-01, -3.85317775e+00,  4.22138930e-01,\n        1.70055611e+00, -7.94168448e+00,  8.94329198e+00, -1.63172288e+02,\n        1.23030119e+00])\n\n\n\ny_3rd_pred = lr3.predict(X_3rd_test)\n\n\nr2_score(y_3rd_test, y_3rd_pred)\n\n-816.2729091062955\n\n\n\n\n5.3.1.2 Avaliação dos modelos de regressão\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n\n\nprint(\"                   MAE             MSE           RMSE          MedAE         R2\")\nprint(\"     1ª Tentativa {:.12f} {:.10f} {:.8f} {:.8f} {:.8f}\"\n        .format(mean_absolute_error(y_test, y_pred),\n                mean_squared_error(y_test,y_pred),\n                mean_squared_error(y_test,y_pred,squared=False), # dá a raiz quadrada do MSE\n                median_absolute_error(y_test,y_pred),\n                r2_score(y_test,y_pred)))\nprint(\"     2º Tentativa: {:.12f} {:.10f} {:.10f} {:.10f} {:.12f}\"\n        .format(mean_absolute_error(y_2nd_test, y_2nd_pred),\n                mean_squared_error(y_2nd_test,y_2nd_pred),\n                mean_squared_error(y_2nd_test,y_2nd_pred,squared=False),\n                median_absolute_error(y_2nd_test,y_2nd_pred),\n                r2_score(y_2nd_test,y_2nd_pred)))\nprint(\"     3º Tentativa: {:.11f} {:.8f} {:.8f} {:.8f} {:.8f}\"\n        .format(mean_absolute_error(y_3rd_test, y_3rd_pred),\n                mean_squared_error(y_3rd_test,y_3rd_pred),\n                mean_squared_error(y_3rd_test,y_3rd_pred,squared=False),\n                median_absolute_error(y_3rd_test,y_3rd_pred),\n                r2_score(y_3rd_test,y_3rd_pred)))\n\n                   MAE             MSE           RMSE          MedAE         R2\n     1ª Tentativa 553.426660102138 603699.9634376692 776.98131473 364.80029201 -45066.18373373\n     2º Tentativa: 0.000000000000 0.0000000000 0.0000000000 0.0000000000 1.000000000000\n     3º Tentativa: 48.28875523857 10947.82465798 104.63185298 5.86979198 -816.27290911\n\n\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n\n\n\n\n\n\n\n\nWarning\n\n\n\no valor negativo de R2 indica que o modelo é pior que um modelo constante\n\n\n\n\n5.3.1.3 Gravar o modelo\n\nimport pickle\n\n# escolher o nome do ficheiro\nfilename = \"data\\linearRegression_SK.pickle\"\n\n# gravar o modelo\npickle.dump(lr3, open(filename, \"wb\"))\n\n\n# fazer load do modelo\nloaded_model = pickle.load(open(filename, \"rb\"))\n\n\n# ve rificar que conseguimos carregar o modelo gravado\nloaded_model.coef_\n\n\n\n\n5.3.2 Classificação\nQUESTÃO:\nCom este conjunto de dados que inclui o número de médicos e enfermeiros em várias especialidades será que consigo estimar se essa unidade em particular tem serviço de cirurgia?\nColuna t_cirurgia criada a partir da coluna C21071 que deve ser retirada depois de criada a etiqueta.\n\n5.3.2.1 Classification Trees\n\nimport numpy as np\nimport pandas as pd\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\2009169450.py:4: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\nANO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012.0\n229.0\n17.0\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\nyes\n\n\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\nno\n\n\n2012.0\n65.0\n16.0\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\nyes\n\n\n2012.0\n106.0\n17.0\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\nyes\n\n\n2012.0\n209.0\n11.0\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\nyes\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ndf_hosp = df_hosp.reset_index() # \n\ndf = df_hosp.drop(columns='C21071', axis=1)  # drop da coluna C21071\n\ndf['t_cirurgia'] = df['t_cirurgia'].fillna(0) # preencher os missing values com 0\n\ndf['t_cirurgia'] = df['t_cirurgia'].replace({'yes': 1, 'no': 0}) # substituir os valores yes e no por 1 e 0\n\ndf.head()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_8160\\1416912252.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df['t_cirurgia'] = df['t_cirurgia'].replace({'yes': 1, 'no': 0}) # substituir os valores yes e no por 1 e 0\n\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\n\n\n0\n2012.0\n229.0\n17.0\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\n1\n\n\n1\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\n0\n\n\n2\n2012.0\n65.0\n16.0\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\n1\n\n\n3\n2012.0\n106.0\n17.0\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\n1\n\n\n4\n2012.0\n209.0\n11.0\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\n1\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ndf1 = df.dropna() # drop das linhas com valores missing\n\ndf1.describe()\n\n\n\n\n\n\n\n\nANO\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\n\n\ncount\n86.0\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n...\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n\n\nmean\n2012.0\n118.081395\n14.976744\n642.744186\n113.872093\n73.979651\n0.813953\n6.941860\n0.360465\n2.953488\n...\n22.941860\n3.127907\n6.220930\n4.476744\n3.465116\n0.965116\n3.465116\n1.220930\n189.348837\n0.488372\n\n\nstd\n0.0\n68.264230\n3.620129\n809.928453\n172.413511\n108.256016\n2.144825\n10.384779\n1.146994\n5.346592\n...\n26.911775\n4.752204\n10.678622\n5.740930\n8.559409\n1.482825\n4.421066\n2.783993\n240.396475\n0.502797\n\n\nmin\n2012.0\n3.000000\n11.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2012.0\n57.500000\n11.000000\n120.250000\n7.750000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n3.250000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n30.000000\n0.000000\n\n\n50%\n2012.0\n119.000000\n16.000000\n338.500000\n42.500000\n33.000000\n0.000000\n2.000000\n0.000000\n0.000000\n...\n10.000000\n0.000000\n0.000000\n3.000000\n1.000000\n0.000000\n2.000000\n0.000000\n85.500000\n0.000000\n\n\n75%\n2012.0\n175.750000\n17.000000\n872.500000\n162.500000\n115.500000\n0.000000\n10.750000\n0.000000\n3.750000\n...\n35.750000\n4.000000\n11.000000\n6.000000\n3.000000\n1.000000\n5.000000\n1.000000\n282.250000\n1.000000\n\n\nmax\n2012.0\n229.000000\n30.000000\n5325.000000\n1161.000000\n719.000000\n13.000000\n51.000000\n6.000000\n31.000000\n...\n103.000000\n21.000000\n50.000000\n28.000000\n72.000000\n6.000000\n19.000000\n15.000000\n1515.000000\n1.000000\n\n\n\n\n8 rows × 62 columns\n\n\n\n\n# conta o nº de diferentes valores na coluna\ndf1['t_cirurgia'].nunique()\n\nprint()\n# verifica se há nulos no dataframe\ndf1.isnull().any()\n\n\n\n\nANO           False\nNORDEM        False\nNUTS2         False\nC10001        False\nC20001        False\n              ...  \nC31051        False\nC31061        False\nC31071        False\nC32001        False\nt_cirurgia    False\nLength: 62, dtype: bool\n\n\n\n5.3.2.1.1 Information Gain - Entropia\n\n# função para calcular a entropia\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef compute_impurity(feature, impurity_criterion): # função para calcular a impureza de uma feature\n    \"\"\"\n    This function calculates impurity of a feature.\n    Supported impurity criteria: 'entropy', 'gini'\n    input: feature (this needs to be a Pandas series)\n    output: feature impurity\n    \"\"\"\n    probs = feature.value_counts(normalize=True)\n    \n    if impurity_criterion == 'entropy':\n        impurity = -1 * np.sum(np.log2(probs) * probs)\n    elif impurity_criterion == 'gini':\n        impurity = 1 - np.sum(np.square(probs))\n    else:\n        raise ValueError('Unknown impurity criterion')\n        \n    return(round(impurity, 3))\n\n\n# Exemplos\nprint('impurity using entropy:', compute_impurity(df1['t_cirurgia'], 'entropy'))\nprint('impurity using gini index:', compute_impurity(df1['t_cirurgia'], 'gini'))\n\nimpurity using entropy: 1.0\nimpurity using gini index: 0.5\n\n\n\nfor level in df1['NUTS2'].unique(): # loop sobre os níveis da feature NUTS2\n    print('level name:', level)\n    df_feature_level = df1[df1['NUTS2']== level]\n    print('corresponding data partition:')\n    print(df_feature_level.head(5))\n    print('partition target feature impurity:', compute_impurity(df_feature_level['t_cirurgia'], 'entropy'))\n    print('partition weight:', str(len(df_feature_level)) + '/' + str(len(df1)))\n    print('====================')\n\nlevel name: 17.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n0   2012.0   229.0   17.0  1458.0   247.0   159.0     2.0    11.0     0.0   \n1   2012.0   206.0   17.0   144.0     0.0     0.0     0.0     0.0     0.0   \n3   2012.0   106.0   17.0   801.0   166.0   108.0     0.0    12.0     0.0   \n27  2012.0   151.0   17.0  2587.0   529.0   337.0     3.0    32.0     0.0   \n33  2012.0    12.0   17.0    71.0     7.0     7.0     0.0     0.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n0      5.0  ...    39.0     8.0    19.0     2.0     2.0     3.0     5.0   \n1      0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n3      7.0  ...    36.0     4.0    16.0     5.0     2.0     2.0     7.0   \n27    14.0  ...    40.0     8.0    11.0     6.0     3.0     0.0     0.0   \n33     0.0  ...     4.0     0.0     0.0     0.0     4.0     0.0     0.0   \n\n    C31071  C32001  t_cirurgia  \n0      0.0   454.0           1  \n1      0.0    46.0           0  \n3      0.0   228.0           1  \n27    12.0   793.0           0  \n33     0.0     8.0           1  \n\n[5 rows x 62 columns]\npartition target feature impurity: 1.0\npartition weight: 20/86\n====================\nlevel name: 16.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n2   2012.0    65.0   16.0   894.0   111.0   38.25     0.0     8.0     0.0   \n5   2012.0   115.0   16.0  1461.0   246.0  171.00     2.0    20.0     0.0   \n6   2012.0   189.0   16.0   136.0    10.0    8.00     0.0     0.0     0.0   \n7   2012.0   199.0   16.0  2032.0   483.0  299.00     2.0    30.0     2.0   \n17  2012.0    88.0   16.0   190.0    17.0   15.00     0.0     2.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n2      5.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n5      6.0  ...    48.0    11.0    21.0     3.0     8.0     1.0     3.0   \n6      0.0  ...     5.0     2.0     0.0     1.0     0.0     1.0     1.0   \n7     12.0  ...   103.0    21.0    26.0    17.0    18.0     4.0    14.0   \n17     2.0  ...    10.0     2.0     0.0     1.0     1.0     1.0     5.0   \n\n    C31071  C32001  t_cirurgia  \n2      0.0   337.0           1  \n5      1.0   518.0           1  \n6      0.0    53.0           1  \n7      3.0   577.0           1  \n17     0.0    76.0           1  \n\n[5 rows x 62 columns]\npartition target feature impurity: 0.99\npartition weight: 25/86\n====================\nlevel name: 11.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n4   2012.0   209.0   11.0   221.0    13.0    13.0     0.0     0.0     0.0   \n8   2012.0    57.0   11.0   586.0    86.0    51.0     0.0     5.0     0.0   \n9   2012.0   148.0   11.0   262.0    17.0    13.0     0.0     1.0     0.0   \n10  2012.0   124.0   11.0   293.0    36.0    33.0     0.0     4.0     0.0   \n26  2012.0    14.0   11.0   392.0    53.0    33.0     0.0     0.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n4      0.0  ...     8.0     0.0     0.0     1.0     0.0     1.0     6.0   \n8      0.0  ...    53.0     7.0    12.0    14.0     7.0     4.0     7.0   \n9      0.0  ...    17.0     0.0     0.0     7.0     0.0     0.0    10.0   \n10     1.0  ...    15.0     4.0     0.0     7.0     0.0     1.0     3.0   \n26     0.0  ...    76.0     0.0     0.0     0.0    72.0     2.0     2.0   \n\n    C31071  C32001  t_cirurgia  \n4      0.0    80.0           1  \n8      2.0   184.0           1  \n9      0.0    73.0           0  \n10     0.0    91.0           1  \n26     0.0    52.0           0  \n\n[5 rows x 62 columns]\npartition target feature impurity: 1.0\npartition weight: 30/86\n====================\nlevel name: 30.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n19  2012.0   205.0   30.0   515.0    83.0    57.0     0.0     0.0     0.0   \n20  2012.0   131.0   30.0   169.0     1.0     1.0     0.0     0.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n19     0.0  ...    10.0     0.0     0.0     3.0     2.0     0.0     5.0   \n20     0.0  ...     3.0     0.0     0.0     1.0     1.0     0.0     1.0   \n\n    C31071  C32001  t_cirurgia  \n19     0.0   187.0           1  \n20     0.0    48.0           0  \n\n[2 rows x 62 columns]\npartition target feature impurity: 1.0\npartition weight: 2/86\n====================\nlevel name: 20.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n21  2012.0    41.0   20.0   488.0    46.0    30.0     0.0     3.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n21     2.0  ...    19.0     1.0     8.0     7.0     0.0     0.0     1.0   \n\n    C31071  C32001  t_cirurgia  \n21     2.0   107.0           1  \n\n[1 rows x 62 columns]\npartition target feature impurity: -0.0\npartition weight: 1/86\n====================\nlevel name: 18.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n22  2012.0   130.0   18.0   415.0    43.0    34.0     0.0     3.0     0.0   \n40  2012.0   105.0   18.0  1359.0   251.0   146.0     2.0    14.0     1.0   \n62  2012.0   223.0   18.0  1454.0   273.0   154.0     3.0    11.0     0.0   \n65  2012.0   224.0   18.0   800.0    87.0    46.0     0.0     2.0     0.0   \n66  2012.0   190.0   18.0   384.0    35.0    30.0     0.0     3.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n22     0.0  ...    12.0     0.0     1.0     5.0     0.0     0.0     0.0   \n40     8.0  ...    75.0    14.0    17.0    13.0     9.0     4.0    10.0   \n62    10.0  ...    72.0    12.0    18.0    13.0    14.0     2.0    12.0   \n65     0.0  ...    36.0     4.0    10.0     3.0    10.0     6.0     3.0   \n66     0.0  ...    14.0     1.0     2.0     4.0     1.0     3.0     2.0   \n\n    C31071  C32001  t_cirurgia  \n22     6.0   153.0           0  \n40     8.0   382.0           0  \n62     1.0   411.0           0  \n65     0.0   235.0           1  \n66     1.0   110.0           1  \n\n[5 rows x 62 columns]\npartition target feature impurity: 0.918\npartition weight: 6/86\n====================\nlevel name: 15.0\ncorresponding data partition:\n       ANO  NORDEM  NUTS2  C10001  C20001  C21001  C21011  C21021  C21031  \\\n47  2012.0   208.0   15.0   183.0     5.0     5.0     0.0     0.0     0.0   \n48  2012.0   164.0   15.0  1391.0   172.0   118.0     1.0    11.0     0.0   \n\n    C21041  ...  C31001  C31011  C31021  C31031  C31041  C31051  C31061  \\\n47     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n48     2.0  ...    34.0     9.0    13.0     6.0     4.0     0.0     0.0   \n\n    C31071  C32001  t_cirurgia  \n47     0.0    71.0           1  \n48     2.0   492.0           1  \n\n[2 rows x 62 columns]\npartition target feature impurity: -0.0\npartition weight: 2/86\n====================\n\n\n\n# função para calcular o information gain\ndef comp_feature_information_gain(df, target, descriptive_feature, split_criterion): \n    \"\"\"\n    This function calculates information gain for splitting on \n    a particular descriptive feature for a given dataset\n    and a given impurity criteria.\n    Supported split criterion: 'entropy', 'gini'\n    \"\"\"\n    \n    print('target feature:', target)\n    print('descriptive_feature:', descriptive_feature)\n    print('split criterion:', split_criterion)\n            \n    target_entropy = compute_impurity(df[target], split_criterion)\n    print('the target entropy is', target_entropy)\n\n    # we define two lists below:\n    # entropy_list to store the entropy of each partition\n    # weight_list to store the relative number of observations in each partition\n    entropy_list = list()\n    weight_list = list()\n    \n    # loop over each level of the descriptive feature\n    # to partition the dataset with respect to that level\n    # and compute the entropy and the weight of the level's partition\n    for level in df[descriptive_feature].unique():\n        df_feature_level = df[df[descriptive_feature] == level]\n        entropy_level = compute_impurity(df_feature_level[target], split_criterion)\n        entropy_list.append(round(entropy_level, 3))\n        weight_level = len(df_feature_level) / len(df)\n        print('the level is {} and the weight is {}'.format(level, weight_level))\n        weight_list.append(round(weight_level, 3))\n\n    print('impurity of partitions:', entropy_list)\n    print('weights of partitions:', weight_list)\n\n    feature_remaining_impurity = np.sum(np.array(entropy_list) * np.array(weight_list))\n    print('remaining impurity:', feature_remaining_impurity)\n    \n    information_gain = target_entropy - feature_remaining_impurity\n    print('information gain:', information_gain)\n    \n    print('====================')\n\n    return(information_gain)\n\nVamos ver que a variável do Número de Pessoal ao Serviço - Total C10001 é bastante promissora para faser a divisão do espaço, e a variável do Número de Médicos - Total C20001 é bastante melhor do que o Número de Médicos - Especialistas - Anatomia Patológica - Total C21011\n\nsplit_criterion = 'entropy' # escolher o critério de divisão\nfor feature in df1.drop(columns=['t_cirurgia','ANO','NORDEM']).columns:\n    feature_info_gain = comp_feature_information_gain(df1, 't_cirurgia', feature, split_criterion) # calcular o information gain\n\n\n\n5.3.2.1.2 trees\n\n# define a variável target e os predictors\nX = df1.drop(columns=['t_cirurgia']) # dataframe sem a coluna target\ny = df1['t_cirurgia'].values # target em formato de array\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nclf = DecisionTreeClassifier(max_depth=3) # define a profundidade da árvore\n\nmodel = clf.fit(X,y) # treina o modelo\n\n\n#from sklearn import tree\n\ntext_representation = tree.export_text(clf)\nprint(text_representation) # visualização do modelo\n\n|--- feature_58 &lt;= 1.50\n|   |--- feature_48 &lt;= 0.50\n|   |   |--- feature_20 &lt;= 1.00\n|   |   |   |--- class: 0\n|   |   |--- feature_20 &gt;  1.00\n|   |   |   |--- class: 1\n|   |--- feature_48 &gt;  0.50\n|   |   |--- feature_1 &lt;= 179.50\n|   |   |   |--- class: 0\n|   |   |--- feature_1 &gt;  179.50\n|   |   |   |--- class: 1\n|--- feature_58 &gt;  1.50\n|   |--- feature_60 &lt;= 73.50\n|   |   |--- feature_55 &lt;= 2.50\n|   |   |   |--- class: 1\n|   |   |--- feature_55 &gt;  2.50\n|   |   |   |--- class: 0\n|   |--- feature_60 &gt;  73.50\n|   |   |--- feature_58 &lt;= 9.50\n|   |   |   |--- class: 1\n|   |   |--- feature_58 &gt;  9.50\n|   |   |   |--- class: 0\n\n\n\n\n# gravar o log da Árvore\nwith open(\"data\\decision_tree.log\", \"w\") as fout: # \n    fout.write(text_representation)\n\n\nimport matplotlib.pyplot as plt\n\n# class_names = True em vez da lista  faz print y(0) e y(1)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(clf, class_names= [\"No\",\"Yes\"], filled=True)\n\n\n\n\n\n# gravar a imagem da tree\n\nfig.savefig(\"images/decision_tree.png\")\n\n\n\n\n5.3.2.2 Classification com Scikit\n\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\nfrom sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder # For scaling variables\n\nfrom sklearn.model_selection import train_test_split,cross_val_score # For splitting data\nfrom sklearn.tree import DecisionTreeClassifier # For Decision Tree\n\n# For model evaluation\nfrom sklearn.metrics import accuracy_score,confusion_matrix \nfrom sklearn.metrics import roc_curve,roc_auc_score \n\n\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, \n                                                    test_size = 0.2 ,\n                                                    random_state = 1984)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(68, 61)\n(18, 61)\n\n\n\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.preprocessing import StandardScaler #for scaling variables\nfrom sklearn.pipeline import Pipeline # For setting up pipeline\n\n#Define a pipeline\npipeline = Pipeline([\n('scaler', StandardScaler()),\n('selector', VarianceThreshold()),\n('TREE', DecisionTreeClassifier())])\n\n\n5.3.2.2.1 usando a Pipeline\n\nfrom sklearn import set_config # para configurar a visualização da pipeline\n\nset_config(display=\"diagram\")\npipeline\n\nPipeline(steps=[('scaler', StandardScaler()), ('selector', VarianceThreshold()),\n                ('TREE', DecisionTreeClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('scaler', StandardScaler()), ('selector', VarianceThreshold()),\n                ('TREE', DecisionTreeClassifier())])  StandardScaler?Documentation for StandardScalerStandardScaler()  VarianceThreshold?Documentation for VarianceThresholdVarianceThreshold()  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n# execução da pipeline com parameteros de omissão\npipeline.fit(X_train, y_train)\nprint('Training set score: ' + str(pipeline.score(X_train,y_train)))\nprint('Test set score: ' + str(pipeline.score(X_test,y_test)))\n\nprint()\ny_pred = pipeline.predict(X_test)\ny_pred\n\nprint()\ny_proba = pipeline.predict_proba(X_test)\ny_proba\n\nTraining set score: 1.0\nTest set score: 0.5\n\n\n\n\narray([[0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.]])\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n           pred: No  pred: Yes\nreal: No          5          5\nreal: Yes         4          4\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\nprint('Precision: %.3f' % precision_score(y_test, y_pred))\nprint('Recall: %.3f' % recall_score(y_test, y_pred))\nprint('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n\nPrecision: 0.444\nRecall: 0.500\nAccuracy: 0.500\n\n\n\n\nfrom sklearn.metrics import roc_curve, balanced_accuracy_score\n# antes era plot_roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,\n                                       pipeline.predict_proba(X_test)[:,1],)\n\nRocCurveDisplay.from_estimator(pipeline, X_test, y_test)\n\nplt.show()\n\n\n\n\n\n\n5.3.2.2.2 sem Pipeline\n\nclf2 = DecisionTreeClassifier(max_depth=2) \n\nmodel2 = clf2.fit(X_train, y_train)\n\ny_class = model2.predict(X_test)\ny_class\n\nprint()\ny_class_proba = model2.predict_proba(X_test)\ny_class_proba\n\nprint()\nmodel.predict_proba(X_test)[:, 1]\n\ntree.plot_tree(clf2, class_names= [\"No\",\"Yes\"], filled=True)\n\n\n\n\n\n[Text(0.5, 0.8333333333333334, 'x[57] &lt;= 0.5\\ngini = 0.5\\nsamples = 68\\nvalue = [34, 34]\\nclass = No'),\n Text(0.25, 0.5, 'x[49] &lt;= 0.5\\ngini = 0.444\\nsamples = 36\\nvalue = [24, 12]\\nclass = No'),\n Text(0.375, 0.6666666666666667, 'True  '),\n Text(0.125, 0.16666666666666666, 'gini = 0.278\\nsamples = 18\\nvalue = [15, 3]\\nclass = No'),\n Text(0.375, 0.16666666666666666, 'gini = 0.5\\nsamples = 18\\nvalue = [9, 9]\\nclass = No'),\n Text(0.75, 0.5, 'x[1] &lt;= 15.5\\ngini = 0.43\\nsamples = 32\\nvalue = [10, 22]\\nclass = Yes'),\n Text(0.625, 0.6666666666666667, '  False'),\n Text(0.625, 0.16666666666666666, 'gini = 0.0\\nsamples = 4\\nvalue = [4, 0]\\nclass = No'),\n Text(0.875, 0.16666666666666666, 'gini = 0.337\\nsamples = 28\\nvalue = [6, 22]\\nclass = Yes')]\n\n\n\n\n\npodemos mudar o threshold\n\nthreshold = 0.7\ny_pred = (model2.predict_proba(X_test)[:, 1] &gt; threshold)\nconfusion_matrix(y_test, y_pred)\n\nprint()\ncm_thr70 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr70,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n\n           pred: No  pred: Yes\nreal: No          7          3\nreal: Yes         5          3\n\n\n\nfrom sklearn.metrics import roc_curve,RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,model2.predict_proba(X_test)[:,1],\n                                 drop_intermediate=False) # temos de fazer o unpack dos 3 resultados\n\nRocCurveDisplay.from_estimator(model,X_test,y_test)\n\nplt.show()\n\n\n\n\n\nprint(fpr)\nprint(tpr)\nprint(thresholds)\n\n[0.  0.3 0.6 1. ]\n[0.    0.375 0.75  1.   ]\n[       inf 0.78571429 0.5        0.16666667]\n\n\n\n\n5.3.2.2.3 gravar o modelo\n\n# escolher o nome do ficheiro\nfilename = \"data\\Tree.pickle\"\n\n# gravar o modelo\npickle.dump(model2, open(filename, \"wb\"))"
  },
  {
    "objectID": "600-mod6.html#dados-geográficos",
    "href": "600-mod6.html#dados-geográficos",
    "title": "6  Visualização de Dados Geográficos",
    "section": "6.1 Dados Geográficos",
    "text": "6.1 Dados Geográficos\nSistema de Informação Geográfica (SIG) é um sistema que permite manipular, armazenar, e fazer a análise espacial de dados geográficos.\n\n6.1.1 Tipo de dados\npontos - Point(2,10) linha - LineString([(1,2),(1,5),…]) polígonos - Polygon([(13,1),(14,4),…])\n Preparação do ambiente python:\natravés do prompt ANACONDA, instalar os packages: GEOPANDAS FOLIUM NBB\n### Coordenadas Geográficas\nLatitude e Longitude\nConhecer a Projeção Geográfica é importante para combinar dados de diferentes fontes de informação.\nCoordinate Reference System (CRS): define como as coordenadas são representadas no plano.\nEPSG Geodetic Parameter Dataset: registo global dos diferentes CRS.\nEPSG: European Petroleum Survey Group para identificar os diferentes sistemas de coordenadas.\nWGS84 - World Geodetic System 1984 associado ao sistema de posicionamento global (GPS).\n\nEPSG 4326\ncoordenadaas em graus decimais\n\nPseudo-Mercator/WGS84\n\nEPSG 3857\ncoordenadas em metros\n\n\n6.1.1.1 Exercicio\nMostrar Influencia do tipo de Projeção Geográfica\n\n# Este Script Permite Visualizar diferentes Projeções\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Definir Figura do MATPLOTLIB\n# Conseguem mostrar mais SUBPLOTS alterando argumeto 2 (1,2)\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n# Carregar a geogrfia do mundo a partir d eum shapefile dentro um ficheiro ZIP\n# Atenção Mudar o caminho!\nworld = gpd.read_file(r\"data\\ne_110m_admin_0_countries.zip\")\n\n# # Alternativa - Ler os Dados de um Link GEOJSON:\n# url = \"http://d2ad6b4ur7yvpq.cloudfront.net/naturalearth-3.3.0/ne_110m_land.geojson\"\n# # Ler Ficheiro GEOJSON do URL\n# world = gpd.read_file(url)\n\n# Sistema de coordenadas definidos na listagem em baixo:\n# \n# WGS 84 (Lat/Lon): ['EPSG:4326','WGS 84 (Lat/Lon)']\n# Web Mercator Metros: ['EPSG:3857','Web Mercator']\n# Vander Grinten: ['ESRI:54029','World Robinson']\n# Robinson: ['ESRI:54030','World Robinson - National Geographic']\n# Gall-Peters - ['ESRI:53016','Gall Stereographic']\n# Peirce Quincuncial: ['ESRI:54091','Peirce quincuncial North Pole in a square']\n# winkel-Tripel: ['ESRI:53018','Sphere Winkel I']\n# Goode Homolosine (Molweide) - ['EPSG:7619','Interrupted Goode Homolonsine']\n# Albers Equal Area: ['ESRI:102008','North America Albers Equal Area Conic']\n# Eckert: ['ESRI:53010','Sphere Eckert VI']\n# Portugal Continental: ['EPSG:3763','Portugal Continental: PT-TM06/ETRS89']\n# Açores UTM Fuso 26: ['EPSG:5015','TRF93 / PTRA08 - UTM fuso 26 - Grupo Central e Oriental do Arquipélago dos Açores']\n\n# sistema de Coordenadas para Visualizar - 12 exemplos:\nsistcoord = [['EPSG:4326','WGS 84 (Lat/Lon)'], ['EPSG:3857','Web Mercator'],\n             ['ESRI:54029','World Robinson'], ['ESRI:54030','World Robinson - National Geographic'], \n             ['ESRI:53016','Gall Stereographic'], ['ESRI:54091','Peirce quincuncial North Pole in a square'],\n             ['ESRI:53018','Sphere Winkel I'],['EPSG:7619','Interrupted Goode Homolonsine'],\n             ['ESRI:102008','North America Albers Equal Area Conic'], ['ESRI:53010','Sphere Eckert VI'],\n             ['EPSG:3763','Portugal Continental: PT-TM06/ETRS89'], ['EPSG:5015','TRF93 / PTRA08 - UTM fuso 26 - Grupo Central e Oriental do Arquipélago dos Açores']]\n\n\n# Para mostrar\nmapa1 = 0\nmapa2 = 11\n\n# ------------------------------------------\n# Mudar Projeçoes na indicação da Listagem\n#world.to_crs(epsg=2264).plot(ax=ax[0])\n\nworld.to_crs(sistcoord[mapa1][0]).plot(ax=ax[0])\nax[0].set_title(sistcoord[mapa1][1])\n\n#world.to_crs(epsg=4087).plot(ax=ax[1])\nworld.to_crs(sistcoord[mapa2][0]).plot(ax=ax[1])\nax[1].set_title(sistcoord[mapa2][1])\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n6.1.2 GeoPandas e MatPlotLib\nAs mesmas funcionalidades do package Pandas podem ser usadas com o GeoPandas, usa também outros packages como Shapely e Fiona\n\n\n\n6.1.3 Criar dados GeoPandas\n\n  # Importar GeoPandas\nimport geopandas as gpd\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\NUTS3_2015_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_nuts3 = gpd.read_file(file_path, encoding=encoding)\n\nprint(gdf_nuts3.info())\nprint(gdf_nuts3.head())\ngdf_nuts3.loc[1,'geometry']\n\n#gdf_nuts3.head()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 25 entries, 0 to 24\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       25 non-null     object  \n 1   NUTS2       25 non-null     object  \n 2   NUTS3       25 non-null     object  \n 3   NUTS3_DSG   25 non-null     object  \n 4   SUM_AREA_K  25 non-null     float64 \n 5   Shape_Leng  25 non-null     float64 \n 6   Shape_Area  25 non-null     float64 \n 7   geometry    25 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.7+ KB\nNone\n  NUTS1 NUTS2 NUTS3                    NUTS3_DSG   SUM_AREA_K     Shape_Leng  \\\n0     1    11   111                   Alto Minho  4005.236217  348704.286332   \n1     1    11   112                       Cávado  2230.779298  308604.284583   \n2     1    11   119                          Ave  2589.276604  418041.964330   \n3     1    11   11A  Área Metropolitana do Porto  3596.793275  523879.094897   \n4     1    11   11B                  Alto Tâmega  5240.318314  473818.956252   \n\n     Shape_Area                                           geometry  \n0  4.006183e+09  MULTIPOLYGON (((-911484.585 5182506.803, -9118...  \n1  2.231519e+09  POLYGON ((-895962.526 5133087.503, -895581.858...  \n2  2.589188e+09  POLYGON ((-893252.407 5115827.796, -892566.222...  \n3  3.597098e+09  POLYGON ((-974333.659 5081342.266, -973571.650...  \n4  5.239871e+09  POLYGON ((-880130.085 5150064.989, -878083.752...  \n\n\n\n\n\nMostrar Objecto Geometry\n\n# Informação de um Polygon\n# print(gdf_nuts3.loc[1,'geometry'])\n\nObjecto de Geometria\n\n# Tipo Objecto GeoPandas\nprint (type(gdf_nuts3))\n# Tipo de Dados da coluna de Geometria\nprint (type(gdf_nuts3.geometry))\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;\n\n\nVisualizar GDF\n\n# Mostrar Geografia\n#import geopandas as gpd\nimport matplotlib.pyplot as plt\n\ngdf_nuts3.plot(column = 'NUTS3_DSG',\n              legend = False)\nplt.show()\n\n\n\n\nSelecionar registos dum GDF\n\n# Selecção utilizando os operadores comuns\n# Selecção pode ser efetuada com ou sem a função LOC\ngdf_nuts3_continente = gdf_nuts3[gdf_nuts3['NUTS3'] &lt; '2']\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '11']\n\n# Mostrar numero registos selecionados\nprint(\"Nº registos Continente\", len(gdf_nuts3_continente))\nprint(\"Nº registos NUTS2 11\", len(gdf_nuts3_sel))\n\n# Selecionar utilizando comparação de um String\nselected_rows_like = gdf_nuts3[gdf_nuts3['NUTS3'].str.startswith('11')]\nprint('Nº registos que começam com 11',len(selected_rows_like))\n                        \n# Selecionar Linhas quando existem numa Listagem\nlista_selecao = ['11A', '11E','16E','170']\nselected_rows = gdf_nuts3[gdf_nuts3.NUTS3.isin(lista_selecao)] \nprint('Nº registos que estão numa lista - vs 1',len(selected_rows))\nselected_rows = gdf_nuts3.loc[gdf_nuts3.NUTS3.isin(lista_selecao)] \nprint('Nº registos que estão numa lista - vs 2',len(selected_rows))\n\n# Selecao dos Dados a Mostrar\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '11']\ngdf_nuts3_sel.plot(column = 'NUTS3_DSG',\n              legend = True,\n                  edgecolor = 'black')\n\nNº registos Continente 23\nNº registos NUTS2 11 8\nNº registos que começam com 11 8\nNº registos que estão numa lista - vs 1 4\nNº registos que estão numa lista - vs 2 4\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nVisualização Interactiva com Explore\n\n#import geopandas as gpd\n\n# Selecao dos Dados a Mostrar\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '11']\ngdf_nuts3_sel.explore(column = 'NUTS3_DSG',\n              legend = True,\n                  edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nMostrar geografia\n\n# Mostrar Geografia\n\n# Mostrar Áreas NUTS3 Região Centro\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS2 == '16']\n\n# Definir Legenda \n# Definir Localização (loc), Localização em relação ao eixos (bbox_to_anchor), Nº colunas (ncol)\nlgnd_kwds = {'title': 'Nuts3 (Centro)',\n               'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 1}\n# Argumetnos: Atributo (column), Colormap (cmap), colocar Legenda (legend), \n#  Legenda definida (legend_kwds), \ngdf_nuts3_sel.plot(column = 'NUTS3_DSG',\n              cmap = 'tab20',\n              legend = True,\n              legend_kwds  = lgnd_kwds,\n              edgecolor = 'dimgray')\n\n# Rotulos  Eixos\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Áreas NUTS3 de Portugal Continental')\n\nplt.show()\n\n\n\n\n\n6.1.3.0.1 Exercício\n\n# packages necessários\nimport geopandas as gpd\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\CAOP20200_MN_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_caop = gpd.read_file(file_path, encoding=encoding)\n\nprint(gdf_caop.info())\nprint(gdf_caop.head())\nprint(gdf_caop.DISTRITO.unique()) \n\ngdf_caop.loc[1,'geometry']\n\nprint()\nprint(gdf_caop.NUTS3_02.unique()) # lista com a opções de NUTS3_02\n\n# cria nova coluna para legenda\ngdf_caop['DTMN2'] = gdf_caop['DTMNDSG'] + '(' + gdf_caop['DTMN'].astype(str) + ')'\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 308 entries, 0 to 307\nData columns (total 12 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   OBJECTID    308 non-null    int64   \n 1   DTMN        308 non-null    object  \n 2   DTMNDSG     308 non-null    object  \n 3   ILHA        30 non-null     object  \n 4   NUTS3_02    308 non-null    object  \n 5   NUTS3_15    308 non-null    object  \n 6   NUTS3_24    308 non-null    object  \n 7   OBJECTID_2  308 non-null    int64   \n 8   NUTS1_15    278 non-null    object  \n 9   NUTS2_15    278 non-null    object  \n 10  DISTRITO    278 non-null    object  \n 11  geometry    308 non-null    geometry\ndtypes: geometry(1), int64(2), object(9)\nmemory usage: 29.0+ KB\nNone\n   OBJECTID  DTMN                DTMNDSG                ILHA NUTS3_02  \\\n0         1  4901                  Corvo       Ilha do Corvo      200   \n1         2  4801       Lajes das Flores     Ilha das Flores      200   \n2         3  4802  Santa Cruz das Flores     Ilha das Flores      200   \n3         1  4201                  Lagoa  Ilha de São Miguel      200   \n4         2  4202               Nordeste  Ilha de São Miguel      200   \n\n  NUTS3_15 NUTS3_24  OBJECTID_2 NUTS1_15 NUTS2_15 DISTRITO  \\\n0    PT200      200           0     None     None     None   \n1    PT200      200           0     None     None     None   \n2    PT200      200           0     None     None     None   \n3      200      200           0     None     None     None   \n4      200      200           0     None     None     None   \n\n                                            geometry  \n0  POLYGON ((-3463610.566 4817991.232, -3463602.6...  \n1  POLYGON ((-3479397.272 4791980.698, -3479270.4...  \n2  POLYGON ((-3474231.695 4797050.671, -3474205.1...  \n3  POLYGON ((-2845070.611 4547832.086, -2845103.7...  \n4  POLYGON ((-2812174.924 4560067.448, -2812097.9...  \n[None 'Évora' 'Lisboa' 'Portalegre' 'Porto' 'Santarém' 'Castelo Branco'\n 'Faro' 'Viana do Castelo' 'Viseu' 'Coimbra' 'Setúbal' 'Vila Real'\n 'Aveiro' 'Braga' 'Guarda' 'Bragança' 'Beja' 'Leiria']\n\n['200' '183' '182' '16B' '115' '114' '185' '166' '150' '111' '165' '162'\n '16C' '181' '117' '118' '161' '116' '113' '164' '168' '167' '171' '172'\n '16A' '169' '184' '163' '112' '300']\n\n\nvizualisar com explore\n\ngdf_caop.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\ncontagens e filtros\n\n# conta o número de registos por NUTS3_02\nprint(gdf_caop.NUTS3_24.value_counts())\n\n# selecionar linhas a partir de uma lista\nlista_selecao = ['11A','11E','16E','170']\n\nselect_rows = gdf_caop.loc[gdf_caop.NUTS3_24.isin(lista_selecao)]\n\nprint('Nº de registos que estão na lista ', lista_selecao, len(select_rows))\n\nNUTS3_24\n200    19\n11D    19\n192    19\n11A    17\n150    16\n111    16\n1C3    15\n196    15\n194    14\n1C4    14\n1C2    13\n1D1    12\n191    11\n1D2    11\n1D3    11\n11C    11\n300    11\n193    10\n11E     9\n1A0     9\n1B0     9\n195     8\n119     8\n11B     6\n1C1     5\nName: count, dtype: int64\nNº de registos que estão na lista  ['11A', '11E', '16E', '170'] 26\n\n\nFazer a Seleção de um DISTRITO e mostrar os munícipios\n\nimport matplotlib.pyplot as plt\n\n# Selecao dos Dados a Mostrar\ngdf_caop_sel = gdf_caop.loc[gdf_caop.DISTRITO == 'Guarda'] \n\n# Definir Legenda \n# Definir Localização (loc), Localização em relação ao eixos (bbox_to_anchor), Nº colunas (ncol)\nlgnd_kwds = {'title': 'Municipios',\n              'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 1}\n             \ngdf_caop_sel.plot(column = 'DTMN2',\n              legend = True,\n              legend_kwds  = lgnd_kwds,\n              edgecolor = 'black')\n\n# Rotulos  Eixos\nplt.xlabel('X Coordinados')\nplt.ylabel('Y Coordinados')\nplt.title('Municipios do Distrito da Guarda')\n# Rodar xticks; ha é o alinhamento\nplt.xticks(rotation=45, ha='right')\n              \n\n\n# **Codigo Alternativo Utilizando ax objecto**\n# fig, ax = plt.subplots()\n# gdf_caop_sel.plot(ax=ax,\n#               column = 'DTMN2',\n#               cmap = 'tab20',\n#               legend = True,\n#               legend_kwds  = lgnd_kwds,\n#               edgecolor = 'dimgray')\n# \n# # Rotulos  Eixos - Diferentes Funções\n# ax.set_xlabel('X Coordinados')\n# ax.set_ylabel('Y Coordinados')\n \nplt.show()\n\n\n\n\nusando o explore\n\ngdf_caop_sel.explore(column = 'DISTRITO',\n              legend = True,\n                  edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n6.1.4 Importar dados de tabelas\n\n6.1.4.1 ACES\n\n# Importar dados DGS Utentes em Centros de Saude\n# Ver referencia:  https://dados.gov.pt/pt/datasets/evolucao-mensal-de-utentes-atendidos-nos-centros-de-saude-agregado-por-aces-no-ambito-da-saude-oral-nos-cuidados-de-saude-primarios-socsp/#resources\n# Atributos: Período, ARS, ACES, Localização Geográfica, Sexo, Faixa Etária, Nº Utentes,ID \nimport pandas as pd\n\n# Link DGS (Utentes Centro Saude)  \nficheiro = r'http://dados.gov.pt/pt/datasets/r/dc54ea6f-31f3-483b-a719-718d0d7451f3'\n# Ler ficheiro do computador:\n#ficheiro = r'C:\\TEMp\\utentes-atendidos-nos-centros-de-saude-no-ambito-da-soep.csv'\n\n# Importar em DataFrame\nencoding = 'utf-8'\ndf_utentes = pd.read_csv(ficheiro, sep=';', encoding=encoding)\n\n\n# Mostrar informação df\nprint(df_utentes.head(5))\nprint(df_utentes.info())\nprint(df_utentes.describe())\n\n   Período    ARS                           ACES Localização Geográfica  \\\n0  2019-07  Norte                   ULS Nordeste  41.8069684,-6.7587977   \n1  2019-07  Norte  Douro I - Marão e Douro Norte  41.2968711,-7.7483727   \n2  2019-07  Norte  Douro I - Marão e Douro Norte  41.2968711,-7.7483727   \n3  2019-07  Norte           Douro II - Douro Sul  41.0953745,-7.8123805   \n4  2019-07  Norte           Douro II - Douro Sul  41.0953745,-7.8123805   \n\n        Sexo Faixa Etária  Nº Utentes  \\\n0   Feminino        20-34          30   \n1  Masculino        35-49          23   \n2   Feminino        50-64          70   \n3  Masculino        20-34          41   \n4  Masculino        35-49          80   \n\n                                                  ID  \n0                 2019-7/20-34/Feminino/ULS Nordeste  \n1  2019-7/35-49/Masculino/Douro I - Marão e Douro...  \n2  2019-7/50-64/Feminino/Douro I - Marão e Douro ...  \n3        2019-7/20-34/Masculino/Douro II - Douro Sul  \n4        2019-7/35-49/Masculino/Douro II - Douro Sul  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30692 entries, 0 to 30691\nData columns (total 8 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   Período                 30692 non-null  object\n 1   ARS                     30692 non-null  object\n 2   ACES                    30692 non-null  object\n 3   Localização Geográfica  30096 non-null  object\n 4   Sexo                    30692 non-null  object\n 5   Faixa Etária            30692 non-null  object\n 6   Nº Utentes              30692 non-null  int64 \n 7   ID                      30692 non-null  object\ndtypes: int64(1), object(7)\nmemory usage: 1.9+ MB\nNone\n         Nº Utentes\ncount  30692.000000\nmean      59.380457\nstd       71.069367\nmin        1.000000\n25%       15.000000\n50%       36.000000\n75%       78.000000\nmax      851.000000\n\n\npassar para o GDF\n\n# Verificar os Dados\n# Total numero de Pontos Unicos para Ficheiro:\nprint('Num localizações Geográficos:',len(df_utentes['Localização Geográfica'].unique()))\n\n# Verificar valores únicos de outras variáveis\nprint('Num ACES:',len(df_utentes['ACES'].unique()))\nprint('Num ARS:',len(df_utentes['ARS'].unique()))\nprint(df_utentes['Faixa Etária'].unique())\nprint(df_utentes['Sexo'].unique())\n\nNum localizações Geográficos: 51\nNum ACES: 79\nNum ARS: 35\n['20-34' '35-49' '50-64' '&lt;20' '65 e +']\n['Feminino' 'Masculino']\n\n\n\n# DataFrame com dados Utentes: df_utentes\n\n# Criar Colunas lat e Long a partir da coluna 'Localização Geográfica'\n# no ficheiro original a informação existe em apenas uma coluna\nprint (\"Tipo de atributo\", type(df_utentes['Localização Geográfica']))\n# Fazer um Split dos Valores utilizando virgula como separador\ndf_utentes[['lat', 'long']] = df_utentes['Localização Geográfica'].str.split(',', expand=True)\n\n# Mostrar informação df\nprint(df_utentes.info())\n\nTipo de atributo &lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30692 entries, 0 to 30691\nData columns (total 10 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   Período                 30692 non-null  object\n 1   ARS                     30692 non-null  object\n 2   ACES                    30692 non-null  object\n 3   Localização Geográfica  30096 non-null  object\n 4   Sexo                    30692 non-null  object\n 5   Faixa Etária            30692 non-null  object\n 6   Nº Utentes              30692 non-null  int64 \n 7   ID                      30692 non-null  object\n 8   lat                     30096 non-null  object\n 9   long                    30096 non-null  object\ndtypes: int64(1), object(9)\nmemory usage: 2.3+ MB\nNone\n\n\n\n# Criar DataFrame com Pontos Unicos e Numero de Utentos\n\n# Total numero de Pontos Unicos para Ficheiro:\nprint('Num loc:',len(df_utentes['Localização Geográfica'].unique()))\n\n# Converter variaveis numericos para numeric\ndf_utentes['Nº Utentes'] = pd.to_numeric(df_utentes['Nº Utentes'], errors='coerce')\ndf_utentes['lat'] = pd.to_numeric(df_utentes['lat'], errors='coerce')\ndf_utentes['long'] = pd.to_numeric(df_utentes['long'], errors='coerce')\n\n# Criar Novo DataFrame com ACES e Soma Nº Utentes\n# Group by 'ARS', 'ACES', 'Localização Geográfica'\n# O nº de utentes é um exemplo de um atributo que podemos utilizar para visualização\ndf_aces = df_utentes.groupby(['ARS', 'ACES', 'Localização Geográfica','lat', 'long']).agg({\n    'Nº Utentes': 'sum'\n}).reset_index()\n\nprint(df_aces.head())\n\nNum loc: 51\n        ARS                     ACES          Localização Geográfica  \\\n0  Alentejo         Alentejo Central           38.8442031,-7.5826619   \n1   Algarve      Algarve I - Central  37.0274264,-7.9395983999999995   \n2   Algarve      Algarve I - Central           37.0274264,-7.9395984   \n3   Algarve  Algarve II - Barlavento           37.1387554,-8.5445093   \n4   Algarve   AlgarveIII - Sotavento            37.383008,-7.7293275   \n\n         lat      long  Nº Utentes  \n0  38.844203 -7.582662       45349  \n1  37.027426 -7.939598        5051  \n2  37.027426 -7.939598       37627  \n3  37.138755 -8.544509       35960  \n4  37.383008 -7.729328       23136  \n\n\n\n# Mesmo com Pandas DataFrame já é possível mostrar a geografia\n# Importar Bibliotecas\n#import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Mostrar a localização\nplt.scatter(df_aces.long, df_aces.lat)\n \n# Show the plot\nplt.show()\n\n\n\n\n\n# Criar GeoDataFrame \n#import pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Criar a Coluna de geometry no DF a partir informação atributos long e lat \n# Utiliza-se o Point Object do modulo shapely\n# lambda - função anônima no Python \n# df_aces.apply - applica a função a cada linha do DF\n# Argumento apply: axis = 1. Função deve ocorrer para todos as linhas\ndf_aces['geometry'] = df_aces.apply(lambda x: Point(float(x.long), float(x.lat)), axis=1)\n \n# Create a GeoDataFrame from art and verify the type\ngdf_aces = gpd.GeoDataFrame(df_aces, crs = \"EPSG:4326\", geometry = df_aces.geometry)\n\nprint(type(gdf_aces))\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\n\n\nimportar dados poligonos\n\n# Shapefile NUTS3: NUTS3_2015_PT.shp (dados estão em WebMercator)\n# Mostrar Geografia\n#import geopandas as gpd\n#import matplotlib.pyplot as plt\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\NUTS3_2015_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_nuts3 = gpd.read_file(file_path, encoding=encoding)\n\nprint(gdf_nuts3.info())\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 25 entries, 0 to 24\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       25 non-null     object  \n 1   NUTS2       25 non-null     object  \n 2   NUTS3       25 non-null     object  \n 3   NUTS3_DSG   25 non-null     object  \n 4   SUM_AREA_K  25 non-null     float64 \n 5   Shape_Leng  25 non-null     float64 \n 6   Shape_Area  25 non-null     float64 \n 7   geometry    25 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.7+ KB\nNone\n\n\nmostrar pontos com Polygons\n\n# Existem diferenças entre o CRS!\nprint(\"CRS of the GeoDataFrame:\", gdf_aces.crs)\nprint(\"CRS of the GeoDataFrame:\", gdf_nuts3.crs)\n\n# Diferença nos CRS -&gt; Converter para WebMercator\ngdf_aces_m = gdf_aces.to_crs(\"epsg:3857\")\n\n# Obter apenas Municipios do Continente:\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS1 == '1']\n\n# Mostrar os 2 mapas:\n# Argumento cmap tem a predefinição de cores\n# Lista Referencias ColorMaps: https://matplotlib.org/stable/gallery/color/colormap_reference.html\nax = gdf_nuts3_sel.plot(column='NUTS3', \n                        cmap='Set2', \n                        legend=True, \n                        figsize=(10, 8))\n\n# Imprimir os pontos no mesmo ax (subplot)\n# Lista MatPlotLib cores: https://matplotlib.org/stable/gallery/color/named_colors.html\ngdf_aces_m.plot(ax=ax, \n                color='red',\n                markersize=30, \n                edgecolor='Black', \n                label='Cidades')\n\n# Adicionar Legenda\nax.legend()\nplt.title('Localização das ACES por NUTS3')\n\n# Mostrar o plot\nplt.show()\n\nCRS of the GeoDataFrame: EPSG:4326\nCRS of the GeoDataFrame: EPSG:3857\n\n\n\n\n\nO package Contextily permite adicionar um basemap\n\n# Import contextily\nimport contextily\n    \n# Existem diferenças entre o CRS!\nprint(\"CRS of the GeoDataFrame:\", gdf_aces.crs)\nprint(\"CRS of the GeoDataFrame:\", gdf_nuts3.crs)\n\n# Diferença nos CRS -&gt; Converter para WebMercator\ngdf_aces_m = gdf_aces.to_crs(\"epsg:3857\")\n\n# Obter apenas Municipios do Continente:\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS1 == '1']\n\n# Mostrar os 2 mapas:\n# Argumento cmap tem a predefinição de cores\n# Lista Referencias ColorMaps: https://matplotlib.org/stable/gallery/color/colormap_reference.html\n# Adicionar Transperência (alpha) ao layer\nax = gdf_nuts3_sel.plot(column='NUTS3', \n                        cmap='Set2', \n                        legend=True, \n                        figsize=(10, 8),\n                       alpha = 0.5)\n\n# Imprimir os pontos no mesmo ax (subplot)\n# Lista MatPlotLib cores: https://matplotlib.org/stable/gallery/color/named_colors.html\ngdf_aces_m.plot(ax=ax, \n                color='red',\n                markersize=30, \n                edgecolor='Black', \n                label='ACES')\n\n# Adicionar um BaseMap no Ax\ncontextily.add_basemap(ax)\n\n# Adicionar Legenda\nax.legend()\nplt.title('Localização das ACES por NUTS3')\n\n# Show the plot\nplt.show()\n\nCRS of the GeoDataFrame: EPSG:4326\nCRS of the GeoDataFrame: EPSG:3857\n\n\n\n\n\nObter Informações sobre o Objecto de Geometria\nA partir do geometry conseguimos obter um conjunto de caracteristicas\n\n# Tipo de Dados da coluna de Geometria\nprint (type(gdf_aces.geometry))\nprint (type(gdf_nuts3_sel.geometry))\nprint(gdf_nuts3_sel.info())\n\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 23 entries, 0 to 22\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       23 non-null     object  \n 1   NUTS2       23 non-null     object  \n 2   NUTS3       23 non-null     object  \n 3   NUTS3_DSG   23 non-null     object  \n 4   SUM_AREA_K  23 non-null     float64 \n 5   Shape_Leng  23 non-null     float64 \n 6   Shape_Area  23 non-null     float64 \n 7   geometry    23 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.6+ KB\nNone\n\n\nobter Área e Centroid\n\n# Obter Informaçao Geografia - Utilizando gdf_caop\n# Importar geometry\nfrom shapely.geometry import Point, Polygon\n\n# Adicionar Nova coluna com area em Km2\n# Atenção Deveriamos adicionar novos atributos sempre a (Geo)DataFrame de Origem\n# Não adicionar a seleção, neste caso gdf_nuts3_sel!\ngdf_nuts3['area_km2'] = gdf_nuts3['geometry'].area / 10**6  \n\n# Ordenar por Area\narea_gdf = gdf_nuts3.sort_values(by='area_km2', ascending=False)\n\n# Print the GeoDataFrame with the new column\nprint(area_gdf[['NUTS3','NUTS3_DSG', 'area_km2']])\n\n   NUTS3                     NUTS3_DSG      area_km2\n19   184                Baixo Alentejo  13733.680581\n22   187              Alentejo Central  12124.085355\n16   16J     Beiras e Serra da Estrela  10917.782234\n21   186                 Alto Alentejo  10140.463336\n7    11E      Terras de Trás-os-Montes   9909.588021\n18   181              Alentejo Litoral   8565.542590\n8    150                       Algarve   7899.809228\n14   16H                   Beira Baixa   7847.568545\n11   16E             Região de Coimbra   7444.322853\n6    11D                         Douro   7118.775462\n20   185               Lezíria do Tejo   7110.526353\n15   16I                    Médio Tejo   5639.532885\n13   16G              Viseu Dão Lafões   5638.896615\n4    11B                   Alto Tâmega   5239.870518\n17   170  Área Metropolitana de Lisboa   4963.947294\n12   16F              Região de Leiria   4157.670418\n0    111                    Alto Minho   4006.182814\n23   200    Região Autónoma dos Açores   3789.073859\n9    16B                         Oeste   3711.889332\n3    11A   Área Metropolitana do Porto   3597.098437\n5    11C                Tâmega e Sousa   3236.885188\n10   16D              Região de Aveiro   2942.595609\n2    119                           Ave   2589.188487\n1    112                        Cávado   2231.518979\n24   300    Região Autónoma da Madeira   1136.433784\n\n\nadicionar nova coluna com o centroide\n\n# Criar Nova coluna com Centroid\n# Adicionar Nova coluna com centroid\ngdf_nuts3['mn_center'] = gdf_nuts3['geometry'].centroid  \n\n# Print the GeoDataFrame with the new column\nprint(gdf_nuts3[['NUTS3','NUTS3_DSG', 'mn_center']])\n\n   NUTS3                     NUTS3_DSG                         mn_center\n0    111                    Alto Minho   POINT (-946980.726 5142757.347)\n1    112                        Cávado   POINT (-941320.248 5103934.014)\n2    119                           Ave   POINT (-909301.972 5085897.504)\n3    11A   Área Metropolitana do Porto   POINT (-943820.268 5025611.477)\n4    11B                   Alto Tâmega   POINT (-848339.419 5111654.442)\n5    11C                Tâmega e Sousa   POINT (-905197.685 5040987.781)\n6    11D                         Douro   POINT (-825322.310 5034791.692)\n7    11E      Terras de Trás-os-Montes   POINT (-759535.812 5097352.556)\n8    150                       Algarve   POINT (-905203.263 4473163.600)\n9    16B                         Oeste  POINT (-1015558.750 4762587.544)\n10   16D              Região de Aveiro   POINT (-949322.164 4958172.134)\n11   16E             Região de Coimbra   POINT (-927707.382 4898122.619)\n12   16F              Região de Leiria   POINT (-962098.688 4841198.729)\n13   16G              Viseu Dão Lafões   POINT (-881906.240 4968423.075)\n14   16H                   Beira Baixa   POINT (-825652.276 4850783.449)\n15   16I                    Médio Tejo   POINT (-920812.797 4807647.369)\n16   16J     Beiras e Serra da Estrela   POINT (-810641.235 4941054.650)\n17   170  Área Metropolitana de Lisboa  POINT (-1006290.656 4683604.593)\n18   181              Alentejo Litoral   POINT (-952473.033 4579022.311)\n19   184                Baixo Alentejo   POINT (-870119.976 4561859.007)\n20   185               Lezíria do Tejo   POINT (-958500.668 4738718.638)\n21   186                 Alto Alentejo   POINT (-848325.426 4749055.249)\n22   187              Alentejo Central   POINT (-872968.675 4665202.808)\n23   200    Região Autónoma dos Açores  POINT (-3038400.829 4629675.418)\n24   300    Região Autónoma da Madeira  POINT (-1887053.937 3862343.766)\n\n\ncalcular a distancia entre dois pontos\n\n#import geopandas as gpd\n#from shapely.geometry import Point, Polygon\n\n# Calcular diancia entre 2 ACES ('Amadora' and 'Lisboa Central')\n\n# Como dados são em Graus Decimais será necessário de os converter para Metricos\nprint(\"CRS gdf_aces:\", gdf_aces.crs)\n\norigem = 'Amadora'\ndestino = 'Grande Porto V - Porto Ocidental'\n# Converter para WebMercator\ngdf_aces_m = gdf_aces.to_crs(\"epsg:3857\")\n\n# Filtrar as geometrias para obter a Origem e Destino definidos\norigem_geometry = gdf_aces_m[gdf_aces_m['ACES'] == origem]['geometry'].iloc[0]\ndestino_geometry = gdf_aces_m[gdf_aces_m['ACES'] == destino]['geometry'].iloc[0]\n\n# Calculate the distance between 'Amadora' and 'Lisboa Central'\ndistance_km = origem_geometry.distance(destino_geometry) / 1000  # Converter para quilómetro\n\nprint(f\"A distância entre {origem} e {destino} é aproximadamente  {distance_km:.2f} quilómetros.\")\n\nCRS gdf_aces: EPSG:4326\nA distância entre Amadora e Grande Porto V - Porto Ocidental é aproximadamente  356.68 quilómetros.\n\n\n\n\n\n6.1.5 Juntar Dados a um GDF\n\n6.1.5.1 Censos 2021\nimporta dados de Censos2021 por NUTS3 e Municipio\n\n# Obter Password e Utilizador para Ligacao SQL\n#from getpass import getpass # para ler a password sem a mostrar\nmy_user = '\"BRUNO.LIMA\"'  \nmy_password = getpass(\"Password: \")\n\n# Ler Dados da BD\n# criar conexão\nimport cx_Oracle \nimport pandas as pd\nhost = 'c21oradev01.int.ine.pt'\nport = '1521'\nservice = 'FORMACAO'\ndsn_tns = cx_Oracle.makedsn(host, port, service_name=service) \n\n# Criar a conexão com todos os elementos,\n# incluingo user e password\nconn = cx_Oracle.connect(user=my_user, password=my_password, dsn=dsn_tns) \n\n# Cursor:\n# Criar o cursor na conexão conn que criámos antes\nc = conn.cursor()\n\n# ---------------------------------\n# Ler View com Dados por NUTS3:\n\n# SQL String\nmy_sql = \"\"\"\nselect *\nfrom BDIFRM.V_BGRI2021_N3_PT \n\"\"\"\n\n# Executar o cursor c com a string como parâmetro\nc.execute(my_sql)\n# Criar Nomes colunas\nnames = [ x[0] for x in c.description]\ndf_n3_c2021 = pd.DataFrame(c.fetchall(), columns = names)\n\n# Fechar cursor\nc.close()\n\n# fechar conexão\nconn.close()\n\nfazer JOIN dos dados GDF com as NUTS3\n\n# Juntar Dados NUTS3 do C2021 a GDF com as areas NUTS3\n\n# gdf_nuts3 obtido de Shapefile NUTS3: NUTS3_2015_PT.shp (dados estão em WebMercator)\n# Colunas Comuns: NUTS3 e NUTS3 - podemos ver os valores em ambos os DF\n#print(sorted(df_n3_c2021.NUTS3.unique()))\n#print(sorted(gdf_nuts3.NUTS3.unique()))\n\n# Fazer o Join, especificar: DF\ngdf_nuts3_2 = gdf_nuts3.merge(df_n3_c2021, left_on='NUTS3', right_on='NUTS3', how='left') \n\nprint(gdf_nuts3_2.info())\n\n\n\n\n6.1.6 Criar Mapas Temáticos\nVisualizar a classificação da relação entre 2 variáveis por NUTS3\n\n# Import packages\n# import matplotlib.pyplot as plt\n# import pandas as pd\n# import geopandas as gpd\n \n# Exemplo criar Plot dos Areas NUTS3 - Mostrando total de população 65+ no total de população\n# Utilizar geodataframe com join \"gdf_nuts3_2\"\n\n# Selecionar Dados Portugal Continental:\n# Fazer Seleção da NUTS1_x , houve rename da coluna apos join\ngdf_nuts3_sel = gdf_nuts3_2.loc[gdf_nuts3_2.NUTS1_x == '1']\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 2}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf_nuts3_sel.plot(column=gdf_nuts3_sel.N_INDIVIDUOS_65_OU_MAIS/gdf_nuts3_sel.N_INDIVIDUOS, \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=7, \n                      cmap='YlGn', \n                      legend=True,\n                      edgecolor = 'dimgray',\n                      legend_kwds  = lgnd_kwds)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Relacao população 65+ no total de população')\nplt.show()\n\n\n6.1.6.1 Exercício\nCriar Mapa Temático das NUTS com apresentação de cidades Estatisticas\n\n# Fazer o Join de NUTS e dados dos Censos , especificar: DF\ngdf_nuts3_2 = gdf_nuts3.merge(df_n3_c2021, left_on='NUTS3', right_on='NUTS3', how='left')\n\nprint(gdf_nuts3_2.info())\n\n# Importar os dados das cidades\nficheiro = r'data\\CIDADES_PONTOS_CONT.csv'\n\n# Importar em DataFrame\nencoding = 'utf-8'\ndf_cidades = pd.read_csv(ficheiro, sep=';', encoding=encoding)\n\ndf_cidades.head()\n\n#from shapely.geometry import Point, Polygon\n\n# Criar coluna com geometry (mudar nome df e nomes atributos)\ndf_cidades['geometry'] = df_cidades.apply(lambda x: Point(float(x.LONGITUDE), float(x.LATITUDE)), axis=1)\n\n# Criar uma gdf a partir da coluna geometry (mudar nome df)\ngdf_cidades = gpd.GeoDataFrame(df_cidades, crs = \"EPSG:4326\", geometry = df_cidades.geometry)\n\n# Converter Cidades para WebMercator\ngdf_cidades_m = gdf_cidades.to_crs(\"epsg:3857\")\n\n# Utilizar: gdf_nuts3_2\n# Input variáveis: gdf_aces e gdf_nuts3_2\n\n# Selecionar Dados Portugal Continental:\n# Fazer Seleção da NUTS1_x , houve rename da coluna apos join\ngdf_nuts3_sel = gdf_nuts3_2.loc[gdf_nuts3_2.NUTS1_x == '1']\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 2}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf_nuts3_sel.plot(column=(gdf_nuts3_sel.N_EDIFICIOS_CONSTR_ANTES_1945+gdf_nuts3_sel.N_EDIFICIOS_CONSTR_1946_1980)/gdf_nuts3_sel.N_EDIFICIOS_CLASSICOS, \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=7, \n                      cmap='YlGnBu', \n                      legend=True,\n                      edgecolor = 'gray',\n                      legend_kwds  = lgnd_kwds)\n\n# Adicionar os Pontos no mesmo AX\ngdf_cidades_m.plot(ax=ax, color='lightcoral', markersize=10, edgecolor='Black', label='Cidades')\n\n# Adicionar Legenda\nplt.title('Rácio Edificios construidos até 1980 (com representação das cidades)')\n\n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Relacao edificios até 1980 no total de edificios')\nplt.show()\n\n\ngdf_cidades_m.explore()\n\n\n\n\n6.1.7 Folium\n\nimport folium\n\n# Mostrar Zona do INE \n# Lisboa: 38.738345820101536, -9.138601637922605\n# Porto: 41.150887961411634, -8.629046747840249\n# Longitude e Latitude\nine = folium.Map(location = [41.150887961411634, -8.629046747840249], \n                 zoom_start = 15)\n\nine\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nImportar Dataset com NUTS3\n\n# Importar GeoPandas\nimport geopandas as gpd\n\n# Carregar Dados com read_file (Shapefile preferivel, GeoJSON -  Mais Lento)\n# Mudar Caminho para onde estão os dados - atenção de ter os ficheiros .shp\\.shx\\.dbf\\.prj\nfile_path = r\"data\\NUTS3_2015_PT.shp\"\n\n# Definir o encoding para evitar problemas de desenho dos nomes\nencoding = 'utf-8'  \n# Ler Shapefile:\ngdf_nuts3 = gpd.read_file(file_path, encoding=encoding)\n\ngdf_nuts3.loc[1,'geometry']\n\n\n\n\nMostrar Localização Central da GDF das NUTS3\n\n# Obter a Localização Central\n# Print the head of the urban_polygon\n#import geopandas as gdp\nimport folium\n\n# Obter o centroid de toda a geometria\n# Converter para 4326 e a seguir obter o centroid de união de toda a geometria\ngdf_nuts3_sel = gdf_nuts3.loc[gdf_nuts3.NUTS1 == '1'] # Selecionar apenas o continente\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude longitude\ncenter_map = [centroid.y, centroid.x]\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=8, tiles='OpenStreetMap')\n\nfolium_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nadicionar geografia (GDF) ao mapa\n\n# Adicionar a Geografia a mapa\n# Print the head of the urban_polygon\n# import geopandas as gdp\n# import folium\n# from shapely.geometry import Point, Polygon\n\n# Converter para 4326 e a seguir obter o centroid de união de toda a geometria\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=7, tiles='OpenStreetMap')\n\n# Adicionar Geografia folium map\n# folium.GeoJson constructor\nfolium.GeoJson(gdf_nuts3_sel).add_to(folium_map)\n\nfolium_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nadicionar pop e tooltip\n\ngdf_nuts3_sel.info()\n\n# Incluir Informação de POPUP\n# Print the head of the urban_polygon\n#import geopandas as gdp\n#import folium\nimport folium.plugins \n\n# Converter para 4326 e a seguir obter o centroid de união de toda a geometria\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=7, tiles='OpenStreetMap')\n\n# Codigo Especifico para tooltip\ntooltip = folium.GeoJsonTooltip(\n    fields=[\"NUTS3\"],\n    aliases=[\"NUTS3\"],\n    localize=True,\n    sticky=False,\n    labels=True,\n    style=\"\"\"\n        background-color: #F0EFEF;\n        border: 2px solid black;\n        border-radius: 3px;\n        box-shadow: 3px;\n    \"\"\",\n    max_width=800,\n)\n\n# Definir popup e nome\nfolium.GeoJson(gdf_nuts3_sel,\n        zoom_on_click = True,\n        popup = folium.GeoJsonPopup(\n            fields=['NUTS3','NUTS3_DSG'],\n            aliases=['NUTS3','NUTS3_DSG']\n        ),\n        tooltip = tooltip       \n               \n    ).add_to(folium_map) \n\n\nfolium_map\n# folium_map.save(r'data\\xxmap.html')\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 23 entries, 0 to 22\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   NUTS1       23 non-null     object  \n 1   NUTS2       23 non-null     object  \n 2   NUTS3       23 non-null     object  \n 3   NUTS3_DSG   23 non-null     object  \n 4   SUM_AREA_K  23 non-null     float64 \n 5   Shape_Leng  23 non-null     float64 \n 6   Shape_Area  23 non-null     float64 \n 7   geometry    23 non-null     geometry\ndtypes: float64(3), geometry(1), object(4)\nmemory usage: 1.6+ KB\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.1.8 Criar Mapa Temático em Folium\n\n# Import packages\n# import pandas as pd\n# import geopandas as gpd\n# import folium, folium.plugins \n\n# Mesmo Exemplo criar Plot dos Areas NUTS3 - Mostrando total de população 65+ no total de população\n# Assegurar que foi efetuado Merge com Dados C2021\n# GDF input: gdf_nuts3_2\n\n# Selecionar Dados Portugal Continental: (copy() para criar nova coluna)\ngdf_nuts3_sel = gdf_nuts3_2.loc[gdf_nuts3_2.NUTS1_x == '1'].copy()\n\n# Novo Atributo\ngdf_nuts3_sel['ind65'] = gdf_nuts3_sel.N_INDIVIDUOS_65_OU_MAIS/gdf_nuts3_sel.N_INDIVIDUOS\n\n# Obter o centroid de toda a geometria\ncentroid = gdf_nuts3_sel.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=6, tiles='OpenStreetMap')\n\n# Criar choropleth\nchoropleth = folium.Choropleth(\n    geo_data=gdf_nuts3_sel,\n    name='População 65+ mais',\n    data=gdf_nuts3_sel,\n    columns=['NUTS3', 'ind65'],\n    key_on='feature.properties.NUTS3',\n    fill_color='Reds', #\n    fill_opacity=0.5,\n    line_opacity=1.0,\n    bins =8,\n    legend_name='Relacao população 65+ no total de população'\n)\n\nfolium.GeoJson(gdf_aces,\nzoom_on_click = True).add_to(folium_map)\n\n# Adicionar a mapa\nchoropleth.add_to(folium_map)\n\n# Widget para controlar os layer visiveis            \nfolium.LayerControl().add_to(folium_map)\n\nfolium_map.save(r'data\\xxmap.html')\n\nfolium_map\n\n# Limpar Object da memoria\ngdf_nuts3_sel = None"
  },
  {
    "objectID": "700-mod7.html#classes",
    "href": "700-mod7.html#classes",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.1 Classes",
    "text": "7.1 Classes\n\n7.1.1 Herança\n Classes são modelos para criar objetos. A herança é um mecanismo que permite a uma classe herdar atributos e métodos de outra classe.\n\nclass pai:\n  # código do pai\n  pass\n\nclass filho(pai):\n  # código do filho\n  pass\n\n\nHerança simples\n\n\nclass Pessoa(object):\n\n  def __init__(self, nome):\n    self._nome = nome\n\n  @property\n  def nome(self):\n    return self._nome\n\n  @nome.setter\n  def nome(self, novo_nome):\n    self._nome = novo_nome\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Empregado(Pessoa):\n  num_funcionario: int\n\n  def __init__(self, nome, num_funcionario):\n     Pessoa.__init__(self, nome)\n     self.num_funcionario = num_funcionario\n\n    # ou\n    #super().__init__(nome) # construtor da classe pai\n\n\ncidadao = Pessoa(\"Carla\")\n\nfuncionario = Empregado(\"Pedro\", 950)\n\nprint(f'A {cidadao.nome} é uma Pessoa')\n\nprint(f'O {funcionario.nome} é um Funcionário com número de funcionário {funcionario.num_funcionario}.')\n\nA Carla é uma Pessoa\nO Pedro é um Funcionário com número de funcionário 950.\n\n\n\nHerança múltipla\n\n\nclass Aviao:\n  def sabe_voar(self):\n    print(\"Sei voar\")\n\nclass Barco:\n  def sabe_navegar(self):\n    print(\"Sei navegar\")\n\nclass Hidroviao(Aviao, Barco):\n  pass\n\n_ = Hidroviao()\n_.sabe_navegar()\n_.sabe_voar()\n\nSei navegar\nSei voar\n\n\nProblema Diamond\n\nclass A:\n\n   def display(self):\n       print(\"Class A\")\n\nclass X(A):\n\n   def display(self):\n       print(\"Class X\")\n\nclass Y(A):\n\n   def display(self):\n       print(\"Class Y\")\n\nclass Z(X, Y):\n   pass\n\nobj = Z()\nobj.display()\n\nClass X\n\n\n\n# @title Saber a MRO de uma classe\n\n#print(Z.mro())\nprint(Z.__mro__)\n\n(&lt;class '__main__.Z'&gt;, &lt;class '__main__.X'&gt;, &lt;class '__main__.Y'&gt;, &lt;class '__main__.A'&gt;, &lt;class 'object'&gt;)\n\n\n\nHerança multi-nível\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Pessoa:\n  nome: str\n\n@dataclass\nclass Atleta (Pessoa):\n  clube: str\n\n@dataclass\nclass Futebolista(Atleta):\n  posicao: str\n\npele = Futebolista(\"Pelé\", \"Santos\", \"Avançado\")\nprint(pele)\n\nFutebolista(nome='Pelé', clube='Santos', posicao='Avançado')\n\n\nEsconder atributos\n\nclass Utilizador:\n  def __init__(self, meu_username, meu_password):\n    self._username = meu_username # protected: começa por _ (unerscore)\n    self.__password = meu_password # private: começa por __ (duplo underscore)\n    self.__gerar_id()\n\n  def __gerar_id(self):  # private\n    from random import randint\n    self.id = randint(1, 99999)\n\nclass Cliente(Utilizador):\n  def __init__(self, cliente_username, cliente_password, data_de_entrada):\n    self.data_registo = data_de_entrada # public\n    Utilizador.__init__(self,  cliente_username, cliente_password)\n\ncliente1 = Cliente('telma', 'xyzpass', '2020-10-16')\nprint(cliente1._username)\nprint(cliente1.data_registo)\n#print(cliente1.__password)\n#cliente1.__gerar_id()\nprint(cliente1.id)\n\ntelma\n2020-10-16\n52537\n\n\n\nprint(cliente1._Utilizador__password) # privado mas com transparẽncia\n\nprint(cliente1.id)\ncliente1._Utilizador__gerar_id()  # privado mas com transparẽncia\nprint(cliente1.id)\n\nxyzpass\n52537\n15535\n\n\n\nHerança híbrida: quando se misturam vários tipos de herança;\nHernça hierárquica: várias classes herdam duma classe comum.\n\n\n\n7.1.2 Polimorfismo\n\n\n7.1.2.1 Funções polimórficas\n\n# @title Python built-in\n\n# len() para string\nprint(len(\"palavra\"))\n\n# len() para lista\nprint(len([10, 20, 30]))\n\n7\n3\n\n\n\n# @title User Defined Function\ndef soma(a,b,c=0):\n  return a+b+c\n\nprint(soma(2,3))\nprint(soma(2,3,4))\n\n5\n9\n\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Poligono:\n  nome: str\n  cor: str\n\nclass Triangulo(Poligono):\n  def __init__(self, cor_do_triangulo):\n    super().__init__(\"triangulo\",cor_do_triangulo)\n\n  def desenha(self):\n    import matplotlib.pyplot as plt\n\n    # Draw a triangle\n    plt.plot([0, 1], [0, 0], self.cor)\n    plt.plot([1, 0.5], [0, 0.5], self.cor)\n    plt.plot([0.5, 0], [0.5, 0], self.cor)\n    plt.axis('off')\n    plt.show()\n\nclass Rectangulo(Poligono):\n  def __init__(self,cor_do_rectangulo):\n    super().__init__( \"rectangulo\",cor_do_rectangulo)\n\n  def desenha(self):\n    import matplotlib.pyplot as plt\n    # Draw a rectangle\n    plt.plot([4, 6], [0, 0], self.cor)\n    plt.plot([6, 6], [0, 1], self.cor)\n    plt.plot([6, 4], [1, 1], self.cor)\n    plt.plot([4, 4], [1, 0], self.cor)\n\n    plt.axis('off')\n    plt.show()\n\n\nt = Triangulo('b')\nr = Rectangulo('g')\n\n# 'b' as blue\n# 'g' as green\n# 'r' as red\n# 'c' as cyan\n# 'm' as magenta\n# 'y' as yellow\n# 'k' as black\n# 'w' as white\n\nfor poligono in (t,r):\n  print(poligono.nome)\n  poligono.desenha()\n\ntriangulo\n\n\n\n\n\nrectangulo\n\n\n\n\n\n\n\n7.1.2.2 Exercícios\n\n# @title Exercício (copy/paste)\n\nclass Classe1():\n  s = \"Bom\"\n\nclass Classe2:\n  s = \"trabalho\"\n\nclass Classe3(Classe1, Classe2):\n\n  def __init__(self):\n    # imprime Bom trabalho\n    print(Classe1.s, Classe2.s)\n    \n_ = Classe3()\n\nBom trabalho\n\n\nna herança multipla uma classe que herda mais do que uma classe"
  },
  {
    "objectID": "700-mod7.html#sequências-e-conjuntos",
    "href": "700-mod7.html#sequências-e-conjuntos",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.2 Sequências e Conjuntos",
    "text": "7.2 Sequências e Conjuntos\n\n\n7.2.1 Tuplos\ncoleção ordenada; imutável; heterogénea; indexada a zero [0]; permite duplicados.\n\nestacoes = (\"Primavera\", \"Verão\", \"Outono\", \"Inverno\")\n\n# outra forma, a partir do Python 3.11\nvalues : tuple[int | str, ...] = (1,2,4,\"palavra\")\n# túplo que aceita string ou int e pode ter vários de cada\n\nsó com um elemento\n\ntuplo = (\"sozinho\",) # acaba com uma vírgula\nprint(type(tuplo))\n\n#NOT a tuple\nnao_tuplo = (\"só\")\nprint(type(nao_tuplo))\n\n&lt;class 'tuple'&gt;\n&lt;class 'str'&gt;\n\n\nusando um construtor\n\nt = tuple([\"baixo\", \"médio\", \"alto\"])\nprint(t)\n\n('baixo', 'médio', 'alto')\n\n\naceder a um elemento do tuplo\n\nt = tuple((\"baixo\", \"médio\", \"alto\"))\n\nprint(t[1])\n\nprint(t[-1])\n\nmédio\nalto\n\n\noperações com tuplos\n\n# @title Concatenar\nt1 = (\"a\", \"b\")\nt2 = (1, 2)\nt3 = t1 + t2\nt3\n\n# @title Nesting\nt1 = (\"a\", \"b\")\nt2 = (1, 2)\nt3 = (t1, t2)\nt3\n\n# @title Repetição\nt1 = (\"a\", \"b\")\nt2 = t1 * 3\nt2\n\n# @title Slicing\nt1 = (0 ,1, 2, 3)\n\nprint(t1[1:])\n\nprint(t1[::-1])\n\nprint(t1[2:4])\n\n# @title Número de elemtentos\nt1 = (\"a\", \"c\", \"c\")\nlen(t1)\n\n# @title Apagar túplo\nt1 = (1,)\ndel t1\n\n#### Converter Lista em Túplo\nlista = [1, 2, 3, 4]\ntuplo = tuple(lista)\ntuplo\n\n# @title Converter String em Túplo\ns = \"abcd\"\nt = tuple(s)\nt\n\n(1, 2, 3)\n(3, 2, 1, 0)\n(2, 3)\n\n\n('a', 'b', 'c', 'd')\n\n\niterar um tuplo\n\nt1 = (\"a\", 1, True, \"b\")\n\nfor i in t1:\n  print(type(i))\n\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n&lt;class 'bool'&gt;\n&lt;class 'str'&gt;\n\n\n\n\n7.2.2 Sets\ncoleção não ordenada; mutável; sem duplicados; pode ser iterada; sintaxe: entre chavetas { } e separado por vírgulas.\n\nconjunto = {\"tangerina\", \"papaia\", \"melão\"}\n\nconjunto = set([\"tangerina\", \"papaia\", \"melão\"])\n\nconjunto\n\n{'melão', 'papaia', 'tangerina'}\n\n\n\n# @title Conjunto vazio\nset1 = set()\nprint(\"Conjunto vazio\")\nprint(set1)\n\n# @title Set a partir de uma String\nset1 = set(\"palavra\")\nprint(set1)\n\noutra_string = \"outra palavra\"\nset1 = set(outra_string)\nprint(set1)\n\n# @title Set a partir de uma lista\nset1 = set([\"a\", \"b\", \"c\"])\nprint(set1)\n\n# @title Set a partir de um tuplo\nt = (\"um\",\"dois\",\"três\")\nprint(set(t))\n\n# @title Set a partir de um dicionário\nd={\"Sim\":\"yes\", \"Não\":\"No\", \"Talvez\":\"Maybe\"}\nprint(set(d))\n\nConjunto vazio\nset()\n{'r', 'v', 'l', 'p', 'a'}\n{'t', 'r', 'u', 'v', 'l', 'p', 'o', 'a', ' '}\n{'a', 'c', 'b'}\n{'um', 'três', 'dois'}\n{'Não', 'Sim', 'Talvez'}\n\n\nadicionar um elemento a um conjunto\n\ns = {1, 2}\ns.add(3)\ns\n\n{1, 2, 3}\n\n\nadicionar vários elementos a um conjunto\n\ns = {1, 2}\nnovos_elementos=[3, 4, 5] # não tem de ser uma lista\ns.update(novos_elementos)\ns\n\n{1, 2, 3, 4, 5}\n\n\naceder a elementos\ncomo não há ordem não se pode aceder a elementos de um conjunto via indice\n\nset1 = set([\"a\", \"b\", \"c\"])\n\nfor elemento in set1:\n    print(elemento, end=\" \")\n\na c b \n\n\nverificar se um elemento existe num conjunto\n\nset1 = set([\"a\", \"b\", \"c\"])\nprint(\"b\" in set1)\n\nTrue\n\n\nremover um elemento de um conjunto\n\ns = {1, 2, 3}\n\ns.remove(2)\n\n#s.remove(4) # daria erro\ns.discard(4) # não dá se o elemento não existir\n\ns\n\n{1, 3}\n\n\nremover todos os elementos de um conjunto\n\ns = {9, 8, 7}\ns.clear()\ns\n\nset()\n\n\nremover sem saber o quê\n\ns = {2, 1, 3, 4, 5}\nremovi = s.pop()\n\nprint(removi)\ns\n\n1\n\n\n{2, 3, 4, 5}\n\n\nFrozensets: conjunto imutável\n\nfrz=frozenset({1, 2, 3})\nfrz\n\nfrozenset({1, 2, 3})\n\n\nconverter em set (type casting)\n\n# @title list\nlista = [1, 2, 3, 3, 4, 5, 5, 6, 2]\nconjunto = set(lista)\nconjunto\n\n{1, 2, 3, 4, 5, 6}\n\n\n\n# @title string\ns = \"Palavra\"\nconjunto = set(s)\nconjunto\n\n# @title dictionary\nd = {1: \"Um\", 2: \"Dois\", 3: \"Três\"}\nconjunto = set(d)\nconjunto\n\n{1, 2, 3}\n\n\nOperações de conjuntos\n\n# @title Unir\ns1 = {1, 2}\ns2 = {3, 4, 5}\ns3 = s1.union(s2)\ns3\n\n# @title Intersecção\ns1 = {1, 2, 3, 4, 5}\ns2 = {4, 5, 6, 7, 8}\ns3 = s1.intersection(s2)\ns3\n\n# @title Diferença\ns1 = {1, 2, 3, 4, 5}\ns2 = {4, 5, 6, 7, 8}\n\n# pertence a s1 e não pertence a s2\ns3 = s1.difference(s2)\ns3\n\n# @title diferença simétrica\ns1 = {1, 2, 3, 4, 5}\ns2 = {4, 5, 6, 7, 8}\n\n# elementos dos dois conjuntos que não são comuns\ns3 = s1.symmetric_difference(s2)\ns3\n\n# @title Verifica se é subconjunto\ns1 = {1, 2, 3, 4, 5}\ns2 = {2, 3, 4}\nprint(s2.issubset(s1))\n\n# @title Verifica se é um superset\ns1 = {1, 2, 4, 5}\ns2 = {2, 3, 4}\nprint(s1.issuperset(s2))\n\nTrue\nFalse"
  },
  {
    "objectID": "700-mod7.html#module-collections",
    "href": "700-mod7.html#module-collections",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.3 module collections",
    "text": "7.3 module collections\n\nCounters\nOrderedDict\nDefaultdict\nChainMap\nNamedTuple\nDeQue\nUserDict\nUserList\nUserString\n\n\n# @title Criar counters\nfrom collections import Counter\n\n# sequencia de items\nprint(Counter(['B','B','A','B','C','A','B','B','A','C']))\n\n# dictionary\nprint(Counter({'A':3, 'B':5, 'C':2}))\n\n# keyword arguments\nprint(Counter(A=3, B=5, C=2))\n\n\n# @title Actualizar counters\ncoun = Counter()\n\ncoun.update([1, 2, 3, 1, 2, 1, 1, 2])\nprint(coun)\n\ncoun.update([1, 2, 4])\nprint(coun)\n\n# @title Subtrair counters\n# Python program to demonstrate that counts in\n# Counter can be 0 and negative\nc1 = Counter(A=4, B=3, C=10, J=40)\nc2 = Counter(A=10, B=3, C=4, D=5)\n\nc1.subtract(c2)\nprint(c1)\n\n# @title Distinct count numa lista\nz = ['chá', 'café', 'chá', 'vinho', 'água', 'chá', 'café']\n# Quantos elementos únicos existem na lista, e quantas unidade e cada elemento?\nprint(Counter(z))\n\n\n# @title imprimir valores do counter\nc = Counter('abracadabra')\nprint(c.keys())\nprint(c.values())\nprint(c.items())\n\nCounter({'B': 5, 'A': 3, 'C': 2})\nCounter({'B': 5, 'A': 3, 'C': 2})\nCounter({'B': 5, 'A': 3, 'C': 2})\nCounter({1: 4, 2: 3, 3: 1})\nCounter({1: 5, 2: 4, 3: 1, 4: 1})\nCounter({'J': 40, 'C': 6, 'B': 0, 'D': -5, 'A': -6})\nCounter({'chá': 3, 'café': 2, 'vinho': 1, 'água': 1})\ndict_keys(['a', 'b', 'r', 'c', 'd'])\ndict_values([5, 2, 2, 1, 1])\ndict_items([('a', 5), ('b', 2), ('r', 2), ('c', 1), ('d', 1)])\n\n\norderedDict subclasse de dicionário que mantém a ordem de inserção dos elementos\n\nfrom collections import OrderedDict\n\nprint(\"Dicionário:\\n\")\nd = {}\nd['a'] = 1\nd['b'] = 2\nd['c'] = 3\nd['d'] = 4\n\n# remover e inserir\nd.pop('a')\nd['a'] = 1\n\n# alterar o valor\nd['c'] = 5\n\nfor key, value in d.items():\n    print(key, value)\n\nprint(\"\\nOrdered Dict:\\n\")\nod = OrderedDict()\nod['a'] = 1\nod['b'] = 2\nod['c'] = 3\nod['d'] = 4\n\n# remover e inserir\nod.pop('a')\nod['a'] = 1\n\n# alterar o valor\nod['c'] = 5\n\nfor key, value in od.items():\n    print(key, value)\n\nDicionário:\n\nb 2\nc 5\nd 4\na 1\n\nOrdered Dict:\n\nb 2\nc 5\nd 4\na 1\n\n\ndefaultDict: subclasse de dicionário que permite definir um valor padrão para chaves inexistentes\n\nfrom collections import defaultdict\n\ndef default_e_5():\n    return 5\n\nd = defaultdict(int)\n\n# O valor por omissão (default value) é 0\nprint(d[15])\n\nd2 = defaultdict(default_e_5)\nprint(d2[15])\n\n\nd = defaultdict(int)\ndicionario_normal={}\n\nL = [1, 2, 3, 4, 2, 4, 1, 2]\n\nfor i in L:\n\n    # O valor por omissão (default value) é 0\n    # por isso não é necessário inserir a chave primeiro\n    d[i*2] += 1\n    #dicionario_normal[i*2] += 1\n\nprint(d)\n\n# outro exmplo\nd = defaultdict(list)\n\nfor i in range(5):\n    d[i].append(i)\n\nprint(d)\n\n0\n5\ndefaultdict(&lt;class 'int'&gt;, {2: 2, 4: 3, 6: 1, 8: 2})\ndefaultdict(&lt;class 'list'&gt;, {0: [0], 1: [1], 2: [2], 3: [3], 4: [4]})\n\n\nchainMap: permite juntar vários dicionários numa única estrutura e devolve uma lista de dicionários\n\nfrom collections import ChainMap\n\nd1 = {'a': 1, 'b': 2}\nd2 = {'c': 3, 'd': 4}\nd3 = {'e': 5, 'f': 6}\n\n# Defining the chainmap\njuntos = ChainMap(d1, d2, d3)\n\nprint(juntos)\n\nprint(juntos['a'], juntos['f'])\n\nChainMap({'a': 1, 'b': 2}, {'c': 3, 'd': 4}, {'e': 5, 'f': 6})\n1 6\n\n\nnamedTuple: permite criar tuplos nomeados. Pode ser visto como uma formaleve de criar uma lista\n\nfrom collections import namedtuple\n\n# namedtuple()\nEstudante = namedtuple('Estudante', ['nome', 'curso', 'data_nascimento'])\n\n# Adicionar valor\ns = Estudante('Mónica', 'Filosofia', '25-12-1997')\n\n# Aceder via index\nprint(f\"O curso é {s[1]}\")\n\n# Aceder via nome\nprint(f\"O nome é {s.nome} \")\n\nprint(s)\n\nO curso é Filosofia\nO nome é Mónica \nEstudante(nome='Mónica', curso='Filosofia', data_nascimento='25-12-1997')\n\n\ndeQue: é uma lista de dupla extremidade (double-ended queue)\n\nfrom collections import deque\n\ndq = deque([3, 4, 5])\n\nprint(dq)\n\ndq.append(6)\nprint(dq)\n\ndq.appendleft(9)\nprint(dq)\n\ndq.pop()\nprint(dq)\n\ndq.popleft()\nprint(dq)\n\ndeque([3, 4, 5])\ndeque([3, 4, 5, 6])\ndeque([9, 3, 4, 5, 6])\ndeque([9, 3, 4, 5])\ndeque([3, 4, 5])\n\n\nuserDict, userList, userString: são subclasses de dicionário, lista e string, respetivamente com funcionalidades especificas\n\n# title userDict\nfrom collections import UserDict\n\n# Dicionário que não permite apagar elementos\nclass MeuDict(UserDict):\n\n    # Função para impedir apagar do dicionário\n    def __del__(self):\n        raise RuntimeError(\"Não é permitido apagar (del)\")\n\n    # Função para impedir pop() do dicionário\n    def pop(self, s = None):\n        raise RuntimeError(\"Não é permitido apagar (pop)\")\n\n    # Função para impedir popitem() do dicionário\n    def popitem(self, s = None):\n        raise RuntimeError(\"Não é permitido apagar (popitem)\")\n\nd = MeuDict({'a':1, 'b': 2, 'c': 3})\n\n#d.pop(1)\n\n# @title UserList\n\nfrom collections import UserList\n\n# Lista que não permite apagar\nclass MinhaLista(UserList):\n\n    # Função para impedir apagar da lista\n    def remove(self, s = None):\n        raise RuntimeError(\"Não é permitido apagar (remove)\")\n\n    # Função para impedir apagar pop() da lista\n    def pop(self, s = None):\n        raise RuntimeError(\"Não é permitido apagar (pop)\")\n\nL = MinhaLista([1, 2, 3, 4])\nL.append(5)\nprint(L)\n\n#L.remove()\n\n# @title UserString\nfrom collections import UserString\n\n# String mutável\nclass MutableString(UserString):\n\n    # Função para adicionar a uma stirng\n    def append(self, s):\n        self.data += s\n\n    # Função para remover de uma string\n    def remove(self, s):\n        self.data = self.data.replace(s, \"\")\n\n\ns1 = MutableString(\"Palavra\")\nprint(\"String original:\", s1.data)\n\n# Adicionar\ns1.append(\"s\")\nprint(\"Após adicionar:\", s1.data)\n\n# Remover\ns1.remove(\"vra\")\nprint(\"Após remover:\", s1.data)\n\n[1, 2, 3, 4, 5]\nString original: Palavra\nApós adicionar: Palavras\nApós remover: Palas"
  },
  {
    "objectID": "700-mod7.html#programação-recursiva",
    "href": "700-mod7.html#programação-recursiva",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.4 Programação Recursiva",
    "text": "7.4 Programação Recursiva\num processo que se chama a si mesmo directa ou indirectamente para computar o seu resultado.\n\n# @title Successor\ndef suc(n):\n  if n == 0:\n    return 1\n  else:\n    return 1 + suc(n-1)\n\nprint(suc(4))\n\n# @title Factorial\ndef factorial(num):\n  if num == 1:\n    return num\n  else:\n    return num * factorial(num - 1)\n\nprint(factorial(4))\n\n5\n24\n\n\n\n7.4.1 Exercícios\n\n# # fazer função get_comuns que recebe 3 listas e devolve uma lista com os elementos comuns às 3 listas\n# def get_comuns(l1, l2, l3[]):\n#   s1 = set(l1)\n#   s2 = set(l2)\n#   \n#   comuns = s1.intersection(s2, s3)\n#   \n#   if len(3)&gt;0:\n#     s3 = set(l3)\n#     comuns = comuns.intersection(s3)\n#   \n#   return list(comuns)\n\n\n# fazer função um_to_n que recebe um número n e devolve a sequencia de 1 a n\ndef um_to_n(n):\n  if n == 1:\n    return [1]\n  else:\n    return um_to_n(n-1) + [n]\n\num_to_n(4)\n\n[1, 2, 3, 4]"
  },
  {
    "objectID": "700-mod7.html#expressões-regulares",
    "href": "700-mod7.html#expressões-regulares",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.5 Expressões Regulares",
    "text": "7.5 Expressões Regulares\n\nimport re\ns=\"Um grande dia para falar de expressões regulares\"\n\nmatch=re.search(r\"falar\", s)\n\nprint(match.start(), match.end())\n\n19 24\n\n\nmeta caracteres:\n\n\\ - Caracter de escape, remove o a funcionalidade especial do carcater que está a seguir\n[] - Representa uma classe de caracteres\n^ - Faz match com o início\n$ - Faz match com o fim\n. - Faz match com qualquer caracter excepto o “newline”\n| - Significa “OR”. Faz match com qualquer dos caracteres separados pelo símbolo\n? - Faz match com zero ou mais ocorrências\n* - Faz match com qualquer número de ocorrências (incluindo 0 ocorrências)\n+ - Uma ou mais ocorrências\n{} - Indica o número de ocorrências que a expressão imediatamente anterior tem de fazer match\n() - Representa um grupo da expressão regular\n\nOperações\n\nimport re\n\nprint(re.search(r\"9+\",\"289908\")) # produra os 9's\nprint(re.search(r\"\\d{3}\",\"abc1234de\"))\n\n# primeiro conjunto de letras\nprint(re.match(r\"\\w+\",\"são 13 horas?\")) # \nx=re.match(r\"\\w+\",\"são 13 horas?\")\n\n# primeiro simbolo que é não letra\nprint(re.search(r\"\\W\",\"São 13 horas?\"))\n\nprint(re.search(r\"x*e+\",\"abxcdxxxefxghxx\")) # x seguido de zero ou mais e seguido de um ou mais e\n\nprint(re.findall(r\"xx+\",\"abxcdxxxefxghxx\")) # x seguido de um ou mais x\n\nprint(re.search(r\"[ine]\",\"ana\")) # procura i, n ou e\n\n# @title group\nm = re.match(r\"(\\w+) (\\w+)\", \"Instituto Nacional de Estatística\")\nm.group(0)       # O match completo\n#'Instituto Nacional'\n\nm.group(1)       # O 1º subgroupo entre parentises.\n#'Instituto'\n\nm.group(2)       # O 2º subgroupo entre parentises\n#'Nacional'\n\nm.group(2, 1)    # Vários argumentos, devolve-nos um tuplo\n#('Nacional','Instituto')\n#m.groups()\n\n# @title group() e groups()\nresultado = (re.search(r\"(?:AB)\",\"ACABC\"))\nprint(resultado)\nprint(resultado.groups()) # todos os grupos a partir do grupo 1\nprint(resultado.group())\n#print(resultado.group(0))\n\nresultado = re.search(r\"(\\w*), (\\D*), (\\w*)\",\"espinafres, 12, combóio, ?, falange\")\nprint(resultado.groups())\nprint(resultado.group())\n\nresultado = (re.search(r\"(?:AB)\",\"acabc\"))\nprint(resultado)\n\nresultado = (re.search(r\"(?:AB)\",\"acabc\", flags=re.IGNORECASE))\nprint(resultado)\n\nresultado = (re.search(r\"(?i:AB)\",\"acabc\"))\nprint(resultado)\n\npartes = re.split(r'\\.', '127.0.0.1', maxsplit=5)\nprint(partes)\n\n# @title validar\nvalid = re.compile(r\"^[a2-9tjqk]{5}$\")\n\nmatch = valid.match(\"akt5q\")  # Válido\n\nprint(match.group())\n\nmatch = valid.match(\"akt5e\") # Inválido\n\nmatch = valid.match(\"akt\") # Inválido\n\nmatch = valid.match(\"727ak\") # Válido\n\nprint(match.group())\n\n# @title backreferences\npair = re.compile(r\".*(.).*\\1\")\n\nprint(pair.match(\"717ak\"))\n\nprint(pair.match(\"717ak\").group())\n\nprint(pair.match(\"717ak\").group(1))\n\n# @title gropudict (e named references)\nm = re.match(r\"(?P&lt;codigo_postal&gt;\\d{4}-\\d{3}) (?P&lt;localidade&gt;\\w+)\", \"4000-064 PORTO\")\nm.groupdict()\n\n&lt;re.Match object; span=(2, 4), match='99'&gt;\n&lt;re.Match object; span=(3, 6), match='123'&gt;\n&lt;re.Match object; span=(0, 3), match='são'&gt;\n&lt;re.Match object; span=(3, 4), match=' '&gt;\n&lt;re.Match object; span=(5, 9), match='xxxe'&gt;\n['xxx', 'xx']\n&lt;re.Match object; span=(1, 2), match='n'&gt;\n&lt;re.Match object; span=(2, 4), match='AB'&gt;\n()\nAB\n('12', 'combóio, ?', 'falange')\n12, combóio, ?, falange\nNone\n&lt;re.Match object; span=(2, 4), match='ab'&gt;\n&lt;re.Match object; span=(2, 4), match='ab'&gt;\n['127', '0', '0', '1']\nakt5q\n727ak\n&lt;re.Match object; span=(0, 3), match='717'&gt;\n717\n7\n\n\n{'codigo_postal': '4000-064', 'localidade': 'PORTO'}"
  },
  {
    "objectID": "700-mod7.html#lambda-functions",
    "href": "700-mod7.html#lambda-functions",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.6 Lambda Functions",
    "text": "7.6 Lambda Functions\n\n# função UDF\ndef cubo(y):\n    return y*y*y\n\n# função lambda\nlambda_cubo = lambda y: y*y*y\n\nprint(\"usando a UDF:\", cubo(5))\nprint(\"usando a função lambda:\", lambda_cubo(5))\n\nusando a UDF: 125\nusando a função lambda: 125\n\n\nem comprehensions\n\nmultiplos_de_dez = [lambda arg=x: arg * 10 for x in range(1, 9)]\n\nfor item in multiplos_de_dez:\n    print(item())\n\n10\n20\n30\n40\n50\n60\n70\n80\n\n\nem funções breves\n\nMax = lambda a, b : a if(a &gt; b) else b\nprint(Max(1, 2))\n\n\n(lambda x: (x+3)*5*x/2)(3)\n\n2\n\n\n45.0\n\n\nem filtros\n\nli = [5, 7, 22, 97, 54, 62, 77, 23, 73, 61]\n\nimpares = list(filter(lambda x: (x % 2 != 0), li))\nprint(impares)\n\n[5, 7, 97, 77, 23, 73, 61]\n\n\nem mapeamentos\n\nli = [5, 7, 22, 97, 54, 62, 77, 23, 73, 61]\n\ndobro = list(map(lambda x: x*2, li))\nprint(dobro)\n\n[10, 14, 44, 194, 108, 124, 154, 46, 146, 122]\n\n\nA função reduce() aplica-se a listas e executa a mesma função a cada dois elementos da lista. aka Função acumuladora\n\nfrom functools import reduce\nli = [10, 5, 40]\n\nconcat = reduce(lambda x, y: str(x) + '.' + str(y), li)\nprint(concat)\n\nli = [10, 20, 50]\nsum = reduce((lambda x, y: x + y), li)\nprint(sum)\n\n10.5.40\n80\n\n\nImmediately Invoked Function Execution\n\nresultado = (lambda x: x + 1)(2)\nprint(resultado)\n\n3"
  },
  {
    "objectID": "700-mod7.html#excepções",
    "href": "700-mod7.html#excepções",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.7 Excepções",
    "text": "7.7 Excepções\n\n\nx = 2\ny = \"a\"\n\ntry:\n    z = x + y\nexcept TypeError:\n    print(\"Erro: Não pode adicionar int com string\")\nexcept:\n    print(\"Erro: Outro erro\")\nelse:\n    print(z) # não houve excepção\nfinally:\n  print(\"Passa sempre aqui\")\n\nErro: Não pode adicionar int com string\nPassa sempre aqui\n\n\ncausar uma excepção\n\ntry:\n    raise NameError(\"Olá\") # Raise\nexcept NameError:\n    print (\"Ocorreu uma excepção\")\n#   raise # Propagar a excepção para \"cima\"\n\n# @title Excepção genérica\n# x = 4\n# if x&lt;10:\n#   raise Exception(\"uma excepção inventada\")\n\n# @title User defined exception\nclass ValorExcessivo(Exception):\n  pass\n\ntry :\n    x = 2000\n    if (x &gt; 100):\n      raise ValorExcessivo\n\n    print(\"O valor é bom\")\nexcept ValorExcessivo:\n  print (\"O valor é excessívo\")\n\nOcorreu uma excepção\nO valor é excessívo\n\n\nboa prática usar as excepções built-in quando possível\n\ntry:\n  x = 2000\n  if (x &gt; 100):\n    raise ValueError(\"O valor é excessívo\")\n\n  print(\"O valor é bom\")\nexcept ValueError as e:\n  print(e)\n\nO valor é excessívo\n\n\nprocurar em Built-in exceptions\n\n7.7.1 Exercícios\n\n# com tratamento de excepções implementa uma função que divide  x por y e em caso de zero devolve uma mensagem de erro\ndef divide(x, y):\n  try:\n    return x/y\n  except ZeroDivisionError:\n    return \"Erro: Divisão por zero\""
  },
  {
    "objectID": "700-mod7.html#zen-of-python",
    "href": "700-mod7.html#zen-of-python",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.8 Zen of Python",
    "text": "7.8 Zen of Python\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n9 Fabulous Python Tricks That Make Your Code More Elegant"
  },
  {
    "objectID": "700-mod7.html#bónus",
    "href": "700-mod7.html#bónus",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.9 Bónus",
    "text": "7.9 Bónus\n\n# @title Evitar nested loops\n\nlist_a = [1, 2020, 70]\nlist_b = [2, 4, 7, 2000]\nlist_c = [3, 70, 7]\n\n# antes\nfor a in list_a:\n    for b in list_b:\n        for c in list_c:\n            if a + b + c == 2077:\n                print(a, b, c)\n                \n# depois\nfrom itertools import product\n\nfor a, b, c in product(list_a, list_b, list_c):\n    if a + b + c == 2077:\n        print(a, b, c)\n\n70 2000 7\n70 2000 7\n\n\n\n# @title Operador Morsa (Walrus Operator)\n\n# antes\nauthor = \"Yang\"\nprint(author)\n\n# depois\nprint(author:=\"Yang\")\n\nYang\nYang\n\n\n\n# @title Operador trenário\n\n# antes\nif a&lt;b:\n  min = a\nelse:\n  min = b\n\n# depois\nmin = a if a &lt; b else b\n\n\n# @title usar funções higher-order\n\n# map\nnames = ['maTeMáTiCa', 'iNgLês', 'FísiCa', 'PORTUGUÊS']\nnames = map(str.capitalize, names)\nprint(list(names))\n\n# reduce\nfrom functools import reduce\nlista = ['P', 'y', 't', 'h', 'o', 'n', 2, 0, 2, 3]\nlista_to_str = reduce(lambda x, y: str(x) + str(y), lista)\nprint(lista_to_str)\n\n['Matemática', 'Inglês', 'Física', 'Português']\nPython2023\n\n\n\n# @title union operator\ncities_us = {'New York City': 'US', 'Los Angeles': 'US'}\ncities_uk = {'London': 'UK', 'Birmingham': 'UK'}\n\ncities = cities_us|cities_uk\nprint(cities)\n\n# ainda melhor (se for o que se pretende!)\ncities_us |= cities_uk\nprint(cities_us)\n\n{'New York City': 'US', 'Los Angeles': 'US', 'London': 'UK', 'Birmingham': 'UK'}\n{'New York City': 'US', 'Los Angeles': 'US', 'London': 'UK', 'Birmingham': 'UK'}\n\n\n\n# @title Usar *\nA = [1, 2, 3] # lista\nB = (4, 5, 6) # tuplo\nC = {7, 8, 9} # conjunto\n\nL = [*A, *B, *C]\n\nprint(L)\n\n[1, 2, 3, 4, 5, 6, 8, 9, 7]\n\n\n\n# magia com *\na, *mid, b = [1, 2, 3, 4, 5, 6] # mid é uma lista com os elementos do meio\nprint(a, mid, b)\n\n1 [2, 3, 4, 5] 6"
  },
  {
    "objectID": "700-mod7.html#chatgpt",
    "href": "700-mod7.html#chatgpt",
    "title": "7  Programming Techniques (Advanced)",
    "section": "7.10 chatGPT",
    "text": "7.10 chatGPT\n\n7.10.1 com o github copilot\n\n# write a lambda function in Python that receives 3 lists and returns a list with the elements common to the 3 input lists. Also write code to test the function.\nget_commons = lambda l1, l2, l3: list(set(l1) & set(l2) & set(l3))\n\n# test the function\nl1 = [1, 2, 3, 4, 5]\nl2 = [2, 3, 4, 5, 6]\nl3 = [3, 4, 5, 6, 7]\nprint(get_commons(l1, l2, l3))\n\n[3, 4, 5]\n\n\n\n\n7.10.2 Via API\n\npip install openai\n\n\n# @title Chat\nfrom openai import OpenAI, OpenAIError\nfrom google.colab import userdata\n\ntry:\n\n  client = OpenAI(\n      api_key = userdata.get('OPENAI_APIKEY')\n  )\n\n  completion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"És um poeta e um especialista em estatística. Falas português de Portugal.\"},\n      {\"role\": \"user\", \"content\": \"Compõe um poema com 2 quadras que explique a evolução dos preços do IPC no ano de 2020 em Portugal.\"}\n    ]\n  )\n\n  resposta = completion.choices[0].message\n\n  print(resposta.content)\n\nexcept OpenAIError as e:\n  print(\"erro:\", e)\n\n\n# @title Assistant\nfrom openai import OpenAI\nclient = OpenAI(\n    api_key = userdata.get('OPENAI_APIKEY')\n)\n\n# 1. Criar o assistente\nassistant = client.beta.assistants.create(\n  name=\"Math Tutor\",\n  instructions=\"És um especialista em estatística e professor de Python. Escreve e executa código para responder às perguntas.\",\n  tools=[{\"type\": \"code_interpreter\"}],\n  #model=\"gpt-4-turbo\", caro\n  model=\"gpt-3.5-turbo\",\n)\n\n# 2. Criar um historico da conversa\nthread = client.beta.threads.create()\n\n# 3. Iniciar a conversação\nmessage = client.beta.threads.messages.create(\n  thread_id=thread.id,\n  role=\"user\",\n  content=\"Como estimar a inflação?\"\n)\n\n# 4. Executar o assistente e aguardar a resposta\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler\n\n# First, we create a EventHandler class to define\n# how we want to handle the events in the response stream.\nclass EventHandler(AssistantEventHandler):\n  @override\n  def on_text_created(self, text) -&gt; None:\n    print(f\"\\nassistente &gt; \", end=\"\", flush=True)\n\n  @override\n  def on_text_delta(self, delta, snapshot):\n    print(delta.value, end=\"\", flush=True)\n\n  def on_tool_call_created(self, tool_call):\n    print(f\"\\nassistente &gt; {tool_call.type}\\n\", flush=True)\n\n  def on_tool_call_delta(self, delta, snapshot):\n    if delta.type == 'code_interpreter':\n      if delta.code_interpreter.input:\n        print(delta.code_interpreter.input, end=\"\", flush=True)\n      if delta.code_interpreter.outputs:\n        print(f\"\\n\\resposta &gt;\", flush=True)\n        for output in delta.code_interpreter.outputs:\n          if output.type == \"logs\":\n            print(f\"\\n{output.logs}\", flush=True)\n\n# Then, we use the `stream` SDK helper\n# with the `EventHandler` class to create the Run\n# and stream the response.\n\nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions=\"Responde como se estivesses a falar com um colega de trabalho que acabou de sair da faculdade.\",\n  event_handler=EventHandler(),\n) as stream:\n  stream.until_done()"
  },
  {
    "objectID": "800-mod8.html#knn",
    "href": "800-mod8.html#knn",
    "title": "8  Data Science (Advanced)",
    "section": "8.1 KNN",
    "text": "8.1 KNN\n\nK-nearest neighbors (KNN) depende do número de vizinhos (k) e da medida de distância:\n\neuclidean\nmanhattan\nchebyshev\nminkowski\nseuclidean\nmahalanobis\n\né também comum parametrizar weights (uniform ou distance)\nsklearn.neighbors.KNeighborsClassifier\nO algoritno pode ser: + ball_tree + kd_tree + brute + auto\nExemplo\ngerar dados de exemplo:\n\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\ncenters = [[2, 3], [5, 5], [1, 8]]\nn_classes = len(centers)\ndata, labels = make_blobs(n_samples=150, \n                          centers=np.array(centers),\n                          random_state=1)\n\n\nimport matplotlib.pyplot as plt\n\n# define cores                          \ncolours = ('green', 'red', 'blue')\n\nfig, ax = plt.subplots()\nfor n_class in range(0, n_classes):\n    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n               c=colours[n_class], s=10, label=str(n_class))\n\nax.legend(loc='upper right')\n\nplt.show()\n\n\n\n\ndividir os dados em teste e treino\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels,\n                                                    train_size=0.8, test_size=0.2, random_state=1)\n\n\ny_train\n\narray([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2,\n       2, 2, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1, 2, 2, 0, 1, 1, 2, 2, 1, 2, 0,\n       0, 1, 0, 0, 0, 2, 0, 2, 0, 1, 2, 1, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       2, 2, 2, 1, 1, 1, 2, 2, 0, 2, 1, 0, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0,\n       1, 0, 0, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2, 1, 1, 1, 0, 2, 2, 0, 0, 2,\n       2, 1, 1, 1, 1, 2, 0, 0, 2, 1])\n\n\n\n# Create and fit a nearest-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# a instância do KNN está a ser criada sem parâmetros\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train) \n\ny_pred = knn.predict(X_test)\nprint(\"Previsões do classificador:\")\nprint(y_pred)\nprint(\"Target:\")\nprint(y_test)\n\nPrevisões do classificador:\n[2 2 2 0 0 1 1 2 2 1 0 1 0 0 2 0 0 0 1 0 0 1 1 2 0 0 0 1 2 1]\nTarget:\n[2 2 2 0 0 1 1 2 2 1 0 1 0 0 2 0 0 0 1 0 0 1 1 2 0 0 0 1 2 1]\n\n\n\nfrom sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y_pred, y_test))\n\n1.0\n\n\nexemplo ad hoc para calcular accuracy\n\nfrom sklearn.metrics import accuracy_score\n\nexample_predictions = [0, 2, 1, 3, 2, 0, 1]\nexample_labels      = [0, 1, 2, 3, 2, 1, 1]\nprint(accuracy_score(example_predictions, example_labels))\n\n0.5714285714285714\n\n\n\n8.1.1 KNN com Dataset Iris\n\nfrom sklearn import datasets\n#from sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\ndata, labels = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.7, test_size=0.3, random_state=1)\n\n\n#import matplotlib.pyplot as plt\n\ncolours = ('purple', 'red', 'blue')\nn_classes = 3\n\nfig, ax = plt.subplots()\nfor n_class in range(0, n_classes):\n    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n               c=colours[n_class], s=10, label=str(n_class))\n\nax.legend(loc='upper right')\n\nplt.show()\n\n\n\n\n\n# Create and fit a nearest-neighbor classifier\n# sklearn.neighbors import KNeighborsClassifier\n\n# instanciação com parâmetros\nknn2 = KNeighborsClassifier(algorithm='auto', \n                           leaf_size=30, \n                           metric='minkowski',\n                           p=2,\n                           metric_params=None, \n                           n_jobs=1, \n                           n_neighbors=3, \n                           weights='uniform')\n                           \nknn2.fit(X_train, y_train) \n\ny_pred = knn2.predict(X_test)\nprint(\"Previsões do classificador:\")\nprint(y_pred)\nprint(\"Target:\")\nprint(y_test)                           \n\nPrevisões do classificador:\n[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n 0 1 2 2 0 1 2 1]\nTarget:\n[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n 0 1 2 2 0 2 2 1]\n\n\n\n#from sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y_pred, y_test))\n\n0.9777777777777777\n\n\n\ny_test_proba = knn2.predict_proba(X_test)\ny_test_proba[:10]\n\narray([[1.        , 0.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.66666667, 0.33333333],\n       [0.        , 0.        , 1.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [0.        , 0.        , 1.        ]])\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\ncm_thr50 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr50,columns = ['pred: 0','pred: 1','pred: 2'],\n                   index = ['real: 0','real: 1','real: 2']))\n\n         pred: 0  pred: 1  pred: 2\nreal: 0       14        0        0\nreal: 1        0       18        0\nreal: 2        0        1       12\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\n# por omissão average='binary' mas pode também ser None 'micro', 'macro' e 'weighted'\nprint('Precision: %.3f' % precision_score(y_test, y_pred, average='micro'))\nprint('Recall: %.3f' % recall_score(y_test, y_pred, average='micro'))\nprint('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n\nPrecision: 0.978\nRecall: 0.978\nAccuracy: 0.978\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(\n    f\"Classification report for classifier {knn2}:\\n\"\n    f\"{classification_report(y_test, y_pred)}\\n\"\n)\n\nClassification report for classifier KNeighborsClassifier(n_jobs=1, n_neighbors=3):\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        14\n           1       0.95      1.00      0.97        18\n           2       1.00      0.92      0.96        13\n\n    accuracy                           0.98        45\n   macro avg       0.98      0.97      0.98        45\nweighted avg       0.98      0.98      0.98        45\n\n\n\n\n\n\n8.1.2 Avaliação do modelo\n\n\n\n\n\n\n\n\nBias / Variance trade-off\n\\(Total Error = Bias^2 + Variance + Irreductible Error\\)\n Exemplo\nCriar dois grupos e prever o grupo a que pertence cada observação de acordo com os seus vizinhos:\n\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncenters = [[2, 3], [5, 5]]\nn_classes = len(centers)\ndata, labels = make_blobs(n_samples=200, \n                          centers=np.array(centers),\n                          random_state=1)\n\n\nimport matplotlib.pyplot as plt\n\ncolours = ('orange', 'blue')\n# n_classes = 2\n\nfig, ax = plt.subplots()\nfor n_class in range(0, n_classes):\n    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n               c=colours[n_class], s=10, label=str(n_class))\n\nax.legend(loc='upper right')\n\nplt.show()\n\n\n\n\ndividir os dados em treino e teste:\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.8, test_size=0.2, random_state=1)\n\nmodelar para diferentes K:\n\n# Create and fit a nearest-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\n# vamos aumentar a complexidade aumentando o nº de vizinhos\nk_range = range(1,20)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores[k] = metrics.accuracy_score(y_test, y_pred)\n    scores_list.append(metrics.accuracy_score(y_test, y_pred))\n    \nprint(*scores_list, sep = \", \")\n\n0.9, 0.875, 0.9, 0.925, 0.95, 0.925, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95\n\n\n\nimport matplotlib.pyplot as plt\n\n# vamos fazer o plot da relação entre o aumento do nº de vizinhos k\n# e o valor da accuracy medido nos dados de Teste\nplt.plot(k_range, scores_list)\nplt.xlabel(\"Valor de k no KNN\")\nplt.ylabel(\"Testing Accuracy\")\nplt.show()\n\n\n\n\n\n# parece não haver ganhos a partir dos 8 vizinhos\nknn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X_train, y_train) \n\nclasses = {0: 'doces', 1: 'salgados'}\n\nx_new = [[5.88213357, 3.75041164], [1.6546771, 3.6924546 ]]\ny_predict = knn.predict(x_new)\ny_predict_prob = knn.predict_proba(x_new)\n\nprint('O elemento {0} gosta mais de {1}'.format(x_new[0],classes[y_predict[0]]))\nprint('O elemento {0} gosta mais de {1}'.format(x_new[1],classes[y_predict[1]]))\n\nO elemento [5.88213357, 3.75041164] gosta mais de salgados\nO elemento [1.6546771, 3.6924546] gosta mais de doces\n\n\n\n\n8.1.3 Matriz de confusão\nmodelo KNN com K = 6\n\n# Create and fit a nearest-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nknn = KNeighborsClassifier(n_neighbors = 6)\n    \nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nscore = metrics.accuracy_score(y_test, y_pred)\nscore\n\n0.925\n\n\nprobabilidades previstas para os positivos (2ª coluna)\n\nknn.predict_proba(X_test)[:, 1]\n\narray([1.        , 0.16666667, 0.66666667, 0.        , 1.        ,\n       1.        , 1.        , 0.5       , 0.        , 0.        ,\n       1.        , 1.        , 1.        , 1.        , 1.        ,\n       0.16666667, 1.        , 1.        , 1.        , 0.16666667,\n       0.        , 0.66666667, 0.        , 1.        , 1.        ,\n       1.        , 0.16666667, 1.        , 0.        , 0.83333333,\n       1.        , 0.        , 1.        , 1.        , 0.        ,\n       1.        , 0.33333333, 0.        , 1.        , 0.16666667])\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)\n\narray([[14,  1],\n       [ 2, 23]], dtype=int64)\n\n\ncom labels\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n           pred: No  pred: Yes\nreal: No         14          1\nreal: Yes         2         23\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\nprint('Precision: %.3f' % precision_score(y_test, y_pred))\nprint('Recall: %.3f' % recall_score(y_test, y_pred))\nprint('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n\nPrecision: 0.958\nRecall: 0.920\nAccuracy: 0.925\n\n\ncálculo “manual”:\n\n# Accuracy = TP + TN / P + N\nf_acc = (23 + 14)/ (( 2 + 23) + (14 + 1))\nf_acc\n\n# Recall ou sensitivity ou TPR - linha yes da matriz\n# Dos que são positivos quantos previu?\n# TPR = TP / P\nf_recall = 23 / 25\nf_recall\n\n0.92\n\n\n\n# Precision - coluna yes da matriz\n# Dos que previu como positivos quantos eram realmente positivos? \n# Precision = TP / TP + FP\nf_precision = 23 / (23 + 1)\nf_precision\n\n# Specificity ou Selectivity ou TNR - Linha no da matriz\n# TNR = TN / N \nf_specificity = 14 / (14 + 1)\nf_specificity\n\n0.9333333333333333\n\n\n\n\n8.1.4 ROC e AUC\nReceiver Operating Characteristics (ROC) e Area Under the Curve (AUC)\n\nmodel = knn.fit(X_train, y_train)\n\nfrom sklearn.metrics import roc_curve, RocCurveDisplay\n\nfpr, tpr, thresholds = roc_curve(y_test,model.predict_proba(X_test)[:,1], # \n                                 drop_intermediate=False)\n\nknn_disp = RocCurveDisplay.from_estimator(model, X_train, y_train)\nroc_disp = RocCurveDisplay.from_estimator(model, X_test, y_test, ax=knn_disp.ax_) # o ax diz que este novo objecto fica no objecto anterior\n\nplt.show()\n\n\n\n\n\nthresholds\n\narray([       inf, 1.        , 0.83333333, 0.66666667, 0.5       ,\n       0.33333333, 0.16666667, 0.        ])\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n           pred: No  pred: Yes\nreal: No         14          1\nreal: Yes         2         23\n\n\n\n# FPR = FP / N\nFPR_50 = 1 / 15\nFPR_50\n\n0.06666666666666667\n\n\n\n# TPR = TP / P\nTPR_50 = 23 / 25\nTPR_50\n\n0.92\n\n\n\nthreshold = 0.33\ny_pred = (model.predict_proba(X_test)[:, 1] &gt; threshold)\n\ncm_thr33 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr33,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n           pred: No  pred: Yes\nreal: No         13          2\nreal: Yes         1         24\n\n\n\n# FPR = FP / N\nFPR_33 = 2 / 15\n# TPR = TP / P\nTPR_33 = 24 / 25\nprint('O FPR é {0} quando o TPR é {1}' . format(FPR_33,TPR_33))\n\nO FPR é 0.13333333333333333 quando o TPR é 0.96\n\n\n\nthreshold = 0.83\ny_pred = (model.predict_proba(X_test)[:, 1] &gt; threshold)\n\ncm_thr83 = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm_thr83,columns = ['pred: No','pred: Yes'],\n                   index = ['real: No','real: Yes']))\n\n           pred: No  pred: Yes\nreal: No         14          1\nreal: Yes         4         21\n\n\n\n# FPR = FP / N\nFPR_83 = 1 / 15\n# TPR = TP / P\nTPR_83 = 21 / 25\nprint('O FPR é {0} quando o TPR é {1}' . format(FPR_83,TPR_83))\n\nO FPR é 0.06666666666666667 quando o TPR é 0.84\n\n\n\nknn_disp = RocCurveDisplay.from_estimator(model, X_test, y_test)\n\nplt.show()"
  },
  {
    "objectID": "800-mod8.html#regressão-linear",
    "href": "800-mod8.html#regressão-linear",
    "title": "8  Data Science (Advanced)",
    "section": "8.2 Regressão Linear",
    "text": "8.2 Regressão Linear\nProblemas mais comuns:\n\nrelação não linear entre a resposta e predictor\ncorrelação dos termos de erro\nvariância não constante dos erros\noutliers\npontos com high-leverage\ncolinearidade\n\n\nimport numpy as np\nimport pandas as pd\n\ndatadir =\"data\\\\\"\nfilename = \"df_prep.csv\"\n\ndf_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf_hosp.head()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_19764\\1222662527.py:7: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df_hosp = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\nt_cirurgia\n\n\nANO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012.0\n229.0\n17.0\n1458.0\n247.0\n159.00\n2.0\n11.0\n0.0\n5.0\n0.0\n...\n39.0\n8.0\n19.0\n2.0\n2.0\n3.0\n5.0\n0.0\n454.0\nyes\n\n\n2012.0\n206.0\n17.0\n144.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n46.0\nno\n\n\n2012.0\n65.0\n16.0\n894.0\n111.0\n38.25\n0.0\n8.0\n0.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n337.0\nyes\n\n\n2012.0\n106.0\n17.0\n801.0\n166.0\n108.00\n0.0\n12.0\n0.0\n7.0\n0.0\n...\n36.0\n4.0\n16.0\n5.0\n2.0\n2.0\n7.0\n0.0\n228.0\nyes\n\n\n2012.0\n209.0\n11.0\n221.0\n13.0\n13.00\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n8.0\n0.0\n0.0\n1.0\n0.0\n1.0\n6.0\n0.0\n80.0\nyes\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ndf_hosp.describe()\n\n\n\n\n\n\n\n\nNORDEM\nNUTS2\nC10001\nC20001\nC21001\nC21011\nC21021\nC21031\nC21041\nC21061\n...\nC30001\nC31001\nC31011\nC31021\nC31031\nC31041\nC31051\nC31061\nC31071\nC32001\n\n\n\n\ncount\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n...\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n86.000000\n\n\nmean\n118.081395\n14.976744\n642.744186\n113.872093\n73.979651\n0.813953\n6.941860\n0.360465\n2.953488\n0.302326\n...\n212.290698\n22.941860\n3.127907\n6.220930\n4.476744\n3.465116\n0.965116\n3.465116\n1.220930\n189.348837\n\n\nstd\n68.264230\n3.620129\n809.928453\n172.413511\n108.256016\n2.144825\n10.384779\n1.146994\n5.346592\n1.701693\n...\n255.792649\n26.911775\n4.752204\n10.678622\n5.740930\n8.559409\n1.482825\n4.421066\n2.783993\n240.396475\n\n\nmin\n3.000000\n11.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n57.500000\n11.000000\n120.250000\n7.750000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n34.250000\n3.250000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n30.000000\n\n\n50%\n119.000000\n16.000000\n338.500000\n42.500000\n33.000000\n0.000000\n2.000000\n0.000000\n0.000000\n0.000000\n...\n102.500000\n10.000000\n0.000000\n0.000000\n3.000000\n1.000000\n0.000000\n2.000000\n0.000000\n85.500000\n\n\n75%\n175.750000\n17.000000\n872.500000\n162.500000\n115.500000\n0.000000\n10.750000\n0.000000\n3.750000\n0.000000\n...\n321.250000\n35.750000\n4.000000\n11.000000\n6.000000\n3.000000\n1.000000\n5.000000\n1.000000\n282.250000\n\n\nmax\n229.000000\n30.000000\n5325.000000\n1161.000000\n719.000000\n13.000000\n51.000000\n6.000000\n31.000000\n13.000000\n...\n1515.000000\n103.000000\n21.000000\n50.000000\n28.000000\n72.000000\n6.000000\n19.000000\n15.000000\n1515.000000\n\n\n\n\n8 rows × 61 columns\n\n\n\n\n# passa indices para coluna\ndf_hosp = df_hosp.reset_index()\n\n# deixar cair a coluna 't_cirurgia'\ndf_hosp = df_hosp.drop(columns=['t_cirurgia']) \n\n\n# criação de matriz de correlação e selecão do triângulo superior\ncor_matrix = df_hosp.corr().abs()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n\n# seleciona as colunas a remover por serem colunas altamente correlacionadas\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] &gt; 0.9)]\nprint(to_drop)\n\ndf_hosp = df_hosp.drop(columns=to_drop, axis=1)\n\n['C20001', 'C21001', 'C21021', 'C21071', 'C21251', 'C21421', 'C21431', 'C23001', 'C24001', 'C30001', 'C32001']\n\n\n\n# seleciona para remover as colunas ano e ordem\n# que não trazem informação e queremos evitar que ruído seja aprendido\nto_drop = ['ANO','NORDEM']\ndf = df_hosp.drop(columns=to_drop, axis=1)\n\n# excluimos os registos com valores vazios\ndf1 = df.dropna()\n\n# define a variável target e as features\nX = df1.drop(columns=['C31011']) \ny = df1['C31011'].values # array com os valores\n\n## Typecast da coluna para categórica em pandas\nX['NUTS2'] = pd.Categorical(X.NUTS2)\n\nX.shape\n\n# cria variáveis dummy e faz drop da baseline\nX = pd.get_dummies(X, drop_first = True)\n\n# O nº de colunas aumentou por causa das dummy variables\nX.shape\n\n(86, 53)\n\n\ndivide os dados em treino e teste\n\nfrom sklearn.model_selection import train_test_split \n\n#Split data for machine learning\nX_train, X_test, y_train, y_test = train_test_split(X,  y, test_size = 0.2 ,random_state = 2002)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(68, 53)\n(18, 53)\n\n\nfaz o modelo\n\nfrom sklearn.linear_model import LinearRegression \n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\ncalcula o R^2\n\ny_pred = lr.predict(X_test)\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n1.0\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()"
  },
  {
    "objectID": "800-mod8.html#regressão-logística",
    "href": "800-mod8.html#regressão-logística",
    "title": "8  Data Science (Advanced)",
    "section": "8.3 Regressão logística",
    "text": "8.3 Regressão logística\npara prever um valor contínuo\n\nfrom sklearn.linear_model import LogisticRegression\n\n# sem qualquer parâmetro\nlogisticRegr = LogisticRegression()\n\nlogisticRegr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\ny_pred = logisticRegr.predict(X_test)\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n-0.025256923880856874\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred)\nplt.xlim(0, 8)\nplt.ylim(0, 8)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\npara classificar\n\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\n\n# Vamos ver que há 1797 images (imagens 8 por 8 com uma dimensionalidade de 64)\nprint('Image Data Shape' , digits.data.shape)\n# Print to show there are 1797 labels (integers from 0–9)\nprint(\"Label Data Shape\", digits.target.shape)\n\nImage Data Shape (1797, 64)\nLabel Data Shape (1797,)\n\n\nvisualizar algumas imagens\n\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n plt.subplot(1, 5, index + 1)\n # imshow() função dó módulo pyplot module do matplotlib para mostrar dados como imagem; i.e. num raster 2D.\n plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n plt.title('Training: %i\\n' % label, fontsize = 20)\n \nplt.show()\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=2023)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# sem qualquer parâmetro\nlogisticRegr = LogisticRegression()\n\nlogisticRegr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\nprevisão da categoria/classe\n\n# Retorna um Array NumPy \n# Prevê para um único caso (imagem)\nprv1 = logisticRegr.predict(X_test[0].reshape(1,-1))\nprv1\n\narray([5])\n\n\nvisualizar a imagem que foi prevista\n\n# vamos ver a imagem\nplt.figure(figsize=(20,4))\nplt.imshow(np.reshape(X_test[0], (8,8)), cmap=plt.cm.gray)\n\nplt.show()\n\n\n\n\n\n# probabilidade de que seja um 5 é de 9.93529410e-01\nprob1 = logisticRegr.predict_proba(X_test[0].reshape(1,-1))\nprob1\n\narray([[1.35477982e-03, 5.16378720e-05, 3.71852850e-13, 9.59922777e-04,\n        6.82878887e-08, 9.93529949e-01, 3.91881179e-03, 1.30613074e-04,\n        5.23838775e-05, 1.83393892e-06]])\n\n\n\nlogisticRegr.coef_\n\narray com as previões\n\ny_pred = logisticRegr.predict(X_test)\ny_pred\n\nconfusion matrix 9x9\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n\n#cm = metrics.confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nall_sample_title = 'Accuracy Score: {0}'.format(metrics.accuracy_score(y_pred, y_test))\nplt.title(all_sample_title, size = 15)\n\nplt.show()"
  },
  {
    "objectID": "800-mod8.html#regularização",
    "href": "800-mod8.html#regularização",
    "title": "8  Data Science (Advanced)",
    "section": "8.4 Regularização",
    "text": "8.4 Regularização\nregularizações que penalizam os coeficientes grandes\n\n8.4.1 Ridge Regression\n\\(Função Perda = OLS + \\lambda \\times slope^2\\)\nA recta da ridge regressioné menos inclinada dos que a recta da regressão linear, ou seja, as previsões são menos sensíveis ao predictor (processo de dessensitização dos predictores)\n\n#Importa desta feita os datasets\nfrom sklearn import datasets\n\n#Load dataset de diabetes\ndiabetes_X,diabetes_y = datasets.load_diabetes(return_X_y = True , as_frame = True)\n\n# Colunas que temos nos dados\ndiabetes_X.keys\n\n&lt;bound method NDFrame.keys of           age       sex       bmi        bp        s1        s2        s3  \\\n0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n..        ...       ...       ...       ...       ...       ...       ...   \n437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n\n           s4        s5        s6  \n0   -0.002592  0.019907 -0.017646  \n1   -0.039493 -0.068332 -0.092204  \n2   -0.002592  0.002861 -0.025930  \n3    0.034309  0.022688 -0.009362  \n4   -0.002592 -0.031988 -0.046641  \n..        ...       ...       ...  \n437 -0.002592  0.031193  0.007207  \n438  0.034309 -0.018114  0.044485  \n439 -0.011080 -0.046883  0.015491  \n440  0.026560  0.044529 -0.025930  \n441 -0.039493 -0.004222  0.003064  \n\n[442 rows x 10 columns]&gt;\n\n\n\ndiabetes_y\n\n0      151.0\n1       75.0\n2      141.0\n3      206.0\n4      135.0\n       ...  \n437    178.0\n438    104.0\n439    132.0\n440    220.0\n441     57.0\nName: target, Length: 442, dtype: float64\n\n\n\n# print data(feature)shape\ndiabetes_X.shape\n\n(442, 10)\n\n\ndividir os dados\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.3,random_state=2510) \n\nX_test\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\n\n\n\n\n24\n-0.063635\n-0.044642\n0.035829\n-0.022885\n-0.030464\n-0.018850\n-0.006584\n-0.002592\n-0.025953\n-0.054925\n\n\n241\n0.030811\n0.050680\n-0.008362\n0.004658\n0.014942\n0.027496\n0.008142\n-0.008127\n-0.029526\n0.056912\n\n\n121\n0.063504\n-0.044642\n0.017506\n0.021872\n0.008063\n0.021546\n-0.036038\n0.034309\n0.019907\n0.011349\n\n\n138\n0.034443\n0.050680\n0.111276\n0.076958\n-0.031840\n-0.033881\n-0.021311\n-0.002592\n0.028020\n0.073480\n\n\n90\n0.012648\n-0.044642\n-0.025607\n-0.040099\n-0.030464\n-0.045155\n0.078093\n-0.076395\n-0.072133\n0.011349\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n70\n-0.001882\n-0.044642\n-0.069797\n-0.012556\n-0.000193\n-0.009143\n0.070730\n-0.039493\n-0.062917\n0.040343\n\n\n163\n0.016281\n0.050680\n0.072474\n0.076958\n-0.008449\n0.005575\n-0.006584\n-0.002592\n-0.023647\n0.061054\n\n\n328\n-0.038207\n-0.044642\n0.067085\n-0.060756\n-0.029088\n-0.023234\n-0.010266\n-0.002592\n-0.001496\n0.019633\n\n\n333\n0.027178\n0.050680\n-0.006206\n0.028758\n-0.016704\n-0.001627\n-0.058127\n0.034309\n0.029297\n0.032059\n\n\n181\n0.048974\n-0.044642\n-0.042852\n-0.053870\n0.045213\n0.050042\n0.033914\n-0.002592\n-0.025953\n-0.063209\n\n\n\n\n133 rows × 10 columns\n\n\n\nNa função de perda \\(\\alpha\\) é o parâmetro que precisamos selecionar. Um valor \\(\\alpha\\) baixo pode levar a um ajuste excessivo, enquanto um valor \\(\\alpha\\) alto pode levar a um ajuste insuficiente.\n\nfrom sklearn.linear_model import Ridge\n\n# instancia o modelo de regressão Ridge\n# com um valor alfa \nmodel_ridge = Ridge(alpha=0.01)\n\nmodel_ridge.fit(X_train, y_train) \n\ny_pred_ridgetrain= model_ridge.predict(X_train)\n\ny_pred_ridgetest = model_ridge.predict(X_test)\n\ny_pred_ridgetest[0:3]\n\narray([158.83215558, 121.16021195, 187.58510908])\n\n\n\ny_test[0:3]\n\n24     184.0\n241    177.0\n121    173.0\nName: target, dtype: float64\n\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('RMSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_ridgetrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_ridgetrain)))\n\nprint('RMSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_ridgetest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_ridgetest)))\n\nRMSE nos dados de Treino: 53.83174171338626\nR2 nos dados de Treino: 0.5019381528297013\nRMSE nos dados de Teste: 54.68294795042933\nR2 nos dados de Teste: 0.5032327253095996\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_ridgetest)\nplt.xlim(0, 400)\nplt.ylim(0, 400)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\nExemplo com ficheiro Hitters\n\ndatadir =\"data\\\\\"\nfilename = \"Hitters.csv\"\n\ndf = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\ndf.head()\n\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_19764\\2911689644.py:4: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  df = pd.read_csv(f\"{datadir}{filename}\", index_col=0, verbose = False, encoding='latin-1')\n\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n-Andy Allanson\n293\n66\n1\n30\n29\n14\n1\n293\n66\n1\n30\n29\n14\nA\nE\n446\n33\n20\nNaN\nA\n\n\n-Alan Ashby\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n-Alvin Davis\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n-Andre Dawson\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n-Andres Galarraga\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n\n\n\n\n\n\ndf.reset_index()\n\n\n\n\n\n\n\n\nindex\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\n...\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n0\n-Andy Allanson\n293\n66\n1\n30\n29\n14\n1\n293\n66\n...\n30\n29\n14\nA\nE\n446\n33\n20\nNaN\nA\n\n\n1\n-Alan Ashby\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n...\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n-Alvin Davis\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n...\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n-Andre Dawson\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n...\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n-Andres Galarraga\n321\n87\n10\n39\n42\n30\n2\n396\n101\n...\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n317\n-Willie McGee\n497\n127\n7\n65\n48\n37\n5\n2703\n806\n...\n379\n311\n138\nN\nE\n325\n9\n3\n700.0\nN\n\n\n318\n-Willie Randolph\n492\n136\n5\n76\n50\n94\n12\n5511\n1511\n...\n897\n451\n875\nA\nE\n313\n381\n20\n875.0\nA\n\n\n319\n-Wayne Tolleson\n475\n126\n3\n61\n43\n52\n6\n1700\n433\n...\n217\n93\n146\nA\nW\n37\n113\n7\n385.0\nA\n\n\n320\n-Willie Upshaw\n573\n144\n9\n85\n60\n78\n8\n3198\n857\n...\n470\n420\n332\nA\nE\n1314\n131\n12\n960.0\nA\n\n\n321\n-Willie Wilson\n631\n170\n9\n77\n44\n31\n11\n4908\n1457\n...\n775\n357\n249\nA\nW\n408\n4\n3\n1000.0\nA\n\n\n\n\n322 rows × 21 columns\n\n\n\n\n# Vamos descobrir em que colunas há valores em falta\n[col for col in df.columns if df[col].isnull().sum()&gt;0]\n\n# só a coluna salary tem valores em falta\ndf.dropna(inplace = True)\n\n\n# vamos ver que colunas são do tipo Object\nqual_vars = [col for col in df.columns if df[col].dtype == 'object']\nprint(qual_vars)\n\n# e criar variáveis dummy para as nossas variáveis categóricas\ndf = pd.get_dummies(df,columns= qual_vars,drop_first=True)\ndf.head()\n\n['League', 'Division', 'NewLeague']\n\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nPutOuts\nAssists\nErrors\nSalary\nLeague_N\nDivision_W\nNewLeague_N\n\n\n\n\n-Alan Ashby\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\n632\n43\n10\n475.0\nTrue\nTrue\nTrue\n\n\n-Alvin Davis\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\n880\n82\n14\n480.0\nFalse\nTrue\nFalse\n\n\n-Andre Dawson\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\n200\n11\n3\n500.0\nTrue\nFalse\nTrue\n\n\n-Andres Galarraga\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\n805\n40\n4\n91.5\nTrue\nFalse\nTrue\n\n\n-Alfredo Griffin\n594\n169\n4\n74\n51\n35\n11\n4408\n1133\n19\n501\n336\n194\n282\n421\n25\n750.0\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n# Separamos a coluna target das outras\nX = df.drop('Salary',axis = 1)\ny = df['Salary']\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Como vemos nas colunas grandes diferenças de escala vamos standardizar\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=2510) \nprint(X_train.shape)\nprint(y_test.shape)\n\n(184, 19)\n(79,)\n\n\n\nfrom sklearn.linear_model import Ridge\n\n# instanciar o modelo de regressão Ridge\n# com um valor alfa \nmodel_ridge = Ridge(alpha=0.01)\n\nmodel_ridge.fit(X_train, y_train) \n\nRidge(alpha=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Ridge?Documentation for RidgeiFittedRidge(alpha=0.01) \n\n\nver os primeiros resultados\n\ny_pred_ridgetrain= model_ridge.predict(X_train)\n\ny_pred_ridgetest= model_ridge.predict(X_test)\n\ny_pred_ridgetest[0:3]\n\ny_test[0:3]\n\n-Frank White        750.0\n-Marty Barrett      575.0\n-Argenis Salazar    100.0\nName: Salary, dtype: float64\n\n\navaliar o modelo\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('MSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_ridgetrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_ridgetrain)))\n\nprint('MSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_ridgetest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_ridgetest)))\n\nMSE nos dados de Treino: 300.46906944485323\nR2 nos dados de Treino: 0.5539698178486953\nMSE nos dados de Teste: 344.04002446717914\nR2 nos dados de Teste: 0.4164667980776531\n\n\nvisualizar a comparação do previsto com o real\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_ridgetest)\nplt.xlim(0, 1000)\nplt.ylim(0, 1000)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\n\n8.4.1.1 explorar o alpha\n\n\narray([1.00000000e-02, 1.32194115e-02, 1.74752840e-02, 2.31012970e-02,\n       3.05385551e-02, 4.03701726e-02, 5.33669923e-02, 7.05480231e-02,\n       9.32603347e-02, 1.23284674e-01, 1.62975083e-01, 2.15443469e-01,\n       2.84803587e-01, 3.76493581e-01, 4.97702356e-01, 6.57933225e-01,\n       8.69749003e-01, 1.14975700e+00, 1.51991108e+00, 2.00923300e+00,\n       2.65608778e+00, 3.51119173e+00, 4.64158883e+00, 6.13590727e+00,\n       8.11130831e+00, 1.07226722e+01, 1.41747416e+01, 1.87381742e+01,\n       2.47707636e+01, 3.27454916e+01, 4.32876128e+01, 5.72236766e+01,\n       7.56463328e+01, 1.00000000e+02, 1.32194115e+02, 1.74752840e+02,\n       2.31012970e+02, 3.05385551e+02, 4.03701726e+02, 5.33669923e+02,\n       7.05480231e+02, 9.32603347e+02, 1.23284674e+03, 1.62975083e+03,\n       2.15443469e+03, 2.84803587e+03, 3.76493581e+03, 4.97702356e+03,\n       6.57933225e+03, 8.69749003e+03, 1.14975700e+04, 1.51991108e+04,\n       2.00923300e+04, 2.65608778e+04, 3.51119173e+04, 4.64158883e+04,\n       6.13590727e+04, 8.11130831e+04, 1.07226722e+05, 1.41747416e+05,\n       1.87381742e+05, 2.47707636e+05, 3.27454916e+05, 4.32876128e+05,\n       5.72236766e+05, 7.56463328e+05, 1.00000000e+06, 1.32194115e+06,\n       1.74752840e+06, 2.31012970e+06, 3.05385551e+06, 4.03701726e+06,\n       5.33669923e+06, 7.05480231e+06, 9.32603347e+06, 1.23284674e+07,\n       1.62975083e+07, 2.15443469e+07, 2.84803587e+07, 3.76493581e+07,\n       4.97702356e+07, 6.57933225e+07, 8.69749003e+07, 1.14975700e+08,\n       1.51991108e+08, 2.00923300e+08, 2.65608778e+08, 3.51119173e+08,\n       4.64158883e+08, 6.13590727e+08, 8.11130831e+08, 1.07226722e+09,\n       1.41747416e+09, 1.87381742e+09, 2.47707636e+09, 3.27454916e+09,\n       4.32876128e+09, 5.72236766e+09, 7.56463328e+09, 1.00000000e+10])\n\n\nvizualizar o valor dos coeficientes para cada alpha. Quanto maior é o alpha menor é o valor dos coeficientes\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n3.274549e+09\n0.000014\n0.000016\n0.000012\n0.000015\n0.000016\n0.000016\n0.000014\n0.000019\n0.000020\n0.000019\n0.000020\n0.000021\n0.000018\n0.000011\n9.198538e-07\n-1.953073e-07\n-5.164761e-07\n-0.000007\n-1.025022e-07\n\n\n4.328761e+09\n0.000011\n0.000012\n0.000009\n0.000011\n0.000012\n0.000012\n0.000011\n0.000014\n0.000015\n0.000014\n0.000015\n0.000016\n0.000013\n0.000008\n6.958357e-07\n-1.477427e-07\n-3.906955e-07\n-0.000005\n-7.753936e-08\n\n\n5.722368e+09\n0.000008\n0.000009\n0.000007\n0.000009\n0.000009\n0.000009\n0.000008\n0.000011\n0.000011\n0.000011\n0.000012\n0.000012\n0.000010\n0.000006\n5.263742e-07\n-1.117620e-07\n-2.955470e-07\n-0.000004\n-5.865581e-08\n\n\n7.564633e+09\n0.000006\n0.000007\n0.000005\n0.000007\n0.000007\n0.000007\n0.000006\n0.000008\n0.000009\n0.000008\n0.000009\n0.000009\n0.000008\n0.000005\n3.981828e-07\n-8.454381e-08\n-2.235706e-07\n-0.000003\n-4.437104e-08\n\n\n1.000000e+10\n0.000005\n0.000005\n0.000004\n0.000005\n0.000005\n0.000005\n0.000005\n0.000006\n0.000007\n0.000006\n0.000007\n0.000007\n0.000006\n0.000004\n3.012107e-07\n-6.395428e-08\n-1.691230e-07\n-0.000002\n-3.356510e-08\n\n\n\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\nwarnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n\nplt.figure(figsize = (16,8))\nsns.lineplot(data = tmp, dashes=False)\nplt.axhline(y = 0,linestyle = 'dashed',lw = 0.1,color = 'black')\nplt.xscale('log')\nplt.xticks(list_alpha)\nplt.ylabel('Coeffients',fontsize = 14)\nplt.xlabel('Alpha')\nplt.legend(loc='right')\n\nplt.show()\n\n\n\n\nalguns alphas\n\nprint(list_alpha[0])\nprint(list_alpha[3])\nprint(list_alpha[97])\nprint(list_alpha[99])\n\n0.01\n0.023101297000831605\n5722367659.35022\n10000000000.0\n\n\n\n# para um valor baixo de alpha os coeficientes são elevados\nprint(coeff_matrix[list_alpha[8]])\n\n# para um valor elevado de alpha os coeficientes são próximos de zero\n# mas não realmente zero\nprint(coeff_matrix[list_alpha[90]])\n\n[-289.5186532702079, 332.67911842340266, 34.77944193671617, -55.84478229633608, -24.166567449874282, 133.80205477153825, -19.909741728270834, -369.7426875016733, 100.54237339882312, -3.614039922952728, 454.2056856600365, 241.57214343743524, -209.5884504988336, 78.76557723286052, 53.01946396095578, -22.42328507878044, 31.27853944423717, -58.67555869272159, -12.638577929628285]\n[5.763318248704298e-05, 6.40427770922997e-05, 5.007915350288836e-05, 6.129576852892078e-05, 6.561689194262846e-05, 6.480083916119167e-05, 5.849247359810473e-05, 7.681124525639192e-05, 8.013609785207737e-05, 7.663536669557499e-05, 8.214613025072635e-05, 8.277213810215086e-05, 7.150981326993898e-05, 4.386760495701926e-05, 3.71346122622258e-06, -7.884609239667501e-07, -2.08500226533533e-06, -2.810549748199536e-05, -4.13788785581499e-07]\n\n\n\n# linalg.norm(x, ord=None, axis=None, keepdims=False)\n# Calcula a norma matricial ou vetorial, dependendo do ord, 8 normas diferentes\n# Quando ord é None para um vector x retorna a 2-norm\n\nlist_l2_norm = []\n\nfor alpha in list_alpha:\n    list_l2_norm.append(np.linalg.norm(coeff_matrix[alpha]))\n\nprint(list_l2_norm[0])\nprint(list_l2_norm[-1])\n\n858.0516634402755\n2.0720439677307483e-05\n\n\n\nplt.figure(figsize = (14,6))\nplt.plot(list_alpha,list_l2_norm)\nplt.xlabel('Alpha')\nplt.ylabel('L2 Norm')\nplt.xlim(0,100)\n\n(0.0, 100.0)\n\n\n\n\n\n\n\n8.4.1.2 seleccionar o melhor \\(\\alpha\\)\n\n\n[14371397.402950881,\n 14371254.68811181,\n 14371066.701396009,\n 14370819.366358213,\n 14370494.440462416]\n\n\n\n# visualização dos scores\nsns.lineplot(x=list_alpha,y=validation_score)\nplt.xscale('log')\n\nplt.show()\n\n\n\n\nver valores por posição\n\nnp.argmin(validation_score)\n\nvalidation_score[69]\n\n14401641.44770242\n\n\nescolhido o score aplicamos o respectivo alpha\n\nmodel_ridge = Ridge(alpha=validation_score[69])\nmodel_ridge.fit(X_train, y_train) \ny_pred_ridgetrain= model_ridge.predict(X_train)\ny_pred_ridgetest= model_ridge.predict(X_test)\ny_pred_ridgetest[0:3]\n\narray([304.75916192, 594.01215749, 782.30716378])\n\n\n\n# reparem que como estamos a partir/split com seed diferente \n# temos diferentes elementos no conjunto de teste\ny_test[0:3]\n\n-Ron Roenicke       191.000\n-Keith Moreland    1043.333\n-Phil Garner        450.000\nName: Salary, dtype: float64\n\n\n\n\n\n8.4.2 Lasso Regression\n\\(Função Perda = OLS + \\alpha \\times \\sum|\\beta|)\\)\nExemplo com dataset do Scikit\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.3,random_state=2510) \n\nfrom sklearn.linear_model import Lasso\n\n# instancia o modelo de regressão Lasso\n# com um valor alfa \nmodel_lasso = Lasso(alpha=0.01)\n\nmodel_lasso.fit(X_train, y_train) \n\nLasso(alpha=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Lasso?Documentation for LassoiFittedLasso(alpha=0.01) \n\n\n\ny_pred_lassotrain = model_lasso.predict(X_train)\n\ny_pred_lassotest = model_lasso.predict(X_test)\n\ny_pred_lassotest[:5]\n\ny_test[:5]\n\n24     184.0\n241    177.0\n121    173.0\n138    336.0\n90      98.0\nName: target, dtype: float64\n\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('MSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_lassotrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_lassotrain)))\n\nprint('MSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_lassotest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_lassotest)))\n\nMSE nos dados de Treino: 53.834764417795725\nR2 nos dados de Treino: 0.5018822179496036\nMSE nos dados de Teste: 54.71440186020355\nR2 nos dados de Teste: 0.5026610748228949\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_lassotest)\nplt.xlim(0, 400)\nplt.ylim(0, 400)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n\n\n\n\n\n8.4.2.1 explorar o \\(\\alpha\\)\n\ndf.head()\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nPutOuts\nAssists\nErrors\nSalary\nLeague_N\nDivision_W\nNewLeague_N\n\n\n\n\n-Alan Ashby\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\n632\n43\n10\n475.0\nTrue\nTrue\nTrue\n\n\n-Alvin Davis\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\n880\n82\n14\n480.0\nFalse\nTrue\nFalse\n\n\n-Andre Dawson\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\n200\n11\n3\n500.0\nTrue\nFalse\nTrue\n\n\n-Andres Galarraga\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\n805\n40\n4\n91.5\nTrue\nFalse\nTrue\n\n\n-Alfredo Griffin\n594\n169\n4\n74\n51\n35\n11\n4408\n1133\n19\n501\n336\n194\n282\n421\n25\n750.0\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n# Separamos a coluna target das outras\nX = df.drop('Salary',axis = 1)\ny = df['Salary']\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Como vemos nas colunas grandes diferenças de escala vamos standardizar\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=2510) \nprint(X_train.shape)\nprint(y_test.shape)\n\n(210, 19)\n(53,)\n\n\n\n\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.182e+06, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.172e+06, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.794e+06, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.458e+06, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+06, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+06, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.852e+06, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.255e+05, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.872e+05, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.493e+05, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.225e+04, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.792e+04, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.740e+04, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.515e+04, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+04, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.099e+04, tolerance: 5.332e+03\n  model = cd_fast.enet_coordinate_descent(\n\n\n\ntmp = pd.DataFrame(coeff_matrix_lasso).T\ntmp.index = list_alpha\ntmp.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n0.010000\n-286.905007\n331.363096\n36.212359\n-57.357808\n-25.658648\n133.787447\n-14.313048\n-422.215279\n133.673334\n-4.578178\n464.460502\n246.073678\n-208.802960\n78.639945\n54.350452\n-22.467425\n31.583507\n-58.365877\n-12.672573\n\n\n0.013219\n-286.950122\n331.325695\n36.091031\n-57.257930\n-25.528647\n133.747959\n-14.398744\n-420.990800\n132.935083\n-4.435307\n464.206011\n245.764342\n-208.778380\n78.644615\n54.304079\n-22.452758\n31.554027\n-58.363511\n-12.643337\n\n\n0.017475\n-287.009869\n331.275922\n35.930298\n-57.125588\n-25.356432\n133.695761\n-14.512163\n-419.372611\n131.963418\n-4.244520\n463.867405\n245.352140\n-208.745736\n78.650806\n54.242814\n-22.433398\n31.515092\n-58.360387\n-12.604741\n\n\n0.023101\n-287.089063\n331.210178\n35.717682\n-56.950581\n-25.128615\n133.626802\n-14.662262\n-417.232465\n130.679408\n-3.991625\n463.419197\n244.805989\n-208.702686\n78.659004\n54.161818\n-22.407810\n31.463628\n-58.356260\n-12.553732\n\n\n0.030539\n-287.193575\n331.122512\n35.436185\n-56.718727\n-24.827025\n133.535567\n-14.860641\n-414.406159\n128.989613\n-3.654757\n462.823461\n244.079779\n-208.645293\n78.669850\n54.054836\n-22.374038\n31.395662\n-58.350806\n-12.486383\n\n\n\n\n\n\n\n\n\n&lt;matplotlib.legend.Legend at 0x21e5ffbb1d0&gt;\n\n\n\n\n\n\n# para valores pequenos de alpha os coeficientes são grandes\nprint(coeff_matrix_lasso[list_alpha[8]])\n\n# para valores maiores de alpha alguns coeficientes são zero\nprint(coeff_matrix_lasso[list_alpha[30]])\n\n[-288.06252906937283, 330.36981199887316, 33.06222643992299, -54.759037031180235, -22.284629558490636, 132.76311903093176, -16.525200498348504, -390.659230557264, 114.8268814172216, -0.8084173075280119, 457.7814835607485, 237.95136546687914, -208.14987606522263, 78.76080852606074, 53.1542878424637, -22.089840987640706, 30.823239242661334, -58.30463802012498, -11.918959657362254]\n[0.0, 74.03594663563689, 0.0, 0.0, 0.0, 41.19913426013344, 0.0, 0.0, 0.0, 0.0, 59.78901166959339, 121.68559725913998, 0.0, 42.406428354257805, 0.0, -0.0, 0.0, -27.734808911994236, 0.0]\n\n\n\n# para valores suficientemente grandes de alpha\n# são todos encolhidos até ao zero \n# ao contrário do que sucedia com o Ridge em que se\n# aproximavam mas nunca eram mesmo zero\nprint(coeff_matrix_lasso[list_alpha[90]])\n\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0]\n\n\n\n\n\n8.4.3 Elastic Net Regression\n\\(Função Perda = OLS + \\alpha_1 \\times \\sum\\beta^2 + \\alpha_2 \\times \\sum|\\beta|)\\)\nElasticNet combina as propriedades da regressão Ridge e Lasso. Funciona penalizando o modelo usando a norma l2 e a norma l1.\nNo scikit-learn, um modelo de regressão ElasticNet é construído usando a classe ElasticNet.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.3,random_state=2510) \n\nfrom sklearn.linear_model import ElasticNet\n\n# instancia o modelo de regressão ElasticNet\n# com um valor alfa \nmodel_elnet = ElasticNet(alpha = 0.01)\n\nmodel_elnet.fit(X_train, y_train) \n\nElasticNet(alpha=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ElasticNet?Documentation for ElasticNetiFittedElasticNet(alpha=0.01) \n\n\n\ny_pred_elnettrain= model_elnet.predict(X_train)\n\ny_pred_elnettest= model_elnet.predict(X_test)\ny_pred_elnettest[:5]\n\nprint()\ny_test[:5]\n\n\n\n\n24     184.0\n241    177.0\n121    173.0\n138    336.0\n90      98.0\nName: target, dtype: float64\n\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint('MSE nos dados de Treino: {}' .format(np.sqrt(mean_squared_error(y_train,y_pred_elnettrain))))\nprint('R2 nos dados de Treino: {}' .format(r2_score(y_train, y_pred_elnettrain)))\n\nprint('MSE nos dados de Teste: {}' .format(np.sqrt(mean_squared_error(y_test,y_pred_elnettest)))) \nprint('R2 nos dados de Teste: {}' .format(r2_score(y_test, y_pred_elnettest)))\n\nMSE nos dados de Treino: 60.662341933166346\nR2 nos dados de Treino: 0.36752297059357386\nMSE nos dados de Teste: 62.49579343732485\nR2 nos dados de Teste: 0.3511404335037812\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize = (12,6))\nsns.scatterplot(x= y_test, y= y_pred_elnettest)\nplt.xlim(0, 400)\nplt.ylim(0, 400)\nplt.title(\"Predictions\")\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()"
  },
  {
    "objectID": "800-mod8.html#cross-validation",
    "href": "800-mod8.html#cross-validation",
    "title": "8  Data Science (Advanced)",
    "section": "8.5 Cross Validation",
    "text": "8.5 Cross Validation\n\n\n8.5.1 hold-out\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n\n(150, 4)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.35,random_state=2023)\nX_train.shape\n\n(97, 4)\n\n\n\nmodel=DecisionTreeClassifier()\nmodel.fit(X_train,y_train)\nmod_score = model.score( X_test, y_test) # devolve a accuracy\nmod_score\n\n0.9622641509433962\n\n\n\n\n8.5.2 Leave-one-out CV\n\nverifica as partições\n\nimport numpy as np\nfrom sklearn.model_selection import LeaveOneOut\n\n# criar dados de exemplo\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 3, 4])\n\n# iniciar o metodo de CV\nloo = LeaveOneOut()\n# métodos que dá o número de splits para cross validadtion\nloo.get_n_splits(X)\n\nprint(loo)\n\n# a função split faz os splits\nfor i, (train_index, test_index) in enumerate(loo.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n\nLeaveOneOut()\nFold 0:\n  Train: index=[1 2 3]\n  Test:  index=[0]\nFold 1:\n  Train: index=[0 2 3]\n  Test:  index=[1]\nFold 2:\n  Train: index=[0 1 3]\n  Test:  index=[2]\nFold 3:\n  Train: index=[0 1 2]\n  Test:  index=[3]\n\n\nexemplo para calcular Score com dados do Iris\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n\n(150, 4)\n\n\n\nmodel = DecisionTreeClassifier()\n\nleave_val = LeaveOneOut()\nleave_val\n\nmod_score = cross_val_score( model, X, y, cv = leave_val) # acertos\nmod_score\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nprint(np.mean(mod_score)) # accuracy\n\n0.9466666666666667\n\n\n\n\n8.5.3 Leave-P-out\nverificar as partições\n\nimport numpy as np\nfrom sklearn.model_selection import LeavePOut\n\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 3, 4])\n\nlpo = LeavePOut(2)\n# métodos que dá o número de splits para cross validadtion\nlpo.get_n_splits(X)\n\nprint(lpo)\n\n# a função split faz os splits\nfor i, (train_index, test_index) in enumerate(lpo.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n\nLeavePOut(p=2)\nFold 0:\n  Train: index=[2 3]\n  Test:  index=[0 1]\nFold 1:\n  Train: index=[1 3]\n  Test:  index=[0 2]\nFold 2:\n  Train: index=[1 2]\n  Test:  index=[0 3]\nFold 3:\n  Train: index=[0 3]\n  Test:  index=[1 2]\nFold 4:\n  Train: index=[0 2]\n  Test:  index=[1 3]\nFold 5:\n  Train: index=[0 1]\n  Test:  index=[2 3]\n\n\nexemplo com os dados do Iris\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import LeavePOut\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n\n(150, 4)\n\n\nlistar os acertos\n\nmodel = DecisionTreeClassifier()\n\nleave_val = LeavePOut(2)\nleave_val\n\nmod_score = cross_val_score( model, X, y, cv = leave_val)\nmod_score\n\narray([1., 1., 1., ..., 1., 1., 1.])\n\n\ncalcular a accuracy que é a média dos acertos\n\nprint(np.mean(mod_score))\n\n0.9484563758389262\n\n\n\n\n8.5.4 K-fold\n\nverifica partições\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4])\n\nkf = KFold(n_splits=2)\nkf.get_n_splits(X)\n\n2\n\n\n\nprint(kf)\n\nKFold(n_splits=2, random_state=None, shuffle=False)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n\nKFold(n_splits=2, random_state=None, shuffle=False)\nFold 0:\n  Train: index=[2 3]\n  Test:  index=[0 1]\nFold 1:\n  Train: index=[0 1]\n  Test:  index=[2 3]\n\n\nexemplo com treino do modelo para os Iris\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import datasets\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n\n(150, 4)\n\n\n\n# K-fold split\nk_folds = 5\nkf = KFold(n_splits=k_folds, shuffle=True, random_state=1972) # faz o sufle dos dados antes de criar os folds\n\nkf\n\nKFold(n_splits=5, random_state=1972, shuffle=True)\n\n\naccuracy\n\n# se quisessemos só calcular o score\nmodel=DecisionTreeClassifier()\n\nmod_score =  cross_val_score( model, X, y, cv = kf) # accuracy de cada fold\n\nprint(np.mean(mod_score))\n\n0.9466666666666667\n\n\npodemos também calcular ‘manualmente’ a accuracy de cada fold\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = []\n# Aqui só estamos a obter os indices de split\n# por isso servem tanto para X como para y\nfor train_idx, test_idx in kf.split(X):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    #... treina o modelo, etc\n    model=DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n    mod_score = model.score(X_test, y_test)\n    scores.append(mod_score)\n    \n    \nprint(X_train.shape)\nprint(scores)\nprint(np.mean(scores))\n\n(120, 4)\n[1.0, 0.9333333333333333, 0.9333333333333333, 0.9666666666666667, 0.9333333333333333]\n0.9533333333333334\n\n\n\n\n8.5.5 Repeated K-fold\nrepete o k-fold mas com folds diferentes\n\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 1], [2, 2]])\ny = np.array([0, 0, 1, 1, 0, 1])\n\n\nrkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=2652124)\nrkf.get_n_splits(X, y)\n\n10\n\n\n\n```{python}\nRepeatedKFold(n_repeats=2, n_splits=2, random_state=2652124)\nfor i, (train_index, test_index) in enumerate(rkf.split(X)):\n    print(f\"Fold {i}:\")\n    print(f\"  Train: index={train_index}\")\n    print(f\"  Test:  index={test_index}\")\n```\n\nFold 0:\n  Train: index=[0 4 5]\n  Test:  index=[1 2 3]\nFold 1:\n  Train: index=[1 2 3]\n  Test:  index=[0 4 5]\nFold 2:\n  Train: index=[1 3 5]\n  Test:  index=[0 2 4]\nFold 3:\n  Train: index=[0 2 4]\n  Test:  index=[1 3 5]\nFold 4:\n  Train: index=[0 3 5]\n  Test:  index=[1 2 4]\nFold 5:\n  Train: index=[1 2 4]\n  Test:  index=[0 3 5]\nFold 6:\n  Train: index=[0 3 4]\n  Test:  index=[1 2 5]\nFold 7:\n  Train: index=[1 2 5]\n  Test:  index=[0 3 4]\nFold 8:\n  Train: index=[0 2 3]\n  Test:  index=[1 4 5]\nFold 9:\n  Train: index=[1 4 5]\n  Test:  index=[0 2 3]\n\n\ncalcular o score accuracy para Iris\n\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import datasets\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n\n(150, 4)\n\n\n\nmodel = DecisionTreeClassifier()\n\nrkf = RepeatedKFold(n_splits=10, n_repeats=2, random_state=2020)\nrkf\n\nmod_score = cross_val_score( model, X, y, cv = rkf)\nprint(mod_score)\n\n[0.86666667 0.86666667 1.         1.         1.         1.\n 0.93333333 0.93333333 1.         1.         0.86666667 1.\n 0.93333333 0.93333333 0.93333333 0.86666667 1.         1.\n 1.         0.93333333]\n\n\n\n\n8.5.6 Stratifies K-fold\na importância de instanciar\n\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, KFold\n\n# criar dados sinteticos para classificação\nmake_class = make_classification(n_samples=500,n_features=3,\n                                 n_redundant=0,n_informative=2,\n                                 n_classes=3,n_clusters_per_class=1,\n                                 random_state=11)\n\ndata = pd.DataFrame(make_class[0],columns=range(make_class[0].shape[1]))\ndata['target'] = make_class[1]\ndata.head()\n\n\n\n\n\n\n\n\n0\n1\n2\ntarget\n\n\n\n\n0\n0.035466\n-0.892312\n-0.380444\n2\n\n\n1\n0.906985\n1.563291\n-1.761234\n0\n\n\n2\n0.710053\n0.307698\n-0.325675\n0\n\n\n3\n-0.725445\n-0.957154\n-0.799394\n2\n\n\n4\n-0.413388\n0.582738\n1.417160\n1\n\n\n\n\n\n\n\nver como se distribuiram os dados entre treino e teste\n\ntrain_df,test_df = train_test_split(data,test_size=0.2,random_state=11)\nprint(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\\n{data[\"target\"].value_counts() / len(data)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TRAINING SET\\n{train_df[\"target\"].value_counts() / len(train_df)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TEST SET\\n{test_df[\"target\"].value_counts() / len(test_df)}')\n\nPROPORTION OF TARGET IN THE ORIGINAL DATA\ntarget\n1    0.342\n2    0.330\n0    0.328\nName: count, dtype: float64\n\nPROPORTION OF TARGET IN THE TRAINING SET\ntarget\n1    0.345\n2    0.345\n0    0.310\nName: count, dtype: float64\n\nPROPORTION OF TARGET IN THE TEST SET\ntarget\n0    0.40\n1    0.33\n2    0.27\nName: count, dtype: float64\n\n\n\n# na divisão dos dados em treino e test podemos estartificar pelo 'target'\ntrain_df,test_df = train_test_split(data,test_size=0.2,stratify=data['target'],random_state=11)\nprint(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\\n{data[\"target\"].value_counts() / len(data)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TRAINING SET\\n{train_df[\"target\"].value_counts() / len(train_df)}\\n\\n'+\n      f'PROPORTION OF TARGET IN THE TEST SET\\n{test_df[\"target\"].value_counts() / len(test_df)}')\n\nPROPORTION OF TARGET IN THE ORIGINAL DATA\ntarget\n1    0.342\n2    0.330\n0    0.328\nName: count, dtype: float64\n\nPROPORTION OF TARGET IN THE TRAINING SET\ntarget\n1    0.3425\n2    0.3300\n0    0.3275\nName: count, dtype: float64\n\nPROPORTION OF TARGET IN THE TEST SET\ntarget\n1    0.34\n0    0.33\n2    0.33\nName: count, dtype: float64\n\n\nexemplo para Iris\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import datasets\n\n# Load the X and y data from the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX.shape\n\n(150, 4)\n\n\n\nk = 5 # numero de blocos\nskf = StratifiedKFold(n_splits=k, shuffle=True)\nskf\n\nStratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n\n\n\nmodel = DecisionTreeClassifier()\nmod_score = cross_val_score(model, X, y,cv=skf)\nmod_score\n\nprint(np.mean(mod_score))\n\n0.9466666666666667\n\n\n‘manualmente’\n\nscores = []\n\n# o split do stratified recebe 2 argumentos\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    #... treina o modelo, etc\n    model=DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n    mod_score = model.score(X_test, y_test)\n    scores.append(mod_score)\n    \n    \nprint(X_train.shape)\nprint(scores)\nprint(np.mean(scores))\n\n(120, 4)\n[0.9666666666666667, 0.9333333333333333, 0.8333333333333334, 1.0, 0.9666666666666667]\n0.9400000000000001"
  },
  {
    "objectID": "800-mod8.html#random-forest",
    "href": "800-mod8.html#random-forest",
    "title": "8  Data Science (Advanced)",
    "section": "8.6 Random Forest",
    "text": "8.6 Random Forest\n\nconstruir uma Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=4,\n                            n_informative=2, n_redundant=0,\n                            random_state=2023, shuffle=False)\n# print(X)\n# print(y)\n\n\nclf = RandomForestClassifier(max_depth=2, oob_score=True,random_state=2023)\nclf.fit(X, y)\n\nclf.score(X, y)\n\nclf.oob_score_\n\n0.922\n\n\nprevisoes\n\nprint(clf.predict([[0.8, 1.43, 0.65, 2.1]]))\n\nprint(clf.predict_proba([[0.8, 1.43, 0.65, 2.1]]))\n\nprint(clf.predict([[0.5, -0.3, -0.25, 0.1]]))\n\n[1]\n[[0.15366738 0.84633262]]\n[0]\n\n\n\n8.6.1 Out-of-bag error\n\nos registos que não foram usados pelo bootsrap podem ser usados para avaliar a árvore criada\n\nfrom collections import OrderedDict\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\nRANDOM_STATE = 2023\n\n# Fazer a classificação com 500 datapoints (dados sinteticos)\nX, y = make_classification(\n    n_samples=500,\n    n_features=25,\n    n_clusters_per_class=1,\n    n_informative=15,\n    random_state=RANDOM_STATE,\n)\n\n# X tem 25 features/colunas\nX\n\narray([[-1.03507303,  0.74316058, -2.45157401, ...,  0.5779369 ,\n        -1.49098328, -1.03897662],\n       [ 0.02242699,  1.29471708, -1.90010424, ...,  0.64932473,\n        -2.84355268,  0.05187387],\n       [-0.37973513, -1.04108384, -0.94793872, ..., -4.20918979,\n        -0.75214391, -2.2830426 ],\n       ...,\n       [-2.77479227, -4.93285359,  1.08000128, ..., -0.23486044,\n         0.84228215, -1.57101225],\n       [ 1.38102277,  0.28753157, -1.17678184, ...,  1.26890482,\n        -2.42740796, -0.84212714],\n       [ 1.54576985,  3.46133511,  0.96445593, ...,  1.62788345,\n         0.89764588, -0.93738213]])\n\n\nParâmetros impotantes\n\nn_estimators será o número de árvores a crescer na floresta, por omissão 100\nmax_depth será o máximo para cada árvore, se vazio continua a crescer a árvore até atingir folhas puras ou até que todas as folhas tenham menos que min_samples_split exemplos nas folhas\nmin_samples_split será o minimo de exemplo num nó para fazer um split e continuar a crescer a árvore\nmax_features será o número de features/colunas a considerar quando procuramos o melhor split para cada crescimento da árvore\noob_score se quisermos ter o erro out-of-bag tem de estar a True, por omissão está a false e usa o accuracy_score\n\nensemble de varias random forest mudando o max_feat\n\n\nOrderedDict([(\"RandomForestClassifier, max_features='sqrt'\",\n              [(15, 0.14400000000000002),\n               (20, 0.11199999999999999),\n               (25, 0.09999999999999998),\n               (30, 0.08799999999999997),\n               (35, 0.09599999999999997),\n               (40, 0.09199999999999997),\n               (45, 0.08599999999999997),\n               (50, 0.08599999999999997),\n               (55, 0.08399999999999996),\n               (60, 0.08799999999999997),\n               (65, 0.08199999999999996),\n               (70, 0.07999999999999996),\n               (75, 0.07799999999999996),\n               (80, 0.07799999999999996),\n               (85, 0.07999999999999996),\n               (90, 0.07799999999999996),\n               (95, 0.07799999999999996),\n               (100, 0.07999999999999996),\n               (105, 0.07799999999999996),\n               (110, 0.07599999999999996),\n               (115, 0.07799999999999996),\n               (120, 0.07999999999999996),\n               (125, 0.07399999999999995),\n               (130, 0.07399999999999995),\n               (135, 0.07199999999999995),\n               (140, 0.06999999999999995),\n               (145, 0.07199999999999995),\n               (150, 0.06999999999999995)]),\n             (\"RandomForestClassifier, max_features='log2'\",\n              [(15, 0.14),\n               (20, 0.118),\n               (25, 0.118),\n               (30, 0.10399999999999998),\n               (35, 0.09599999999999997),\n               (40, 0.09199999999999997),\n               (45, 0.08999999999999997),\n               (50, 0.08199999999999996),\n               (55, 0.09199999999999997),\n               (60, 0.08199999999999996),\n               (65, 0.08599999999999997),\n               (70, 0.08799999999999997),\n               (75, 0.09399999999999997),\n               (80, 0.09199999999999997),\n               (85, 0.08799999999999997),\n               (90, 0.08599999999999997),\n               (95, 0.08799999999999997),\n               (100, 0.08399999999999996),\n               (105, 0.07999999999999996),\n               (110, 0.07799999999999996),\n               (115, 0.07799999999999996),\n               (120, 0.07799999999999996),\n               (125, 0.07399999999999995),\n               (130, 0.07399999999999995),\n               (135, 0.07599999999999996),\n               (140, 0.07599999999999996),\n               (145, 0.07599999999999996),\n               (150, 0.07599999999999996)]),\n             ('RandomForestClassifier, max_features=None',\n              [(15, 0.124),\n               (20, 0.124),\n               (25, 0.122),\n               (30, 0.09999999999999998),\n               (35, 0.10799999999999998),\n               (40, 0.09599999999999997),\n               (45, 0.10599999999999998),\n               (50, 0.10599999999999998),\n               (55, 0.10599999999999998),\n               (60, 0.10199999999999998),\n               (65, 0.09799999999999998),\n               (70, 0.09999999999999998),\n               (75, 0.09999999999999998),\n               (80, 0.10199999999999998),\n               (85, 0.10199999999999998),\n               (90, 0.10199999999999998),\n               (95, 0.10199999999999998),\n               (100, 0.09599999999999997),\n               (105, 0.09599999999999997),\n               (110, 0.09599999999999997),\n               (115, 0.09399999999999997),\n               (120, 0.08999999999999997),\n               (125, 0.09399999999999997),\n               (130, 0.09399999999999997),\n               (135, 0.09199999999999997),\n               (140, 0.09199999999999997),\n               (145, 0.09799999999999998),\n               (150, 0.09199999999999997)])])\n\n\nvisualização do OOB error de acordo com o número de arvores usadas e para os três max_features\n\n# Prepara o gráfico \"OOB error rate\" vs. \"n_estimators\" \nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"OOB error rate\")\nplt.legend(loc=\"upper right\")\nplt.show()"
  },
  {
    "objectID": "800-mod8.html#tuning-dos-hiperparametros",
    "href": "800-mod8.html#tuning-dos-hiperparametros",
    "title": "8  Data Science (Advanced)",
    "section": "8.7 Tuning dos hiperparametros",
    "text": "8.7 Tuning dos hiperparametros\nGrid Search e Randomized Search são as duas técnicas mais amplamente utilizadas no ajuste de hiperparâmetros.\n\n8.7.1 GridSearchCV\npesquisa exaustiva para cada combinação de hiperparametros especificados\nexemplo com dados Breast_Cancer\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import datasets\n\n# Load  X e y data a partir do dataset\ncancer = datasets.load_breast_cancer()\n\ncancer.keys()\n\nprint()\ncancer['feature_names']\nprint()\ncancer['target_names']\n\n\n\n\n\narray(['malignant', 'benign'], dtype='&lt;U9')\n\n\n\nX = cancer.data\ny = cancer.target\n\nX.shape\n\n(569, 30)\n\n\ndefinir o algoritmo a usar e os respectivosparametros a testar\n\nlogModel = LogisticRegression()\n\n# Contruimos a grid de parâmetros\nparam_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]\n\nparam_grid\n\n[{'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n  'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n         4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n         2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n         1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n         5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n  'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n  'max_iter': [100, 1000, 2500, 5000]}]\n\n\na função GridSearchCV tem o parametro refit por defeito TRUE o que significa quje o treino é feito com os melhores parametros encontrados apos CV.\n\n# por omissão refit = True\nclf = GridSearchCV(logModel, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\nclf\n\nGridSearchCV(cv=3, estimator=LogisticRegression(), n_jobs=-1,\n             param_grid=[{'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n                          'max_iter': [100, 1000, 2500, 5000],\n                          'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n                          'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag',\n                                     'saga']}],\n             verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViNot fittedGridSearchCV(cv=3, estimator=LogisticRegression(), n_jobs=-1,\n             param_grid=[{'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n                          'max_iter': [100, 1000, 2500, 5000],\n                          'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n                          'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag',\n                                     'saga']}],\n             verbose=True) estimator: LogisticRegressionLogisticRegression()  LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\nVamos fazer o fit para 3 Folds de: - 20 valores possíveis de C - 4 possíveis penalty - 5 possíveis solvers - 4 máximos para iterações \nPortanto no total vamos fazer 20x4x5x4 = 1600 candidatos x número de folds (3)\nOu seja, 4800 fits\n\nbest_clf = clf.fit(X,y)\n\nbest_clf.best_estimator_\n\nprint (f'Accuracy : {best_clf.score(X,y):.3f}')\n\nvamos fazer a previsão para para o primeiro array de X\n\nX[0]\n\nprint()\npred = clf.predict(X[0].reshape(1, -1))\npred[0]\n\nprint()\npred_prob = clf.predict_proba(X[0].reshape(1, -1))\npred_prob\n\n\n\n8.7.2 RandomizedSerachCV\n\nfrom scipy.stats import randint as sp_randint\n\nexemplo = sp_randint(1, 11)\nexemplo\n\n&lt;scipy.stats._distn_infrastructure.rv_discrete_frozen at 0x21e5f725490&gt;\n\n\n\n# sp_randint pode ser usado nos parâmetros \n# exemplo é um objecto da rv_class\n# o método rvs vai gerar random variate sample\n# random_state pode ser usado\nexemplo.rvs(20)\n\narray([ 3,  7, 10,  6,  1,  5,  4,  9,  3,  4,  6,  2,  1,  7,  7,  3,  7,\n       10,  6, 10], dtype=int64)\n\n\npode ser usado qualquer rvs\n\nimport scipy\n\na = scipy.stats.expon(scale=.1)\na.rvs(10)\n\narray([0.20731074, 0.146981  , 0.09936324, 0.05244919, 0.04635788,\n       0.38803707, 0.47398721, 0.07785736, 0.01881432, 0.46095715])\n\n\n\n# distribuição normal com mean .25 stddev 0.1, entre 0 e 1\nb = scipy.stats.truncnorm(a=0, b=1, loc=0.25, scale=0.1)\nb.rvs(10)\n\n# distribuição uniforme entre .01 e .2\nc = scipy.stats.uniform(0.01, 0.199)\nc.rvs(10)\n\narray([0.17598444, 0.06577611, 0.13389787, 0.097304  , 0.14016357,\n       0.16754231, 0.11156686, 0.17364086, 0.13752604, 0.11422778])\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import datasets\n\n# Load the X and y data from the dataset\ncancer = datasets.load_breast_cancer()\n\n\nX = cancer.data\ny = cancer.target\n\nX.shape\n\n(569, 30)\n\n\n\nparam_dist = {\"max_depth\": [3, 5], \n    \"max_features\": sp_randint(1, 11), \n    \"min_samples_split\": sp_randint(2, 11), \n    \"bootstrap\": [True, False], \n    \"criterion\": [\"gini\", \"entropy\"]} \n\nparam_dist\n\n{'max_depth': [3, 5],\n 'max_features': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen at 0x21e5c93d310&gt;,\n 'min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen at 0x21e66eeb290&gt;,\n 'bootstrap': [True, False],\n 'criterion': ['gini', 'entropy']}\n\n\n\n# construir o classificador indicando o nº \n# de árvores a crescer na floresta\nclf = RandomForestClassifier(n_estimators=50) # 50 árvores\nclf\n\nRandomForestClassifier(n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier(n_estimators=50) \n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15,random_state=2023)\nX_train.shape\n\n(483, 30)\n\n\n\nrandom_search = RandomizedSearchCV(clf, \n                                   param_distributions=param_dist, \n                                   n_iter=20, \n                                   cv=5) \n\nrandom_search.fit(X_train, y_train)\nprint(random_search.best_params_)\n\n{'bootstrap': False, 'criterion': 'gini', 'max_depth': 5, 'max_features': 6, 'min_samples_split': 7}\n\n\n\nX_test[12]\n\nprint()\ny_test[12]\n\n\n\n\n1\n\n\n\nrandom_search.predict(X_test[12].reshape(1, -1))[0]\n\nprint()\ncm = confusion_matrix(y_test, random_search.predict(X_test))\nprint(pd.DataFrame(cm,columns = ['pred: 0','pred: 1'],\n                   index = ['real: 0','real: 1']))\n\n\n         pred: 0  pred: 1\nreal: 0       29        4\nreal: 1        1       52\n\n\n\n\n8.7.3 NestedCV\ncomeçamos por gerar dados sintéticos\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# cria o dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=1, n_informative=10, n_redundant=10)\n\nX\n\narray([[-5.09180894,  0.62982401, -1.56637522, ..., -5.75139697,\n        -1.26632999,  1.38003548],\n       [ 1.79142035,  1.53228466,  2.15499957, ..., -3.36234377,\n         1.54280911,  1.45120234],\n       [-7.57085584, -0.2920971 , -3.63471958, ..., -4.34397246,\n        -1.96120901,  3.26295049],\n       ...,\n       [ 6.52439854,  0.63786484,  1.25975251, ..., -2.64387023,\n         1.58504635,  1.77162916],\n       [ 2.97193516,  2.78895124, -0.8083023 , ...,  4.84079601,\n        -1.78274376, -0.14141052],\n       [ 1.38863762,  0.18337514,  3.10736912, ..., -1.83700097,\n         2.09963137, -1.76449296]])\n\n\nsão feitos dois loops\n\n# configura o loop exterior\ncv_outer = KFold(n_splits=10, shuffle=True, random_state=1999)\n\n# prepara a lista para o loop exterior\nouter_results = list()\n\n# configura o loop CV interior\ncv_inner = KFold(n_splits=3, shuffle=True, random_state=1999)\n\n# o loop interior serve para escolher os parametros\n# define o espaço de procura\nparam_grid = {'n_estimators':  [10, 100, 500],\n              'max_features': [2, 4, 6] }\n\n\n# define o modelo\nmodel = RandomForestClassifier(random_state=1999)\n\n\n# para o loop exterior em que vamos ter KFold (10 blocos)\nfor train_idx, test_idx in cv_outer.split(X):\n    \n    # split dos dados com os indexes dados pelo split\n    X_train, X_test = X[train_idx, :], X[test_idx, :]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # define a procura dos hyperparâmetros\n    search = GridSearchCV(model, param_grid, \n                          scoring='accuracy',\n                          cv=cv_inner,\n                          refit=True)\n    # executa a procura fazendo o fit\n    result = search.fit(X_train, y_train)\n    \n    # regista o melhor fit\n    best_model = result.best_estimator_\n    # prevê com o modelo usando o hold-out dataset\n    y_pred = best_model.predict(X_test)\n    # avalia o modelo\n    acc = accuracy_score(y_test, y_pred)\n    \n    # regista os resultados do outer CV na lista\n    outer_results.append(acc)\n    # reporta o progresso\n    print('&gt;acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n    \n# sumariza a performance estimada do modelo\nprint('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))\n\n&gt;acc=0.980, est=0.926, cfg={'max_features': 2, 'n_estimators': 100}\n\n\n&gt;acc=0.930, est=0.930, cfg={'max_features': 2, 'n_estimators': 500}\n\n\n&gt;acc=0.930, est=0.918, cfg={'max_features': 2, 'n_estimators': 100}\n\n\n&gt;acc=0.950, est=0.920, cfg={'max_features': 4, 'n_estimators': 500}\n\n\n&gt;acc=0.930, est=0.928, cfg={'max_features': 2, 'n_estimators': 500}\n\n\n&gt;acc=0.940, est=0.920, cfg={'max_features': 4, 'n_estimators': 500}\n\n\n&gt;acc=0.880, est=0.933, cfg={'max_features': 2, 'n_estimators': 500}\n\n\n&gt;acc=0.910, est=0.939, cfg={'max_features': 6, 'n_estimators': 100}\n\n\n&gt;acc=0.920, est=0.929, cfg={'max_features': 2, 'n_estimators': 100}\n\n\n&gt;acc=0.930, est=0.924, cfg={'max_features': 4, 'n_estimators': 100}\nAccuracy: 0.930 (0.024)\n\n\nde forma mais condensada\n\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import datasets\n\n# Carrega os Dados\ncancer = datasets.load_breast_cancer()\n\nX = cancer.data\ny = cancer.target\n\n# escolhe o modelo\nrf = RandomForestClassifier()\n\n# define a grid\nparam_grid = {'n_estimators': [50, 100, 200],\n              'max_depth': [None, 5, 10],\n              'min_samples_split': [2, 5, 10]}\n\n# define os loops de CV\nouter_cv = KFold(n_splits=3, shuffle=True, random_state=2023)\ninner_cv = KFold(n_splits=5, shuffle=True, random_state=2023)\n\n# Define a procura de hyperparâmetros do loop interior de CV \nmodel = GridSearchCV(\n    estimator=rf, param_grid=param_grid, cv=inner_cv, n_jobs=-1\n)\n\n# Define a seleção e avaliação de modelo no loop exterior de CV \nscores = cross_val_score(model, X, y,\n                        scoring='accuracy',\n                        cv=outer_cv, n_jobs=-1)\n\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n\nAccuracy: 0.956 (0.015)"
  },
  {
    "objectID": "800-mod8.html#suport-vector-machines-svm",
    "href": "800-mod8.html#suport-vector-machines-svm",
    "title": "8  Data Science (Advanced)",
    "section": "8.8 Suport Vector Machines (SVM)",
    "text": "8.8 Suport Vector Machines (SVM)\n\n\n# importar os datasets\nfrom sklearn import datasets\n\n# load do dataset\ncancer = datasets.load_breast_cancer()\n\n# ver os dois primeiros registos\nprint(cancer.data[0:2])\n\n[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n  4.601e-01 1.189e-01]\n [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n  2.750e-01 8.902e-02]]\n\n\ndividir em treino/test\n\n# importar a função train_test_split \nfrom sklearn.model_selection import train_test_split\n\n# Split do dataset\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, \n                                                    cancer.target, \n                                                    test_size=0.3,\n                                                    random_state=1980) \n\naplicar o SVM\n\n# importar o modelo svm\nfrom sklearn import svm\n\n# criar o classificador svm \n# com um kernel linear\n# para fazer classificação SVC\n# para regressão seria SVR\nclf = svm.SVC(kernel='linear')\n\n# treinar o modelo (fazer o fit)\nclf.fit(X_train, y_train)\n\n# prever com o dataset de teste\ny_pred = clf.predict(X_test)\n\nver resultados\n\n# importar as métricas para avaliar o modelo\nfrom sklearn import metrics\n\n# Model Accuracy: Quantas vezes acerta o classificador\n# Diagonal\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# Model Precision: Dos que previu como positivos quantos eram realmente positivos\n# Coluna direita\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n# Model Recall: Dos que são positivos quantos previu\n# Linha inferior\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nAccuracy: 0.9532163742690059\nPrecision: 0.9396551724137931\nRecall: 0.990909090909091\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: 0','pred: 1'],\n                   index = ['real: 0','real: 1']))\n\n         pred: 0  pred: 1\nreal: 0       54        7\nreal: 1        1      109\n\n\n\nimport numpy as np\nfrom sklearn.model_selection import LearningCurveDisplay\nfrom sklearn.model_selection import cross_validate, ShuffleSplit\n\ncv = ShuffleSplit(random_state=0)\n\ntrain_sizes = np.linspace(0.1, 1, num=10)\ndisp = LearningCurveDisplay.from_estimator(\n    clf,\n    cancer.data, \n    cancer.target,\n    train_sizes=train_sizes,\n    cv=cv,\n    score_type=\"both\",\n    scoring=\"accuracy\",  \n    score_name=\"Accuracy\",\n    std_display_style=\"errorbar\",\n    errorbar_kw={\"alpha\": 0.7},  \n    n_jobs=2,\n)\n\n_ = disp.ax_.set(title=\"Learning curve for support vector machine\")\n\n\n\n\n\n8.8.1 SVM kernels\n\nimport pandas as pd  \nimport numpy as np  \nfrom sklearn import datasets\nfrom sklearn.svm import SVC  \nfrom sklearn.metrics import classification_report, confusion_matrix  \nimport matplotlib.pyplot as plt\n\n# Load  X e y data a partir do dataset\niris = datasets.load_iris()\n\n_, ax = plt.subplots()\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\nplt.show()\n\n\n\n\n\nX = iris.data\ny = iris.target\n\nX.shape\n\n(150, 4)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\nkernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']\n\n# função para instanciar classificador ed acordo com o tipo escolhido (ktype)\ndef getClassifier(ktype):\n    if ktype == 0:\n        # Polynomial kernal\n        return SVC(kernel='poly', degree=8, gamma=\"auto\")\n    elif ktype == 1:\n        # Radial Basis Function kernal\n        return SVC(kernel='rbf', gamma=\"auto\")\n    elif ktype == 2:\n        # Sigmoid kernal\n        return SVC(kernel='sigmoid', gamma=\"auto\")\n    elif ktype == 3:\n        # Linear kernal\n        return SVC(kernel='linear', gamma=\"auto\")\n\n\nfor i in range(4):\n    # Separate data into test and training sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)# Train a SVC model using different kernels\n    svclassifier = getClassifier(i) \n    svclassifier.fit(X_train, y_train)# Make prediction\n    y_pred = svclassifier.predict(X_test)# Evaluate our model\n    print(\"Evaluation:\", kernels[i], \"kernel\")\n    print(classification_report(y_test,y_pred))\n\nEvaluation: Polynomial kernel\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      1.00      1.00        10\n           2       1.00      1.00      1.00        10\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\nEvaluation: RBF kernel\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      1.00      1.00        11\n           2       1.00      1.00      1.00         9\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\nEvaluation: Sigmoid kernel\n              precision    recall  f1-score   support\n\n           0       0.30      1.00      0.46         9\n           1       0.00      0.00      0.00        11\n           2       0.00      0.00      0.00        10\n\n    accuracy                           0.30        30\n   macro avg       0.10      0.33      0.15        30\nweighted avg       0.09      0.30      0.14        30\n\nEvaluation: Linear kernel\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         9\n           1       0.90      1.00      0.95         9\n           2       1.00      0.92      0.96        12\n\n    accuracy                           0.97        30\n   macro avg       0.97      0.97      0.97        30\nweighted avg       0.97      0.97      0.97        30\n\n\n\nCross validation para tunning de parametros\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n\n\n\nFitting 5 folds for each of 48 candidates, totalling 240 fits\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n\n\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n\n\n[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n\n\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n\n\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n\n\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.0s\n[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.0s\n[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.0s\n\n\nGridSearchCV(estimator=SVC(),\n             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\n                         'kernel': ['rbf', 'poly', 'sigmoid']},\n             verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=SVC(),\n             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\n                         'kernel': ['rbf', 'poly', 'sigmoid']},\n             verbose=2) best_estimator_: SVCSVC(C=1, gamma=0.1)  SVC?Documentation for SVCSVC(C=1, gamma=0.1) \n\n\n\nprint(grid.best_estimator_)\n\nSVC(C=1, gamma=0.1)\n\n\n\ny_pred = grid.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(pd.DataFrame(cm,columns = ['pred: 0','pred: 1','pred: 2'],\n                   index = ['real: 0','real: 1','real: 2']))\n\nprint(classification_report(y_test,y_pred))\n\n         pred: 0  pred: 1  pred: 2\nreal: 0        9        0        0\nreal: 1        0        9        0\nreal: 2        0        2       10\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         9\n           1       0.82      1.00      0.90         9\n           2       1.00      0.83      0.91        12\n\n    accuracy                           0.93        30\n   macro avg       0.94      0.94      0.94        30\nweighted avg       0.95      0.93      0.93        30\n\n\n\n\n\n8.8.2 Decision Boundary\n\n# Como vamos mostrar em 2D só podemos ver 2 caracteristicas de cada vez\nX_01 = iris.data[:, :2]\nclf = svm.SVC(C=0.1, gamma=0.1, kernel='poly')\n\nclf.fit(X_01, y)\n\nSVC(C=0.1, gamma=0.1, kernel='poly')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(C=0.1, gamma=0.1, kernel='poly') \n\n\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(clf, X_01, response_method=\"predict\",\n                                              xlabel=iris.feature_names[0], ylabel=iris.feature_names[1],\n                                              alpha=0.5)\n\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=iris.target, edgecolor=\"k\")\nplt.show()\n\n\n\n\n\n\n8.8.3 Boosting - XGBoost\n\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\ndiamonds = sns.load_dataset(\"diamonds\")\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\ndataset Diamonds\n\ndiamonds.shape\n\nprint()\ndiamonds.describe(exclude = np) # sem exclusão de variáveis\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncount\n53940.000000\n53940\n53940\n53940\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n\n\nunique\nNaN\n5\n7\n8\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\nIdeal\nG\nSI1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\n21551\n11292\n13065\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n0.797940\nNaN\nNaN\nNaN\n61.749405\n57.457184\n3932.799722\n5.731157\n5.734526\n3.538734\n\n\nstd\n0.474011\nNaN\nNaN\nNaN\n1.432621\n2.234491\n3989.439738\n1.121761\n1.142135\n0.705699\n\n\nmin\n0.200000\nNaN\nNaN\nNaN\n43.000000\n43.000000\n326.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.400000\nNaN\nNaN\nNaN\n61.000000\n56.000000\n950.000000\n4.710000\n4.720000\n2.910000\n\n\n50%\n0.700000\nNaN\nNaN\nNaN\n61.800000\n57.000000\n2401.000000\n5.700000\n5.710000\n3.530000\n\n\n75%\n1.040000\nNaN\nNaN\nNaN\n62.500000\n59.000000\n5324.250000\n6.540000\n6.540000\n4.040000\n\n\nmax\n5.010000\nNaN\nNaN\nNaN\n79.000000\n95.000000\n18823.000000\n10.740000\n58.900000\n31.800000\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = diamonds.drop('price', axis=1), diamonds[['price']]\n\n# Extrair features de texto/categoricas\ncats = X.select_dtypes(exclude=np.number).columns.tolist() # lista com as colunas que não são numericas\n\n# Converter para categorias Pandas\nfor col in cats:\n    X[col] = X[col].astype('category')\n    \nX.dtypes\n\ncarat       float64\ncut        category\ncolor      category\nclarity    category\ndepth       float64\ntable       float64\nx           float64\ny           float64\nz           float64\ndtype: object\n\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1923)\n\nimport xgboost as xgb\n\n# Cria matrizes de regressão\ndtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n\n# Define hyperparametros\nparams = {\"objective\": \"reg:squarederror\", \"tree_method\": \"exact\"}\n\nn = 100 # numero de vezes que faz o re-train \nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n)\n\n\nfrom sklearn.metrics import mean_squared_error\n\npreds = model.predict(dtest_reg)\n\nrmse = mean_squared_error(y_test, preds, squared=False)\n\nprint(f\"RMSE of the base model: {rmse:.3f}\")\n\nRMSE of the base model: 546.209\n\n\ncom validation sets\n\n\n[0] train-rmse:2851.54115   validation-rmse:2885.52498\n\n\n[1] train-rmse:2075.99334   validation-rmse:2101.02007\n\n\n[2] train-rmse:1541.27780   validation-rmse:1564.66692\n\n\n[3] train-rmse:1177.51050   validation-rmse:1202.61956\n\n\n[4] train-rmse:938.78001    validation-rmse:962.67497\n\n\n[5] train-rmse:781.50843    validation-rmse:809.95955\n\n\n[6] train-rmse:685.28618    validation-rmse:717.84404\n\n\n[7] train-rmse:623.27618    validation-rmse:658.54710\n\n\n[8] train-rmse:585.50338    validation-rmse:620.17248\n\n\n[9] train-rmse:561.29675    validation-rmse:597.72672\n\n\n[10]    train-rmse:543.44211    validation-rmse:582.69529\n\n\n\n\n\n[11]    train-rmse:532.35474    validation-rmse:572.41923\n\n\n\n\n\n[12]    train-rmse:522.30937    validation-rmse:565.70262\n\n\n\n\n\n[13]    train-rmse:515.48104    validation-rmse:561.53956\n\n\n[14]    train-rmse:510.63233    validation-rmse:558.28116\n\n\n\n\n\n[15]    train-rmse:506.34619    validation-rmse:554.40824\n\n\n\n\n\n[16]    train-rmse:499.64836    validation-rmse:553.28517\n\n\n[17]    train-rmse:497.32876    validation-rmse:552.57399\n\n\n[18]    train-rmse:491.05103    validation-rmse:551.14560\n\n\n[19]    train-rmse:488.48323    validation-rmse:550.13292\n\n\n\n\n\n[20]    train-rmse:484.71103    validation-rmse:550.63009\n\n\n[21]    train-rmse:482.21843    validation-rmse:550.75955\n\n\n\n\n\n[22]    train-rmse:478.51034    validation-rmse:551.12872\n\n\n\n\n\n[23]    train-rmse:476.67467    validation-rmse:551.28953\n\n\n\n\n\n[24]    train-rmse:474.49278    validation-rmse:551.73781\n\n\n[25]    train-rmse:471.08966    validation-rmse:550.18362\n\n\n\n\n\n[26]    train-rmse:467.87058    validation-rmse:549.19352\n\n\n\n\n\n[27]    train-rmse:465.31173    validation-rmse:547.34390\n\n\n\n\n\n[28]    train-rmse:464.28636    validation-rmse:547.39703\n\n\n\n\n\n[29]    train-rmse:460.94143    validation-rmse:547.65231\n\n\n\n\n\n[30]    train-rmse:460.48757    validation-rmse:547.30529\n\n\n\n\n\n[31]    train-rmse:460.02334    validation-rmse:547.32384\n\n\n[32]    train-rmse:456.83887    validation-rmse:547.63188\n\n\n\n\n\n[33]    train-rmse:455.42693    validation-rmse:547.47029\n\n\n\n\n\n[34]    train-rmse:451.00532    validation-rmse:547.30785\n\n\n\n\n\n[35]    train-rmse:449.72204    validation-rmse:546.66477\n\n\n\n\n\n[36]    train-rmse:446.97346    validation-rmse:547.71042\n\n\n\n\n\n[37]    train-rmse:443.72536    validation-rmse:548.89474\n\n\n[38]    train-rmse:442.84449    validation-rmse:548.36532\n\n\n\n\n\n[39]    train-rmse:442.15598    validation-rmse:547.87521\n\n\n[40]    train-rmse:440.95540    validation-rmse:547.09643\n\n\n\n\n\n[41]    train-rmse:439.28889    validation-rmse:547.27049\n\n\n\n\n\n[42]    train-rmse:438.17661    validation-rmse:546.75187\n\n\n[43]    train-rmse:437.17148    validation-rmse:545.91500\n\n\n\n\n\n[44]    train-rmse:436.34019    validation-rmse:546.24601\n\n\n\n\n\n[45]    train-rmse:432.78323    validation-rmse:546.32314\n\n\n[46]    train-rmse:429.86356    validation-rmse:546.52959\n\n\n\n\n\n[47]    train-rmse:428.62182    validation-rmse:546.39635\n\n\n\n\n\n[48]    train-rmse:426.76376    validation-rmse:547.30570\n\n\n[49]    train-rmse:425.03735    validation-rmse:546.91084\n\n\n\n\n\n[50]    train-rmse:423.83103    validation-rmse:547.84324\n\n\n[51]    train-rmse:423.16624    validation-rmse:547.64548\n\n\n[52]    train-rmse:421.91132    validation-rmse:547.53996\n\n\n\n\n\n[53]    train-rmse:421.72375    validation-rmse:547.41005\n\n\n[54]    train-rmse:421.01729    validation-rmse:547.61045\n\n\n[55]    train-rmse:419.23700    validation-rmse:546.91899\n\n\n[56]    train-rmse:416.94475    validation-rmse:547.88263\n\n\n\n\n\n[57]    train-rmse:415.08363    validation-rmse:547.53615\n\n\n\n\n\n[58]    train-rmse:414.57494    validation-rmse:547.56363\n\n\n[59]    train-rmse:413.03458    validation-rmse:548.07752\n\n\n\n\n\n[60]    train-rmse:411.16698    validation-rmse:548.19357\n\n\n[61]    train-rmse:409.54243    validation-rmse:547.60719\n\n\n\n\n\n[62]    train-rmse:408.17602    validation-rmse:547.38507\n\n\n[63]    train-rmse:406.92073    validation-rmse:546.68228\n\n\n[64]    train-rmse:405.42162    validation-rmse:545.88412\n\n\n\n\n\n[65]    train-rmse:404.29907    validation-rmse:545.68149\n\n\n[66]    train-rmse:404.17040    validation-rmse:545.69286\n\n\n\n\n\n[67]    train-rmse:403.02720    validation-rmse:545.75936\n\n\n[68]    train-rmse:401.92390    validation-rmse:545.56665\n\n\n[69]    train-rmse:400.43239    validation-rmse:545.41915\n\n\n\n\n\n[70]    train-rmse:399.54626    validation-rmse:545.66828\n\n\n\n\n\n[71]    train-rmse:399.46133    validation-rmse:545.60366\n\n\n[72]    train-rmse:399.20940    validation-rmse:545.56253\n\n\n[73]    train-rmse:397.26732    validation-rmse:545.46926\n\n\n\n\n\n[74]    train-rmse:396.44554    validation-rmse:545.36230\n\n\n\n\n\n[75]    train-rmse:395.72026    validation-rmse:545.41617\n\n\n[76]    train-rmse:394.79344    validation-rmse:545.04992\n\n\n[77]    train-rmse:392.74388    validation-rmse:545.31822\n\n\n[78]    train-rmse:391.93890    validation-rmse:544.74043\n\n\n\n\n\n[79]    train-rmse:391.04432    validation-rmse:544.62966\n\n\n\n\n\n[80]    train-rmse:390.54971    validation-rmse:544.40152\n\n\n\n\n\n[81]    train-rmse:389.30857    validation-rmse:544.36426\n\n\n\n\n\n[82]    train-rmse:389.12224    validation-rmse:544.24513\n\n\n[83]    train-rmse:386.75351    validation-rmse:544.81242\n\n\n\n\n\n[84]    train-rmse:384.85584    validation-rmse:544.74169\n\n\n[85]    train-rmse:384.13838    validation-rmse:544.75912\n\n\n[86]    train-rmse:383.13439    validation-rmse:544.31586\n\n\n[87]    train-rmse:382.46996    validation-rmse:544.37377\n\n\n\n\n\n[88]    train-rmse:380.46399    validation-rmse:544.66472\n\n\n\n\n\n[89]    train-rmse:378.99535    validation-rmse:544.91259\n\n\n[90]    train-rmse:378.04076    validation-rmse:544.37391\n\n\n\n\n\n[91]    train-rmse:377.86955    validation-rmse:544.35478\n\n\n\n\n\n[92]    train-rmse:377.29435    validation-rmse:544.26397\n\n\n\n\n\n[93]    train-rmse:375.62168    validation-rmse:544.56371\n\n\n\n\n\n[94]    train-rmse:374.51269    validation-rmse:545.08575\n\n\n\n\n\n[95]    train-rmse:373.71698    validation-rmse:544.68374\n\n\n\n\n\n[96]    train-rmse:372.93790    validation-rmse:545.54976\n\n\n[97]    train-rmse:372.24304    validation-rmse:545.10499\n\n\n[98]    train-rmse:370.85634    validation-rmse:546.15178\n\n\n\n\n\n[99]    train-rmse:370.32271    validation-rmse:546.20862\n\n\n\n\n\n\n# com early stop\nn = 10000\n\nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n   evals=evals,\n   verbose_eval=50,\n   # Activate early stopping\n   early_stopping_rounds=50 \n)\n\n[0] train-rmse:2851.54115   validation-rmse:2885.52498\n\n\n\n\n\n[50]    train-rmse:423.83103    validation-rmse:547.84324\n\n\n[100]   train-rmse:369.07945    validation-rmse:546.57607\n\n\n[132]   train-rmse:344.97045    validation-rmse:549.19256\n\n\ncom Cross-Validation\n\nparams = {\"objective\": \"reg:squarederror\", \"tree_method\": \"exact\"}\nn = 1000\n\nresults = xgb.cv(\n   params, dtrain_reg,\n   num_boost_round=n,\n   nfold=5, # adicionado o nº de folds\n   early_stopping_rounds=20\n)\n\nresults.tail()\n\n\n\n\n\n\n\n\ntrain-rmse-mean\ntrain-rmse-std\ntest-rmse-mean\ntest-rmse-std\n\n\n\n\n35\n441.539017\n4.652373\n551.692505\n11.636016\n\n\n36\n438.157276\n5.007577\n552.302801\n11.833530\n\n\n37\n435.889516\n5.385259\n551.947682\n11.964930\n\n\n38\n433.413269\n5.050729\n551.837739\n11.917136\n\n\n39\n432.287905\n5.296268\n551.562633\n11.874399\n\n\n\n\n\n\n\n\n\n8.8.4 Stacking\n\navaliar e comparar modelos incluindo stacking\n\n# importar a função para fazer um dataset\nfrom sklearn.datasets import make_classification\n\n# definir o dataset com dados sinteticos\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1973)\n\n# caracteristicas do dataset\nprint(X.shape, y.shape)\n\n(1000, 20) (1000,)\n\n\n\n# todos os imports\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\nfrom matplotlib import pyplot\n\n# função para dar o dataset\n# tal como exemplificado em cima\ndef get_dataset():\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n    return X, y\n\n# função para definir modelos  \ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    return models\n  \n# avaliar um modelo usando cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # estamos a usar a accuracy para o scoring\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n# e agora podemos aplicar as funções definidas \nX, y = get_dataset()\nmodels = get_models()\n\n# avalia os modelos e regista os resultados\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('&gt;%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n\n&gt;lr 0.866 (0.029)\n&gt;knn 0.931 (0.025)\n\n\n&gt;cart 0.826 (0.045)\n\n\n&gt;svm 0.957 (0.020)\n&gt;bayes 0.833 (0.031)\n\n\n\n# faz boxplot do desempenho dos modelos para comparação\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n\n\n\n\n# define o stacking ensemble dos modelos\ndef get_stacking():\n    # define os modelos de base\n    level0 = list()\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('cart', DecisionTreeClassifier()))\n    level0.append(('svm', SVC()))\n    level0.append(('bayes', GaussianNB()))\n    # define o modelo meta learner\n    level1 = LogisticRegression()\n    # define o stacking ensemble usando o Stacking Classifier e cross-validation\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model\n  \n# redefinimos a lista de modelos a avaliar acrescentando o stacking\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    models['stacking'] = get_stacking()\n    return models\n  \nX, y = get_dataset()\nmodels = get_models()\n\n# avalia os modelos e regista os resultados\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('&gt;%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n\n&gt;lr 0.866 (0.029)\n&gt;knn 0.931 (0.025)\n\n\n&gt;cart 0.820 (0.044)\n\n\n&gt;svm 0.957 (0.020)\n&gt;bayes 0.833 (0.031)\n\n\n&gt;stacking 0.965 (0.019)\n\n\n\n# faz boxplot do desempenho dos modelos para comparação\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n\n\n\nprever usando o sctaking de modelos\n\n# imports\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# definir o dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n\n\n# definir os modelos de base\nlevel0 = list()\nlevel0.append(('lr', LogisticRegression()))\nlevel0.append(('knn', KNeighborsClassifier()))\nlevel0.append(('cart', DecisionTreeClassifier()))\nlevel0.append(('svm', SVC()))\nlevel0.append(('bayes', GaussianNB()))\n\n# definir o modelo meta learner\nlevel1 = LogisticRegression()\n\n# definir o stacking ensemble\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n\n\n# treinar o modelo com todos os dados (fazer o fit)\nmodel.fit(X, y)\n\nStackingClassifier(cv=5,\n                   estimators=[('lr', LogisticRegression()),\n                               ('knn', KNeighborsClassifier()),\n                               ('cart', DecisionTreeClassifier()),\n                               ('svm', SVC()), ('bayes', GaussianNB())],\n                   final_estimator=LogisticRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  StackingClassifier?Documentation for StackingClassifieriFittedStackingClassifier(cv=5,\n                   estimators=[('lr', LogisticRegression()),\n                               ('knn', KNeighborsClassifier()),\n                               ('cart', DecisionTreeClassifier()),\n                               ('svm', SVC()), ('bayes', GaussianNB())],\n                   final_estimator=LogisticRegression()) lr LogisticRegression?Documentation for LogisticRegressionLogisticRegression() knn KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier() cart DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() svm SVC?Documentation for SVCSVC() bayes GaussianNB?Documentation for GaussianNBGaussianNB() final_estimator LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\n# data é um novo dado\ndata = [[2.47475454,0.40165523,1.68081787,2.88940715,0.91704519,-3.07950644,4.39961206,0.72464273,-4.86563631,-6.06338084,-1.22209949,-0.4699618,1.01222748,-0.6899355,-0.53000581,6.86966784,-3.27211075,-6.59044146,-2.21290585,-3.139579]]\n\n# prever para esse novo dado\nynew = model.predict(data)\nprint('Predicted Class: %d' % (ynew))\n\nPredicted Class: 0\n\n\nstacking para regressão\n\n# importar\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import StackingRegressor\nfrom matplotlib import pyplot\n\n# definir o dataset\n# repare que estamos a usar a função make_regression e não a make_classification que usámos antes\ndef get_dataset():\n    X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1985)\n    return X, y\n\n# definir o stacking\ndef get_stacking():\n    # definir os modelos base\n    level0 = list()\n    level0.append(('knn', KNeighborsRegressor()))    # em vez de KNeighborsClassifier\n    level0.append(('cart', DecisionTreeRegressor())) # em vez de DecisionTreeClassifier\n    level0.append(('svm', SVR()))                    # em vez de SVC\n    # definir o modelo meta learner\n    level1 = LinearRegression()\n    # definir o stacking ensemble\n    # usando o StackingRegressor em vez do StackingClassifier\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return model\n  \n# definir a lista dos modelos a avaliar\ndef get_models():\n    models = dict()\n    models['knn'] = KNeighborsRegressor()    # em vez de KNeighborsClassifier\n    models['cart'] = DecisionTreeRegressor() # em vez de DecisionTreeClassifier\n    models['svm'] = SVR()                    # em vez de SVC\n    models['stacking'] = get_stacking()\n    return models\n  \n# definir avaliação de um modelo usando cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # estamos a usar neg_mean_absolute_error em vez da accuracy para o scoring\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n  \nX, y = get_dataset()\nmodels = get_models()\n\n# avalia os modelos e regista os resultados\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('&gt;%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n\n# boxplot do desempenho dos modelos para comparação\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n\n&gt;knn -104.398 (8.536)\n\n\n&gt;cart -144.126 (10.460)\n\n\n\n\n\n&gt;svm -163.010 (12.175)\n\n\n&gt;stacking -63.488 (5.983)"
  },
  {
    "objectID": "800-mod8.html#bonus",
    "href": "800-mod8.html#bonus",
    "title": "8  Data Science (Advanced)",
    "section": "8.9 Bonus",
    "text": "8.9 Bonus\n\n8.9.1 Comparar os modelos com Decision Boundaries\n\n# Code source: Gaël Varoquaux\n#              Andreas Müller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.datasets import make_circles, make_classification, make_moons\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nnames = [\n    \"Nearest Neighbors\",\n    \"Linear SVM\",\n    \"RBF SVM\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"Neural Net\",\n    \"AdaBoost\",\n    \n]\n\n# instanciação dos métodos a usar, já com parametros\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025, random_state=42),\n    SVC(gamma=2, C=1, random_state=42),\n    DecisionTreeClassifier(max_depth=5, random_state=42),\n    RandomForestClassifier(\n        max_depth=5, n_estimators=10, max_features=1, random_state=42\n    ),\n    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n    AdaBoostClassifier(random_state=42),\n]\n\n# cria os dados sinteticos\nX, y = make_classification(\n    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n)\nrng = np.random.RandomState(2000)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [\n    make_moons(noise=0.3, random_state=0),\n    make_circles(noise=0.2, factor=0.5, random_state=1),\n    linearly_separable,\n]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.4, random_state=42\n    )\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n    # Plot the testing points\n    ax.scatter(\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n    )\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers): # a função zip cia o par num tuplo\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n\n        clf = make_pipeline(StandardScaler(), clf)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        DecisionBoundaryDisplay.from_estimator(\n            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n        )\n\n        # Plot the training points\n        ax.scatter(\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n        )\n        # Plot the testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            edgecolors=\"k\",\n            alpha=0.6,\n        )\n\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(\n            x_max - 0.3,\n            y_min + 0.3,\n            (\"%.2f\" % score).lstrip(\"0\"),\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        i += 1\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "800-mod8.html#comparação-dos-modelos-complexos",
    "href": "800-mod8.html#comparação-dos-modelos-complexos",
    "title": "8  Data Science (Advanced)",
    "section": "8.10 Comparação dos Modelos Complexos",
    "text": "8.10 Comparação dos Modelos Complexos"
  },
  {
    "objectID": "900-mod9.html#manipulação-de-dados",
    "href": "900-mod9.html#manipulação-de-dados",
    "title": "9  Dados Geográficos",
    "section": "9.1 Manipulação de dados",
    "text": "9.1 Manipulação de dados\nGeometria em GeoPandas\n\nGeoseries\nGeometry\n\nOperações sobre a coluna GeoMetry\n\nsimplificar é importar\nfacilita a visualização\n\n\nimport geopandas as gpd\nimport folium\n\n# Simplify: https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.simplify.html\n# Exemplo Importar os dados da BGRI2021 \n# Caminho para o arquivo GeoPackage\ngpk = r\"data\\geo\\BGRI2021_1106.gpkg\"\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdf1106 = gpd.read_file(gpk)\ngdf1106_2 = gpd.read_file(gpk)\n# Simplificar a geografia para uma precisão de 5 metros\n# Experimenta - diferentes valores para ver o efeito na geometria\ngdf1106_2['geometry'] = gdf1106_2['geometry'].simplify(tolerance=10)\n\n# ------------------\n# Mostrar a localização com Folium\n# Obter Centroid\ncentroid = gdf1106.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=12, tiles='OpenStreetMap')\n\n# Adicionar Geografia folium map\n# folium.GeoJson constructor\nfolium.GeoJson(gdf1106).add_to(folium_map)\n\n# Mudar a cor\nstyle_function = lambda x: {'fillColor': '#ffffff', 'color': '#000000'}\nfolium.GeoJson(gdf1106_2, style_function=style_function).add_to(folium_map)\n\n#  Widget para controloar os diferentes layers:\nfolium.LayerControl().add_to(folium_map)\n\nfolium_map\n# Visualizar o GeoDataFrame\n#gdf1106.plot(column = 'DTMNFR21',\n#              legend = False)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# explore() forma fácil de ver uma GeoDataFrame\n\n# Mostrar a geografia do GeoDataFrame\ngdf1106_2.explore(column = 'DTMNFR21',\n              legend = True,\n                  edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nUtilizar a área de cada elemento\n\n#print(gdf1106.info())\n\n# Codigo que mostra como calcular a população por KM2\n\n# Verificar o CRS da GDF \nprint (gdf1106.crs)\n# Area 1º registo\nprint('BGRI21:', gdf1106.iloc[10].BGRI2021, 'Area:', gdf1106.iloc[10].geometry.area)\n\n# Adicionar nova coluna \n# Caso GDF está noutra CRS será necessario uma correção: gdf1106['geometry'].to_crs(epsg=3857).area\ngdf1106['AREA_KM2'] = gdf1106['geometry'].area / 1000000\ngdf1106['INDIV_KM2'] = gdf1106['N_INDIVIDUOS'] / gdf1106['AREA_KM2']\n\n# Mostrar resultado:\nprint(gdf1106[['DTMNFR21', 'N_INDIVIDUOS','AREA_KM2', 'INDIV_KM2']].head(10))\n\nEPSG:3763\nBGRI21: 11065601903 Area: 12351.477088540338\n  DTMNFR21  N_INDIVIDUOS  AREA_KM2     INDIV_KM2\n0   110656         202.0  0.009658  20915.808095\n1   110657         197.0  0.008445  23326.786403\n2   110658          21.0  0.002582   8131.932073\n3   110658          20.0  0.003078   6498.053191\n4   110658          22.0  0.002556   8608.476675\n5   110607         178.0  0.070032   2541.699854\n6   110656         195.0  0.007235  26953.994420\n7   110610          12.0  0.002018   5947.325321\n8   110657          42.0  0.004894   8581.337906\n9   110610          87.0  0.021791   3992.414803\n\n\nMostrar resultado como mapa\n\n# Import packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\n\n# Definir figura:\nf, ax = plt.subplots(1, figsize=(9, 9))\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 3}\n\n# Generate the choropleth and store the axis\n# natural_breaks\ngdf1106.plot(column=gdf1106.INDIV_KM2, \n              scheme='quantiles', # natural_breaks, quantiles, equal_interval \n              k=9, \n              cmap='PuBu', \n              legend=True,\n              edgecolor = 'None', # sem outline\n              legend_kwds  = lgnd_kwds,\n              ax = ax)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('População por Km2')\nplt.show()\n\n\n\n\nAlterar o tipo de Geometria\n\n#print(gdf1106.info())\n\nimport geopandas as gpd\nimport folium\n\n# Criar um novo GeoDataFrame de pontos\ngdf1106_points = gdf1106.copy()\n\n# Neste momento a Geometria ainda é de Polygons:\nprint (gdf1106_points['geometry'].iloc[0].geom_type)\n\nMultiPolygon\n\n\nconverter para pontos\n\n# Calcular o centróide de cada polígono\ngdf1106_points['geometry'] = gdf1106['geometry'].centroid\n\n# Calcular ponto dentro poligono\n#gdf1106_points['geometry'] = gdf1106['geometry'].representative_point()\n\nprint(\"Geometria original e nova:\",gdf1106.iloc[0]['geometry'].geom_type,gdf1106_points.iloc[0]['geometry'].geom_type)\n\nGeometria original e nova: MultiPolygon Point\n\n\nvisualizar resultado\n\n# Fazer Seleção dos Registos para facilitar visualização\ngdf_pnt110657 = gdf1106_points[gdf1106_points['DTMNFR21'] == '110655']\ngdf_poly110657 = gdf1106[gdf1106['DTMNFR21'] == '110655']\n\n# --------------------------------------\n# Mostrar a localização com Folium\n# São muitos dados - visualização é lento\ncentroid = gdf_pnt110657.to_crs(epsg=4326).unary_union.centroid\n\n# Criar Listagem com localização de latitude  longitude\ncenter_map = [centroid.y, centroid.x]\n# Criar Mapa e mostrar\nfolium_map = folium.Map(location=center_map, zoom_start=15, tiles='OpenStreetMap')\n\n# Adicionar Geografia folium map\n# folium.GeoJson constructor\nfolium.GeoJson(gdf_pnt110657).add_to(folium_map)\nfolium.GeoJson(gdf_poly110657).add_to(folium_map)\n\n#  Widget para controloar os diferentes layers:\nfolium.LayerControl().add_to(folium_map)\n\nfolium_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\ncriar nova geografia a partir de um dissolve\n\nimport geopandas as gpd\n\n# Codigo que mostra um dissolve das subsecções para freguesias\n# Utilizar argumento aggfunc (default = first)\n\n# Alternativa 1: DTMNFR21 passa a ser o index - sem reset_index()\n# gdf1106_freg = gdf1106.dissolve(by='DTMNFR21', aggfunc='sum')\n\n# Alternativa 2 reset_index para manter a coluna\ngdf1106_freg = gdf1106.dissolve(by='DTMNFR21', aggfunc='sum').reset_index()\n\n\n# Mostrar Resultado da nova gdf\n# print(gdf1106_freg.info())\n\n# De seguido será necessária fazer limpeza e correção das colunas\n# Apagar Colunas desnecessários\ngdf1106_freg = gdf1106_freg.drop(columns=['BGRI2021','DTMNFRSEC21','SECNUM21','SSNUM21','SECSSNUM21','SUBSECCAO','NUTS1','NUTS2','NUTS3'])\n\n# Mudar os valores das colunas nivel superior a DTMNFR21\ngdf1106_freg['DTMN21'] = '1106'\ngdf1106_freg['DT21'] = '1106'\n\ngdf1106_freg.head()\n\n\n\n\n\n\n\n\nDTMNFR21\ngeometry\nOBJECTID\nDT21\nDTMN21\nN_EDIFICIOS_CLASSICOS\nN_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ\nN_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS\nN_EDIFICIOS_EXCLUSIV_RESID\nN_EDIFICIOS_1_OU_2_PISOS\n...\nN_INDIVIDUOS_H\nN_INDIVIDUOS_M\nN_INDIVIDUOS_0_14\nN_INDIVIDUOS_15_24\nN_INDIVIDUOS_25_64\nN_INDIVIDUOS_65_OU_MAIS\nSHAPE_Length\nSHAPE_Area\nAREA_KM2\nINDIV_KM2\n\n\n\n\n0\n110601\nPOLYGON ((-93010.757 -106447.914, -93010.685 -...\n4597315\n1106\n1106\n2716.0\n1862.0\n848.0\n2635.0\n1896.0\n...\n6566.0\n7740.0\n1796.0\n1268.0\n7323.0\n3919.0\n66058.035548\n2.876608e+06\n2.876608\n1.576230e+06\n\n\n1\n110602\nPOLYGON ((-92096.453 -107111.798, -92120.721 -...\n3632570\n1106\n1106\n1493.0\n475.0\n1013.0\n1366.0\n578.0\n...\n6269.0\n7581.0\n1774.0\n1235.0\n7500.0\n3341.0\n66916.297439\n5.074775e+06\n5.074775\n1.298304e+06\n\n\n2\n110607\nPOLYGON ((-85689.781 -103386.061, -85713.171 -...\n2891282\n1106\n1106\n1730.0\n1033.0\n694.0\n1673.0\n1046.0\n...\n5786.0\n6397.0\n1351.0\n1148.0\n6542.0\n3142.0\n53627.590860\n2.483785e+06\n2.483785\n1.065595e+06\n\n\n3\n110608\nPOLYGON ((-92823.294 -104458.654, -92953.772 -...\n4639555\n1106\n1106\n2493.0\n1074.0\n1407.0\n2274.0\n1050.0\n...\n15808.0\n19554.0\n4101.0\n3193.0\n17593.0\n10475.0\n115431.283511\n8.024926e+06\n8.024926\n2.225803e+06\n\n\n4\n110610\nPOLYGON ((-90527.955 -104449.513, -90599.907 -...\n3504714\n1106\n1106\n2255.0\n1413.0\n828.0\n2164.0\n1467.0\n...\n6989.0\n7798.0\n1702.0\n1413.0\n8077.0\n3595.0\n66140.890825\n2.774336e+06\n2.774336\n1.185843e+06\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n# Mostrar o resultado (a geografia do GeoDataFrame)\ngdf1106_freg.explore(column = 'DTMNFR21',\n              legend = True,\n                  edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n9.1.1 Operações entre Datasets\nSpatial join\n\nimport geopandas as gpd\nimport folium\n\n# Importar Paragens de autocarro\ngpk = r\"data\\geo\\GPK_CARRIS.gpkg\"\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdfCarris = gpd.read_file(gpk,encoding='utf-8')\n\nprint(gdfCarris.head())\n# Total de 1983 registos\nprint(gdfCarris.info())\n\n\nprint('BGRI21:', gdfCarris.iloc[10].other_tags)\n\n# ------------------\n# Mostrar a localização \ngdfCarris.explore(legend = True,\n                  edgecolor = 'black',\n                  marker_type = 'marker')\n\n      osm_id                                          name barrier   highway  \\\n0   25726357                                AlcÃ¢ntara Mar          bus_stop   \n1  256063280                                      Limoeiro                     \n2  258155049                             EstaÃ§Ã£o Oriente          bus_stop   \n3  376036462  Campo Pequeno - Avenida Defensores de Chaves          bus_stop   \n4  387760167                        EstaÃ§Ã£o de Sete Rios          bus_stop   \n\n     ref address is_in place man_made  \\\n0   4002                                \n1  13804                                \n2  60207                                \n3   1904                                \n4   1621                                \n\n                                          other_tags  \\\n0  \"bench\"=&gt;\"no\",\"bus\"=&gt;\"yes\",\"mapillary\"=&gt;\"30186...   \n1  \"bench\"=&gt;\"yes\",\"bin\"=&gt;\"yes\",\"lit\"=&gt;\"no\",\"netwo...   \n2  \"bench\"=&gt;\"yes\",\"bin\"=&gt;\"yes\",\"bus\"=&gt;\"yes\",\"loca...   \n3  \"bench\"=&gt;\"yes\",\"bin\"=&gt;\"yes\",\"bus\"=&gt;\"yes\",\"depa...   \n4  \"bench\"=&gt;\"yes\",\"bin\"=&gt;\"yes\",\"bus\"=&gt;\"yes\",\"depa...   \n\n                         geometry  \n0  POINT (-90492.623 -106643.486)  \n1  POINT (-86909.942 -105891.249)  \n2   POINT (-84059.365 -99553.293)  \n3  POINT (-87907.740 -102365.259)  \n4  POINT (-89930.824 -102471.872)  \n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 1983 entries, 0 to 1982\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   osm_id      1983 non-null   object  \n 1   name        1983 non-null   object  \n 2   barrier     1983 non-null   object  \n 3   highway     1983 non-null   object  \n 4   ref         1983 non-null   object  \n 5   address     1983 non-null   object  \n 6   is_in       1983 non-null   object  \n 7   place       1983 non-null   object  \n 8   man_made    1983 non-null   object  \n 9   other_tags  1983 non-null   object  \n 10  geometry    1983 non-null   geometry\ndtypes: geometry(1), object(10)\nmemory usage: 170.5+ KB\nNone\nBGRI21: \"bench\"=&gt;\"yes\",\"bus\"=&gt;\"yes\",\"network\"=&gt;\"Carris Metropolitana\",\"network:wikidata\"=&gt;\"Q111611112\",\"official_name\"=&gt;\"PÃ§. Areeiro\",\"public_transport\"=&gt;\"platform\",\"shelter\"=&gt;\"yes\"\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nSpatial join de polygons para points\n\n# Realizar o spatial join\ngdf_join = gpd.sjoin(gdf1106_freg, gdfCarris, how=\"inner\", predicate=\"contains\")\n\n# Limitar Colunas:\n# gdf_join = gpd.sjoin(gdf1106_freg[['DTMNFR21', 'geometry']], gdfCarris, how=\"left\", predicate=\"contains\")\n\nprint (f\"Tipo de Geometria: {gdf_join['geometry'].iloc[0].geom_type}\")\nprint (f\"Nº de Registos Input: {len(gdf1106_freg)}\")\nprint (f\"Nº de Registos Output: {len(gdf_join)}\", \"\\n\")\n\n\n#gdf_join.info()\n\nTipo de Geometria: Polygon\nNº de Registos Input: 24\nNº de Registos Output: 1983 \n\n\n\nJoin de points para polygons\npor exemplo, obter a freguesia para cada paragem de autocarro\n\nimport geopandas as gpd\n\n# Perform spatial join\ngdf_join = gpd.sjoin(gdfCarris, gdf1106_freg[['DTMNFR21', 'geometry']], how='left', predicate='within')\n\n# Mostrar Resultado\nprint (f\"Tipo de Geometria: {gdf_join['geometry'].iloc[0].geom_type}\")\nprint (f\"Nº de Registos Input: {len(gdfCarris)}\")\nprint (f\"Nº de Registos Output: {len(gdf_join)}\", \"\\n\")\n\ngdf_join[['osm_id','DTMNFR21']].head()\n\nTipo de Geometria: Point\nNº de Registos Input: 1983\nNº de Registos Output: 1983 \n\n\n\n\n\n\n\n\n\n\nosm_id\nDTMNFR21\n\n\n\n\n0\n25726357\n110660\n\n\n1\n256063280\n110665\n\n\n2\n258155049\n110662\n\n\n3\n376036462\n110657\n\n\n4\n387760167\n110639\n\n\n\n\n\n\n\nobter a contagem das paragens de freguesia\n\n#gdf1106_freg.drop(columns=['n_paragens_x','n_paragens_y'], inplace=True)\n\n# Realiza o spatial join\n# '''\n# 1. Fazer sjoin: resultado um GDF com o memso numero de registos que os pontos\n# 2. Adicionar uma nova coluna n_paragens com total de registos existentes por Freguesia\n# 3. Obter Dataframe com numero de Valores unicos \n# 4. Fazer merge do novo valor obtido com Geodataframe original\n# 5. Apagar o objecto do join\n# '''\n# Realizar o spatial join\n# Resultado terá o mesmo nº de registos que gdfCarris\ngdf_join = gpd.sjoin(gdf1106_freg, gdfCarris, how=\"inner\", predicate=\"contains\")\n\nprint (f\"Nº de Registos Output Join: {len(gdf_join)}\")\n\n\n# Contar o número de pontos em cada polígono (novo atributo n_paragens)\n# Informação está duplicada para cada Freguesia\ngdf_join[\"n_paragens\"] = gdf_join.groupby(\"DTMNFR21\")[\"geometry\"].transform(\"size\")\n\n# # Selecionar o primeiro valor de 'n_paragens' dentro de cada grupo 'DTMNFR21'\nunique_values = gdf_join.groupby('DTMNFR21')['n_paragens'].first().reset_index()\n\nprint(unique_values.info())\n\n# Exibir o DataFrame resultante\n#print(unique_values.head())\n\n# Apagar coluna - caso repetir o codigo (o merge) sem recriar gdf1106\n# gdf1106_freg.drop(columns=['n_paragens','n_paragens_x','n_paragens_y'], inplace=True)\ngdf1106_freg = gdf1106_freg.merge(unique_values, on='DTMNFR21', how='left')\n\ndel gdf_join\n\n\n# Exibir a GeoDataFrame resultante com o atributo 'n_paragens' adicionado\n# print(gdf1106_freg.info())\nprint(gdf1106_freg[['DTMNFR21', 'n_paragens']].head(10))\n\nNº de Registos Output Join: 1983\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 24 entries, 0 to 23\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   DTMNFR21    24 non-null     object\n 1   n_paragens  24 non-null     int64 \ndtypes: int64(1), object(1)\nmemory usage: 516.0+ bytes\nNone\n  DTMNFR21  n_paragens\n0   110601          71\n1   110602          72\n2   110607          53\n3   110608         132\n4   110610          75\n5   110611          73\n6   110618         137\n7   110621         138\n8   110633         134\n9   110639          85\n\n\n\n# Desenhar mapa com resultado:\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 2}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf1106_freg.plot(column=gdf1106_freg.n_paragens, \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=5, \n                      cmap='YlGn', \n                      legend=True,\n                      edgecolor = 'None', \n                      legend_kwds  = lgnd_kwds)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Nº de paragens Carris')\nplt.show()\n\n\n\n\noutra possibilidade - fazer a seleção de nº de pontos para freguesia específica\n\n# Fazer seleção de nº de pontos para freguesia especifica\nfrom geopandas.tools import sjoin\n\n# Selecionar o polígono específico da freguesia 110655 (Areeiro)\npoligono_especifico = gdf1106_freg[gdf1106_freg['DTMNFR21'] == '110655']\n\n# Fazer Join com gdfCArris para obter os pontos\njoined = sjoin(gdfCarris, poligono_especifico, how='inner', predicate='within')\n\n# Contar quantos pontos estão dentro do polígono específico\nquantidade_pontos = len(joined)\n\n# Mostrar o resultado\nprint(f\"Quantidade de paragens de autocarro dentro a freguesia 110655: {quantidade_pontos}\")\n\nQuantidade de paragens de autocarro dentro a freguesia 110655: 49\n\n\nefectuar um overlay\nexemplo de cortar um círculo (buffer) em volta e um ponto\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport contextily as ctx\nfrom shapely.geometry import Point\n\n# Criar variáveis para a figura\nf, ax = plt.subplots(1, figsize=(9, 9))\n\n# Criar um objeto Point\n# Atenção primeiro o valor x (longitude) e depois o y (latitutde) - Google Maps devolve Latitude, Longitude\nponto = Point(-9.184111016,38.768216306)\n# 38.76821630632057, -9.184111016081756\n# Cria um GeoDataFrame do ponto com CRS WGS84\ngdf_ponto = gpd.GeoDataFrame([1], geometry=[ponto], crs='EPSG:4326')\n\n# Mudar a projeção do pontos para a projeção da gdf1106:\ngdf_ponto = gdf_ponto.to_crs(gdf1106.crs)\n\nprint (f\"Tipo de Geometria: {gdf_ponto['geometry'].iloc[0].geom_type}\")\n\n# Cria um buffer de 500 metros em volta do ponto\ngdf_ponto['geometry'] = gdf_ponto.geometry.buffer(1500)\nprint (f\"Tipo de Geometria: {gdf_ponto['geometry'].iloc[0].geom_type}\")\n\n# Realiza a interseção entre o buffer e os polígonos\n# Opções: intersection’, ‘union’, ‘identity’, ‘symmetric_difference’ or ‘difference’ \nintersecao = gpd.overlay(gdf1106, gdf_ponto, how='symmetric_difference')\n\n# Visualizar o Resultado\nintersecao.plot(column = 'DTMNFR21',\n              legend = False,\n               ax = ax)\n\n# Add basemap do contextily\nctx.add_basemap(\n    ax,\n    crs=intersecao.crs,\n    source=ctx.providers.CartoDB.VoyagerNoLabels,\n)\n\n\nax.set_axis_off()\n\nplt.show()\n\nTipo de Geometria: Point\nTipo de Geometria: Polygon\n\n\n\n\n\n\n\n9.1.2 Exportar dados GeoDataframe\nprincipais formatos de output\n\nShapefile: Sem necessidade de especificar driver\nGeoJSON: driver=‘GeoJSON’\nGeoPackage: driver=‘GPKG’\n\n\ngdf1106_freg.to_file(r'data\\geo\\c2021_fr1106.gpkg', layer='FR1106', driver=\"GPKG\")\n\nlistar outputs possíveis\n\nimport fiona\nfiona.supported_drivers\n\n{'DXF': 'rw',\n 'CSV': 'raw',\n 'OpenFileGDB': 'raw',\n 'ESRIJSON': 'r',\n 'ESRI Shapefile': 'raw',\n 'FlatGeobuf': 'raw',\n 'GeoJSON': 'raw',\n 'GeoJSONSeq': 'raw',\n 'GPKG': 'raw',\n 'GML': 'rw',\n 'OGR_GMT': 'rw',\n 'GPX': 'rw',\n 'Idrisi': 'r',\n 'MapInfo File': 'raw',\n 'DGN': 'raw',\n 'PCIDSK': 'raw',\n 'OGR_PDS': 'r',\n 'S57': 'r',\n 'SQLite': 'raw',\n 'TopoJSON': 'r'}\n\n\nexemplo para obter informação a partir de uma seleção dentro dum buffer\nimportar dados\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Simplify: https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.simplify.html\n# Exemplo Importar os dados da BGRI2021 \n# Caminho para o arquivo GeoPackage\ngpk = r'data\\geo\\BGRI2021_1106.gpkg'\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdf1106 = gpd.read_file(gpk,encoding='utf-8')\n\nmostrar mapa como follium\n\n# Function para mostrar folium\ndef mostrarFolium(gdfExtent,gdfDesenhar):\n    centroid = gdfExtent.to_crs(epsg=4326).unary_union.centroid\n\n    # Criar Listagem com localização de latitude  longitude\n    center_map = [centroid.y, centroid.x]\n    # Criar Mapa e mostrar\n    folium_map = folium.Map(location=center_map, zoom_start=15, tiles='OpenStreetMap')\n\n    # Adicionar Geografia folium map\n    # folium.GeoJson constructor\n    for lay in gdfDesenhar:\n        folium.GeoJson(lay).add_to(folium_map)\n    \n    folium_map\n\nexecutar o buffer\n\nfrom shapely.geometry import Point\n\n# Cria um objeto Point (podem escolher um ponto no google maps)\n# Atenção primeiro o valor x (longitude) e depois o y (latitutde)\nponto = Point(-9.137616,38.738561)\n\n# Cria um GeoDataFrame do ponto com CRS WGS84\ngdf_ponto = gpd.GeoDataFrame([1], geometry=[ponto], crs='EPSG:4326')\n\n# Manteer ponto Original\ngdf_ponto_original = gdf_ponto.copy()\n\n# Mudar a projeção:\ngdf_ponto = gdf_ponto.to_crs(gdf1106.crs)\n\n# Fazer o Buffer\ngdf_ponto['geometry'] = gdf_ponto.geometry.buffer(500)\n\n# Executar o Spatial join entre o ponto e as subsecções da BGRI2021\njoin = gpd.sjoin(gdf1106, gdf_ponto, how=\"inner\", predicate='intersects')\n\n# Somar os valores do atributo N_INDIVIDUOS\nsoma = join['N_INDIVIDUOS'].sum()\n\nprint(f\"Soma de N_INDIVIDUOS nos polígonos selecionados: {soma} em volta do ponto ({ponto.y},{ponto.x})\")\n\n\n#-----------------------------------------------------------------\n# Mostrar \nmostrarFolium(gdf_ponto,[gdf_ponto,gdf_ponto_original])\n\nSoma de N_INDIVIDUOS nos polígonos selecionados: 13943.0 em volta do ponto (38.738561,-9.137616)"
  },
  {
    "objectID": "900-mod9.html#desafio",
    "href": "900-mod9.html#desafio",
    "title": "9  Dados Geográficos",
    "section": "9.2 Desafio",
    "text": "9.2 Desafio\n\nDefinir um ponto, valores latitude e longitude\nCriar um Buffer em volta desse ponto\nFazer o cálculo da população nas subsecções envolventes\n\nPodem fazer download de outros GPK no site do INE: https://mapas.ine.pt/download/index2021.phtml\n\n# Exemplo Importar os dados da BGRI2021 para o municipio de Valongo\n# Caminho para o arquivo GeoPackage\ngpk = r'data\\geo\\BGRI2021_1315.gpkg'\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdf1315 = gpd.read_file(gpk,encoding='utf-8')\n\nfrom shapely.geometry import Point\n\n# Cria um objeto Point (podem escolher um ponto no google maps)\n# Atenção primeiro o valor x (longitude) e depois o y (latitutde)\n# ponto do google maps Sta Rita (Ermesinde)\n# 41.20722032311807, -8.541960984473912\nponto = Point(-8.541960984473912,41.20722032311807)\n\n# Cria um GeoDataFrame do ponto com CRS WGS84\ngdf_ponto = gpd.GeoDataFrame([1], geometry=[ponto], crs='EPSG:4326')\n\n# Manteer ponto Original\ngdf_ponto_original = gdf_ponto.copy()\n\n# Mudar a projeção:\ngdf_ponto = gdf_ponto.to_crs(gdf1315.crs)\n\n# Fazer o Buffer\ngdf_ponto['geometry'] = gdf_ponto.geometry.buffer(500)\n\n# Executar o Spatial join entre o ponto e as subsecções da BGRI2021\njoin = gpd.sjoin(gdf1315, gdf_ponto, how=\"inner\", predicate='intersects')\n\n# Somar os valores do atributo N_INDIVIDUOS\nsoma = join['N_INDIVIDUOS'].sum()\n\nprint(f\"Soma de N_INDIVIDUOS nos polígonos selecionados: {soma} em volta do ponto ({ponto.y},{ponto.x})\")\n\nmostrarFolium(gdf_ponto,[gdf_ponto,gdf_ponto_original])\n\nSoma de N_INDIVIDUOS nos polígonos selecionados: 4981.0 em volta do ponto (41.20722032311807,-8.541960984473912)"
  },
  {
    "objectID": "900-mod9.html#geoestatistica",
    "href": "900-mod9.html#geoestatistica",
    "title": "9  Dados Geográficos",
    "section": "9.3 GeoEstatistica",
    "text": "9.3 GeoEstatistica\n\nAnálise de Padroes espaciais\n\nautocorrelação espacial\nMédia do vizinho mais próximo\n\nMapeamento de clusters\n\nAnálise de clusters e outliers\nAnálise de Hot Spots\nClusterização multivariada\nIndices compostos\n\nModelação de relações espaciais\n\nRegressão linear\nRegressão geograficamente ponderada\nRegressão multiescalar geograficamente opnderada\nMinimos quadrados\nRelações locais bivariadas\n\n\n\n9.3.1 Autocorrelação Espacial\nClacular medidas\n- Moran Global\n- Moran Local\n- Getis and Ord's local statistics\n\nimport matplotlib.pyplot as plt  # Graphics\nfrom matplotlib import colors\nimport seaborn as sns  # Graphics\nimport geopandas as gpd # Spatial data manipulation\nimport pandas as pd  # Tabular data manipulation\n# Para Evitar Aviso Point Patterns\nfrom shapely.geometry import Point\n\n# Bibliotecas pysal\nimport pysal.lib # importação geral\nfrom pysal.explore import esda  # Exploratory Spatial analytics\nfrom pysal.lib import weights  # Spatial weights\nimport contextily  # Background tiles\n\n# Bibliotecas última parte notebook exemplo\n#import rioxarray  # Surface data manipulation\n#import xarray  # Surface data manipulation\n\nimportar dados do Geopackage com variáveis do C2021 da BRGI2021\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Caminho para o arquivo GeoPackage\ngpk = r'data\\geo\\BGRI2021_1106.gpkg'\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdf1106 = gpd.read_file(gpk)\n\n# Simplificar a geografia para uma precisão de 5 metros\ngdf1106['geometry'] = gdf1106['geometry'].simplify(tolerance=5)\n\n# Visualizar o GeoDataFrame\ngdf1106.plot(column = 'DTMNFR21',\n              legend = False)\n\n&lt;Axes: &gt;\n\n\n\n\n\nadicionar alguns atributos que permitem fazer a análise\n\n# Calcular Novo Atributo Racio de População 65+ anos\ngdf1106['IND65'] = gdf1106.N_INDIVIDUOS_65_OU_MAIS/gdf1106.N_INDIVIDUOS\n\n# Calcular Outros Atributos de interesse para Analisar: , N_EDIFICIOS_3_OU_MAIS_PISOS, N_INDIVIDUOS_H, N_INDIVIDUOS_M\ngdf1106['IND14'] = gdf1106.N_INDIVIDUOS_0_14/gdf1106.N_INDIVIDUOS\ngdf1106['IND_H'] = gdf1106.N_INDIVIDUOS_H/gdf1106.N_INDIVIDUOS\ngdf1106['IND_M'] = gdf1106.N_INDIVIDUOS_M/gdf1106.N_INDIVIDUOS\ngdf1106['EDIF_3PISOS'] = gdf1106.N_EDIFICIOS_3_OU_MAIS_PISOS/gdf1106.N_EDIFICIOS_CLASSICOS\n\n\n# Mostrar Dados\nprint(gdf1106[['BGRI2021', 'DTMNFR21', 'N_INDIVIDUOS_65_OU_MAIS', 'N_INDIVIDUOS', 'IND65', 'IND14', 'IND_H', 'IND_M','EDIF_3PISOS']].head(10))\n\n\n# Manter apenas as colunas de interesse: (não é necessário - simplifica o GeoDataFrame)\nmanter_colunas = ['geometry','BGRI2021', 'DTMNFR21', 'N_INDIVIDUOS_65_OU_MAIS', 'N_INDIVIDUOS', 'IND65','IND14', 'IND_H', 'IND_M','N_EDIFICIOS_CLASSICOS','EDIF_3PISOS']\ngdf1106 = gdf1106.loc[:, manter_colunas]\n\n#print(gdf1106.info())\n\n      BGRI2021 DTMNFR21  N_INDIVIDUOS_65_OU_MAIS  N_INDIVIDUOS     IND65  \\\n0  11065602301   110656                     30.0         202.0  0.148515   \n1  11065700203   110657                     62.0         197.0  0.314721   \n2  11065801011   110658                      3.0          21.0  0.142857   \n3  11065801012   110658                      7.0          20.0  0.350000   \n4  11065801013   110658                      4.0          22.0  0.181818   \n5  11060701205   110607                     28.0         178.0  0.157303   \n6  11065602601   110656                     39.0         195.0  0.200000   \n7  11061000601   110610                      1.0          12.0  0.083333   \n8  11065700110   110657                     12.0          42.0  0.285714   \n9  11061001305   110610                     19.0          87.0  0.218391   \n\n      IND14     IND_H     IND_M  EDIF_3PISOS  \n0  0.069307  0.524752  0.475248     0.933333  \n1  0.131980  0.380711  0.619289     1.000000  \n2  0.190476  0.571429  0.428571     0.250000  \n3  0.200000  0.400000  0.600000     0.000000  \n4  0.318182  0.409091  0.590909     0.000000  \n5  0.095506  0.516854  0.483146     0.162162  \n6  0.117949  0.420513  0.579487     0.785714  \n7  0.000000  0.416667  0.583333     0.000000  \n8  0.047619  0.404762  0.595238     1.000000  \n9  0.160920  0.505747  0.494253     0.714286  \n\n\ntartar NaN\n\n# Contar o número total de NaNs no DataFrame\ntotal_nans = gdf1106.isna().sum().sum()\nprint('Número total de registros com NaN:', total_nans)\n\n# Contar o número de NaNs em cada coluna\nnans_por_coluna = gdf1106.isna().sum()\nprint('Número de registros com NaN por coluna:\\n', nans_por_coluna)\n\n# Corrigir NaN\n# Existem 2 possibilidades\n# 1. Deixar fora\n#gdf1106 = gdf1106.dropna()\n# 2. Substituir por outros valores\ngdf1106 = gdf1106.fillna(0)\n\nNúmero total de registros com NaN: 781\nNúmero de registros com NaN por coluna:\n geometry                     0\nBGRI2021                     0\nDTMNFR21                     0\nN_INDIVIDUOS_65_OU_MAIS      0\nN_INDIVIDUOS                 0\nIND65                      157\nIND14                      157\nIND_H                      157\nIND_M                      157\nN_EDIFICIOS_CLASSICOS        0\nEDIF_3PISOS                153\ndtype: int64\n\n\nvisualização inicial\n\n# Set up figure and a single axis\nf, ax = plt.subplots(1, figsize=(9, 9))\n# Build choropleth\ngdf1106.plot(\n    column=\"IND65\",\n    cmap=\"viridis\",\n    scheme=\"quantiles\",\n    k=5,\n    edgecolor=\"white\",\n    linewidth=0.0,\n    alpha=0.75,\n    legend=True,\n    legend_kwds=dict(loc=2),\n    ax=ax,\n)\n# Add basemap\ncontextily.add_basemap(\n    ax,\n    crs=gdf1106.crs,\n    source=contextily.providers.CartoDB.VoyagerNoLabels,\n)\n# Remove axes\nax.set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n9.3.2 Matriz de vizinhança e Moran Global\nQueen\nf we wanted them to be considered as neighbours, we can switch to the more inclusive notion of Queen contiguity, which requires the pair of polygons to only share one or more vertices. We can create the neighbor relations for this same configuration as follows:\nKNN\nThe first type of distance based weights defines the neighbor set of a particular observation as containing its nearest observations, where the user specifies the value of . To illustrate this for the San Diego tracts, we take . This still leaves the issue of how to measure the distance between these polygon objects, however. To do so we develop a representative point for each of the polygons using the centroid.\n\nfrom pysal.model import spreg\n\n# Input gdf1106 (BGRI de Lisboa)\n\n# Calcular a matriz de pesos espaciais\n# Metodo Original: 0.13069266093679838 e Valor-p: 0.001\nw = pysal.lib.weights.Queen.from_dataframe(gdf1106, use_index=True)\n# Metodo knn (I de Moran: 0.11720923169446687; Valor-p: 0.001)\n#w = pysal.lib.weights.KNN.from_dataframe(gdf1106, k=5)\n\n# Lidar com ilhas\n# Normalizar a matriz de pesos\n# Row-standardization \nw.transform = \"r\"\n\n# Corrigir para ilhas\n#w.set_transform('r')\nislands = w.islands\nif islands:\n    for island in islands:\n        w.neighbors[island] = [island]\n        w.weights[island] = [1]  \n\n\n# Selecionar a coluna com a percentagem de população superior a 65 anos\ndata = gdf1106['IND65']\n\n# Calcular a estatística de Moran\nmoran = esda.Moran(data, w)\n\n# Imprimir o valor I de Moran e o valor-p\nprint('I de Moran:', moran.I)\nprint('Valor-p:', moran.p_sim)\n\n# Plotar o gráfico de dispersão de Moran (desatividado - vai ser efetuado em tarefas na próxima seção)\n#plot_moran(moran, zstandard=True, fill=True, figsize=(10,4))\n\nI de Moran: 0.15343960749795466\nValor-p: 0.001\n\n\nInterpretação do valor Moran Global\n\\(H_0: I = 0\\) (padran aleatorio) vs \\(H_1 \\ne 0\\) (padrão espacial com clusters)\n\n\n9.3.3 Motivating local spatial autocorrelation (Moran Plot)\nNesta Parte é efetuado o seguinte:\n\nCalcular o Spatial Lag\nCalcular as versões centradas\nCriar um Scatterplot\nVisualizar a distribuição dos valores em relação ao médio e aos valores nos polígonos vizinhos\n\nMoran Plot\nThe Moran Plot is a way of visualizing a spatial dataset to explore the nature and strength of spatial autocorrelation. It is essentially a traditional scatter plot in which the variable of interest is displayed against its spatial lag. In order to be able to interpret values as above or below the mean, the variable of interest is usually standardized by subtracting its mean:\nCalcular o Spatial log\n\n# Calcular o Spatial Lag\n# \ngdf1106[\"w_IND65\"] = weights.lag_spatial(w, gdf1106['IND65'])\n\n# And their respective centered versions, where we subtract the average off of every value\ngdf1106[\"IND65_std\"] = gdf1106[\"IND65\"] - gdf1106[\"IND65\"].mean()\ngdf1106[\"w_IND65_std\"] = weights.lag_spatial(w, gdf1106['IND65_std'])\n\n\n# Visualizar Valores em Cima e Baixo do Médio\n# Set up the figure and axis\nf, ax = plt.subplots(1, figsize=(6, 6))\n# Plot values\n\nsns.regplot(\n    x=\"IND65_std\", y=\"w_IND65_std\", data=gdf1106, ci=None\n)\nplt.show()\n\n\n\n\nFigura com a relação das vizinhanças (Moran Plot)\nMesma figura com indicação dos 4 quadrantos\n\nLH: Valores na subsecção em baixo do médio, valores circundantes em cima do médio\nHH: Valores na subsecção em cima do médio, valores circundantes em cima do médio\nLL: Valores na subsecção em baixo do médio, valores circundantes em baixo do médio\nHL: Valores na subsecção em cima do médio, valores circundantes em baixo do médio\n\n\n# Criar os Quadrantos\n# Set up the figure and axis\nf, ax = plt.subplots(1, figsize=(6, 6))\n# Plot values\nsns.regplot(\n    x=\"IND65_std\", y=\"w_IND65_std\", data=gdf1106, ci=None\n)\n\n# Esta Parte demora muito tempo\n\n# Add vertical and horizontal lines (definição valor onde adicionar)\nplt.axvline(0, c=\"k\", alpha=0.5)\nplt.axhline(0, c=\"k\", alpha=0.5)\n\n# Adicionar Text para Cada Quadrant - coordinados Text tendo em conta a distribuição dos dados\nplt.text(0.6, 0.20, \"HH\", fontsize=25, c=\"r\")\nplt.text(0.6, -0.15, \"HL\", fontsize=25, c=\"r\")\nplt.text(-0.2, 0.20, \"LH\", fontsize=25, c=\"r\")\nplt.text(-0.15, -0.15, \"LL\", fontsize=25, c=\"r\")\n\n# Displaby\nplt.show()\n\n\n\n\n\n\n9.3.4 Local Moran’s I\nCalcular LISA (Local Indicators of Spatial Association) e Visualização Inicial\n\n# https://pysal.org/esda/generated/esda.Moran_Local.html\n# lisa = moran_loc\n# Diferença com Notebook da Formação - dá erro porque os valores são NaN\nfrom splot.esda import lisa_cluster\n# data = \nlisa = esda.moran.Moran_Local(gdf1106['IND65'], w)\n\n# Plotar o mapa de clusters de Moran\nlisa_cluster(lisa, gdf1106, figsize=(9,9))\n\n(&lt;Figure size 864x864 with 1 Axes&gt;, &lt;Axes: &gt;)\n\n\n\n\n\nKernel Estimate Plotting\n\nimport numpy as np\n\n# Valores LISa primeiros 10 registos\nprint(lisa.Is[:10])\n\n# alguns indacores dos valores\nprint(f'''Minimum: {np.min(lisa.Is)}\nMaximum: {np.max(lisa.Is)}\nSTD: {np.std(lisa.Is)}''')\n\n[ 0.15105448 -0.01156592  0.45131911  0.98016347  0.13006217 -0.22294648\n  0.04772319 -0.98709029  0.0033492   0.01171758]\nMinimum: -6.358580055279035\nMaximum: 5.47984335779404\nSTD: 0.6495277258099246\n\n\n\nimport warnings\n\n# Nao mostrar aviso FutereWarning (não aconselhável)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Draw KDE line\nax = sns.kdeplot(lisa.Is)\n\n# Add one small bar (rug) for each observation\n# along horizontal axis\nsns.rugplot(lisa.Is, ax=ax)\n\nplt.show()\n\n\n\n\nVisualizações diferentes, mapas com medidas LISA\nSignificado dos 4 mapas: 1. Valor LISA cada polígono (valor lisa.Is) 2. Valor do quadrante cada área (esdaplot.lisa_cluster, p = 1) 3. Indicação da significância estatística (lisa.p_sim &lt; 0.05) 4. Combinação dos anterior 2 (esdaplot.lisa_cluster, p = 0.05)\n\nfrom splot import esda as esdaplot\n\n\n# Set up figure and axes\nf, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n# Make the axes accessible with single indexing\naxs = axs.flatten()\n\n# Subplot 1 #\n# Choropleth of local statistics\n# Grab first axis in the figure\nax = axs[0]\n# Assign new column with local statistics on-the-fly\ngdf1106.assign(\n    Is=lisa.Is\n    # Plot choropleth of local statistics\n).plot(\n    column=\"Is\",\n    cmap=\"plasma\",\n    scheme=\"quantiles\",\n    k=5,\n    edgecolor=\"white\",\n    linewidth=0.1,\n    alpha=0.75,\n    legend=True,\n    ax=ax,\n)\n\n# Subplot 2 #\n# Quadrant categories\n# Grab second axis of local statistics\nax = axs[1]\n# Plot Quadrant colors (note to ensure all polygons are assigned a\n# quadrant, we \"trick\" the function by setting significance level to\n# 1 so all observations are treated as \"significant\" and thus assigned\n# a quadrant color\nesdaplot.lisa_cluster(lisa, gdf1106, p=1, ax=ax)\n\n# Subplot 3 #\n# Significance map\n# Grab third axis of local statistics\nax = axs[2]\n#\n# Find out significant observations\nlabels = pd.Series(\n    1 * (lisa.p_sim &lt; 0.05),  # Assign 1 if significant, 0 otherwise\n    index=gdf1106.index  # Use the index in the original data\n    # Recode 1 to \"Significant and 0 to \"Non-significant\"\n).map({1: \"Significant\", 0: \"Non-Significant\"})\n# Assign labels to `gdf1106` on the fly\ngdf1106.assign(\n    cl=labels\n    # Plot choropleth of (non-)significant areas\n).plot(\n    column=\"cl\",\n    categorical=True,\n    k=2,\n    cmap=\"Paired\",\n    linewidth=0.1,\n    edgecolor=\"white\",\n    legend=True,\n    ax=ax,\n)\n\n\n# Subplot 4 #\n# Cluster map\n# Grab second axis of local statistics\nax = axs[3]\n# Plot Quadrant colors In this case, we use a 5% significance\n# level to select polygons as part of statistically significant\n# clusters\nesdaplot.lisa_cluster(lisa, gdf1106, p=0.05, ax=ax)\n\n# Figure styling #\n# Set title to each subplot\nfor i, ax in enumerate(axs.flatten()):\n    ax.set_axis_off()\n    ax.set_title(\n        [\n            \"Local Statistics\",\n            \"Scatterplot Quadrant\",\n            \"Statistical Significance\",\n            \"Moran Cluster Map\",\n        ][i],\n        y=0,\n    )\n# Tight layout to minimize in-between white space\nf.tight_layout()\n\n# Display the figure\nplt.show()\n\n\n\n\nDiferentes contagens (Atributo “q”)\n\nAtributo ‘q’ do Moran Local mostra os valores nos 4 quadrantos\n\n\n# Criar pd.series do resultado\ncounts = pd.Series(lisa.q).value_counts() # Original: pd.value_counts(lisa.q)\ncounts\n\n# Visualizar os valores do primeiros 10 registos \n\nlisa.q[:10]\n\n# Visualizar o numero de áreas com valores estatitisticamente significante\n\n(lisa.p_sim &lt; 0.05).sum() * 100 / len(lisa.p_sim)\n\n16.087880935506732\n\n\n\n\n9.3.5 Getis and Ord’s local statistics\nOutras medidas para representar a autocorrelação espacial (Getis and Ord’s)\n\nthe Gi: statistic, which omits the value at a site in its local summary\nthe Gi*: statistic, which includes the site’s own value in the local summary\n\ncalcular indicadores\n\n# Gi\ngo_i = esda.getisord.G_Local(gdf1106[\"IND65\"], w) # , star=None\n# Gi*\ngo_i_star = esda.getisord.G_Local(gdf1106[\"IND65\"], w, star=True)\n\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\esda\\getisord.py:615: UserWarning: Gi* requested, but (a) weights are already row-standardized, (b) no weights are on the diagonal, and (c) no default value supplied to star. Assuming that the self-weight is equivalent to the maximum weight in the row. To use a different default (like, .5), set `star=.5`, or use libpysal.weights.fill_diagonal() to set the diagonal values of your weights matrix and use `star=None` in Gi_Local.\n  warnings.warn(\n\n\nvisualizar o resultado\n\ndef g_map(g, db, ax, title):\n    \"\"\"\n    Create a cluster map\n    ...\n\n    Arguments\n    ---------\n    g      : G_Local\n             Object from the computation of the G statistic\n    db     : GeoDataFrame\n             Table aligned with values in `g` and containing\n             the geometries to plot\n    ax     : AxesSubplot\n             `matplotlib` axis to draw the map on\n\n    Returns\n    -------\n    ax     : AxesSubplot\n             Axis with the map drawn\n    \"\"\"\n    ec = \"0.8\"\n\n    # Break observations into significant or not\n    sig = g.p_sim &lt; 0.05\n\n    # Plot non-significant clusters\n    ns = db.loc[sig == False, \"geometry\"]\n    ns.plot(ax=ax, color=\"lightgrey\", edgecolor=ec, linewidth=0.1)\n    # Plot HH clusters\n    hh = db.loc[(g.Zs &gt; 0) & (sig == True), \"geometry\"]\n    hh.plot(ax=ax, color=\"red\", edgecolor=ec, linewidth=0.1)\n    # Plot LL clusters\n    ll = db.loc[(g.Zs &lt; 0) & (sig == True), \"geometry\"]\n    ll.plot(ax=ax, color=\"blue\", edgecolor=ec, linewidth=0.1)\n    # Style and draw\n    contextily.add_basemap(\n        ax,\n        crs=db.crs,\n        source=contextily.providers.Esri.WorldTerrain,\n    )\n    # Flag to add a star to the title if it's G_i*\n    st = \"\"\n    if g.star:\n        st = \"*\"\n    # Add title\n    ax.set_title(f\"G{st} statistic {title}\", size=15)\n    # Remove axis for aesthetics\n    ax.set_axis_off()\n    return ax\n\n\n# Set up figure and axes\nf, axs = plt.subplots(1, 2, figsize=(12, 6))\n# Loop over the two statistics\nfor g, ax in zip([go_i, go_i_star], axs.flatten()):\n    # Generate the statistic's map\n    ax = g_map(g, gdf1106, ax, 'Rácio Popuação 65+ Anos')\n# Tight layout to minimise blank spaces\nf.tight_layout()\n# Render\nplt.show()\n\n\n\n\n\n\n9.3.6 Exercício\n**Repetir o código com outras variáveis ecom município de Porto ou outro município\n\nUtilizar o codigo Anterior para Calcular para outras variaveis\n\nLista variáveis: ‘IND65’,‘IND14’, ‘IND_H’, ‘IND_M’,‘EDIF_3PISOS’\n\nRepete a mesma análise para outro municipio\n\nFaz download do gpk de um municipio no link: https://mapas.ine.pt/download/index2021.phtml\n\n\n\n# Caminho para o arquivo GeoPackage do municipio de Valongo\ngpk = r\"data\\geo\\BGRI2021_1315.gpkg\"\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdf1315 = gpd.read_file(gpk)\n\n# Calcular Novo Atributo Racio de População 65+ anos\ngdf1315['IND65'] = gdf1315.N_INDIVIDUOS_65_OU_MAIS/gdf1315.N_INDIVIDUOS\n\n# Calcular Outros Atributos de interesse para Analisar: , N_EDIFICIOS_3_OU_MAIS_PISOS, N_INDIVIDUOS_H, N_INDIVIDUOS_M\ngdf1315['IND14'] = gdf1315.N_INDIVIDUOS_0_14/gdf1315.N_INDIVIDUOS\ngdf1315['IND_H'] = gdf1315.N_INDIVIDUOS_H/gdf1315.N_INDIVIDUOS\ngdf1315['IND_M'] = gdf1315.N_INDIVIDUOS_M/gdf1315.N_INDIVIDUOS\ngdf1315['EDIF_3PISOS'] = gdf1315.N_EDIFICIOS_3_OU_MAIS_PISOS/gdf1315.N_EDIFICIOS_CLASSICOS\n\n\n# Mostrar Dados\nprint(gdf1315[['BGRI2021', 'DTMNFR21', 'N_INDIVIDUOS_65_OU_MAIS', 'N_INDIVIDUOS', 'IND65', 'IND14', 'IND_H', 'IND_M','EDIF_3PISOS']].head(10))\n\n\nfrom pysal.model import spreg\n\n# Input gdf1106 (BGRI de Lisboa)\n\n# Calcular a matriz de pesos espaciais\n# Metodo Original: 0.13069266093679838 e Valor-p: 0.001\nw = pysal.lib.weights.Queen.from_dataframe(gdf1315, use_index=True)\n# Metodo knn (I de Moran: 0.11720923169446687; Valor-p: 0.001)\n#w = pysal.lib.weights.KNN.from_dataframe(gdf1106, k=5)\n\n# Lidar com ilhas\n# Normalizar a matriz de pesos\n# Row-standardization \nw.transform = \"r\"\n\n# Corrigir para ilhas\n#w.set_transform('r')\nislands = w.islands\nif islands:\n    for island in islands:\n        w.neighbors[island] = [island]\n        w.weights[island] = [1]  \n        \n# Selecionar a coluna com a percentagem de população superior a 65 anos\ndata = gdf1315['IND65']\n\n\n# Calcular a estatística de Moran\nmoran = esda.Moran(data, w)\n\n# Imprimir o valor I de Moran e o valor-p\nprint('I de Moran:', moran.I)\nprint('Valor-p:', moran.p_sim)\n\n      BGRI2021 DTMNFR21  N_INDIVIDUOS_65_OU_MAIS  N_INDIVIDUOS     IND65  \\\n0  13150600202   131506                      3.0          39.0  0.076923   \n1  13150300506   131503                      7.0          36.0  0.194444   \n2  13150600203   131506                      9.0          38.0  0.236842   \n3  13150600204   131506                      3.0          14.0  0.214286   \n4  13150300907   131503                     87.0         497.0  0.175050   \n5  13150301501   131503                     11.0          24.0  0.458333   \n6  13150302404   131503                     60.0         262.0  0.229008   \n7  13150300601   131503                     11.0          51.0  0.215686   \n8  13150301601   131503                     37.0         125.0  0.296000   \n9  13150301102   131503                     44.0         188.0  0.234043   \n\n      IND14     IND_H     IND_M  EDIF_3PISOS  \n0  0.076923  0.487179  0.512821     0.083333  \n1  0.111111  0.500000  0.500000     0.000000  \n2  0.131579  0.526316  0.473684     0.066667  \n3  0.142857  0.500000  0.500000     0.666667  \n4  0.152918  0.496982  0.503018     0.067485  \n5  0.083333  0.458333  0.541667     0.125000  \n6  0.125954  0.442748  0.557252     0.454545  \n7  0.098039  0.529412  0.470588     0.444444  \n8  0.056000  0.456000  0.544000     0.500000  \n9  0.117021  0.494681  0.505319     0.812500  \n\n\nI de Moran: nan\nValor-p: 0.001\n\n\n\n# Calcular o Spatial Lag\n# \ngdf1315[\"w_IND14\"] = weights.lag_spatial(w, gdf1315['IND14'])\n\n# And their respective centered versions, where we subtract the average off of every value\ngdf1315[\"IND14_std\"] = gdf1315[\"IND14\"] - gdf1315[\"IND14\"].mean()\ngdf1315[\"w_IND14_std\"] = weights.lag_spatial(w, gdf1315['IND14_std'])\n\n\n# Visualizar Valores em Cima e Baixo do Médio\n# Set up the figure and axis\nf, ax = plt.subplots(1, figsize=(6, 6))\n# Plot values\n\nsns.regplot(\n    x=\"IND14_std\", y=\"w_IND14_std\", data=gdf1315 ci=None\n)\n\n\n# Criar os Quadrantos\n# Set up the figure and axis\nf, ax = plt.subplots(1, figsize=(6, 6))\n# Plot values\nsns.regplot(\n    x=\"IND14_std\", y=\"w_IND14_std\", data=gdf1315, ci=None\n)\n\n# Esta Parte demora muito tempo\n\n# Add vertical and horizontal lines (definição valor onde adicionar)\nplt.axvline(0, c=\"k\", alpha=0.5)\nplt.axhline(0, c=\"k\", alpha=0.5)\n\n# # Adicionar Text para Cada Quadrant - coordinados Text tendo em conta a distribuição dos dados\n# plt.text(0.6, 0.20, \"HH\", fontsize=25, c=\"r\")\n# plt.text(0.6, -0.15, \"HL\", fontsize=25, c=\"r\")\n# plt.text(-0.2, 0.20, \"LH\", fontsize=25, c=\"r\")\n# plt.text(-0.15, -0.15, \"LL\", fontsize=25, c=\"r\")\n\n# Displaby\nplt.show()"
  },
  {
    "objectID": "900-mod9.html#clustering-e-regionalization",
    "href": "900-mod9.html#clustering-e-regionalization",
    "title": "9  Dados Geográficos",
    "section": "9.4 Clustering e Regionalization",
    "text": "9.4 Clustering e Regionalization\n\nimport matplotlib.pyplot as plt  # Graphics\nfrom matplotlib import colors\nimport seaborn as sns  # Graphics\nimport geopandas as gpd # Spatial data manipulation\nimport pandas as pd  # Tabular data manipulation\nimport numpy as np\n\n# Para Evitar Aviso Point Patterns\nfrom shapely.geometry import Point\nimport contextily  # Background tiles\n\n# Bibliotecas Referidos no Notebook\nfrom esda.moran import Moran\nfrom libpysal.weights import Queen, KNN\n#import pysal.lib # importação geral\nfrom pysal.explore import esda  # Exploratory Spatial analytics\nfrom pysal.lib import weights  # Spatial weights\n\npreparação dos dados\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Caminho para o arquivo GeoPackage\ngpk = r'data\\geo\\BGRI2021_1106.gpkg'\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdf1106 = gpd.read_file(gpk)\n\n# Simplificar a geografia para uma precisão de 5 metros\ngdf1106['geometry'] = gdf1106['geometry'].simplify(tolerance=5)\n\n# Visualizar o GeoDataFrame\ngdf1106.plot(column = 'DTMNFR21',\n              legend = False)\n              \n# Nomes Atributos\n#gdf1106.info()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# Incluir os nomes das variáveis que devem ser utilizados para criar os agrupamentos (clusters)\ncluster_variables = [\n    gdf1106.columns[13],  # \n    gdf1106.columns[14],  # \n    gdf1106.columns[15],  # \n    gdf1106.columns[31],  # \n    gdf1106.columns[32],  # \n    gdf1106.columns[37],  # \n    gdf1106.columns[41],  # \n    gdf1106.columns[42],  # E\n    gdf1106.columns[44] # \n]\n\n\n# Tratamento NaN\n# Contar o número total de NaNs no DataFrame\ntotal_nans = gdf1106.isna().sum().sum()\nprint('Número total de registros com NaN:', total_nans)\n\n# Contar o número de NaNs em cada coluna\nnans_por_coluna = gdf1106.isna().sum()\nprint('Número de registros com NaN por coluna:\\n', nans_por_coluna)\n\n# Preencher com valor 0\ngdf1106 = gdf1106.fillna(0)\n\nNúmero total de registros com NaN: 0\nNúmero de registros com NaN por coluna:\n OBJECTID                                                         0\nBGRI2021                                                         0\nDT21                                                             0\nDTMN21                                                           0\nDTMNFR21                                                         0\nDTMNFRSEC21                                                      0\nSECNUM21                                                         0\nSSNUM21                                                          0\nSECSSNUM21                                                       0\nSUBSECCAO                                                        0\nNUTS1                                                            0\nNUTS2                                                            0\nNUTS3                                                            0\nN_EDIFICIOS_CLASSICOS                                            0\nN_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ                              0\nN_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS                    0\nN_EDIFICIOS_EXCLUSIV_RESID                                       0\nN_EDIFICIOS_1_OU_2_PISOS                                         0\nN_EDIFICIOS_3_OU_MAIS_PISOS                                      0\nN_EDIFICIOS_CONSTR_ANTES_1945                                    0\nN_EDIFICIOS_CONSTR_1946_1980                                     0\nN_EDIFICIOS_CONSTR_1981_2000                                     0\nN_EDIFICIOS_CONSTR_2001_2010                                     0\nN_EDIFICIOS_CONSTR_2011_2021                                     0\nN_EDIFICIOS_COM_NECESSIDADES_REPARACAO                           0\nN_ALOJAMENTOS_TOTAL                                              0\nN_ALOJAMENTOS_FAMILIARES                                         0\nN_ALOJAMENTOS_FAM_CLASS_RHABITUAL                                0\nN_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA                0\nN_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS                             0\nN_RHABITUAL_COM_ESTACIONAMENTO                                   0\nN_RHABITUAL_PROP_OCUP                                            0\nN_RHABITUAL_ARRENDADOS                                           0\nN_AGREGADOS_DOMESTICOS_PRIVADOS                                  0\nN_ADP_1_OU_2_PESSOAS                                             0\nN_ADP_3_OU_MAIS_PESSOAS                                          0\nN_NUCLEOS_FAMILIARES                                             0\nN_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25    0\nN_INDIVIDUOS                                                     0\nN_INDIVIDUOS_H                                                   0\nN_INDIVIDUOS_M                                                   0\nN_INDIVIDUOS_0_14                                                0\nN_INDIVIDUOS_15_24                                               0\nN_INDIVIDUOS_25_64                                               0\nN_INDIVIDUOS_65_OU_MAIS                                          0\nSHAPE_Length                                                     0\nSHAPE_Area                                                       0\ngeometry                                                         0\ndtype: int64\n\n\n\n# Mostrar como mapas temáticos os valores dos atributos escolhidos  \n\nf, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n# Make the axes accessible with single indexing\naxs = axs.flatten()\n# Start a loop over all the variables of interest\nfor i, col in enumerate(cluster_variables):\n    # select the axis where the map will go\n    ax = axs[i]\n    # Plot the map\n    gdf1106.plot(\n        column=col,\n        ax=ax,\n        scheme=\"Quantiles\",\n        linewidth=0,\n        cmap=\"RdPu\",\n    )\n    # Remove axis clutter\n    ax.set_axis_off()\n    # Set the axis title to the name of variable being plotted\n    ax.set_title(col)\n# Display the figure\nplt.show()\n\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\mapclassify\\classifiers.py:1592: UserWarning: Not enough unique values in array to form 5 classes. Setting k to 4.\n  self.bins = quantile(y, k=k)\n\n\n\n\n\nCalcular Moran I para todas as variáveis\n\n# Generate W from the GeoDataFrame\n# w = weights.distance.KNN.from_dataframe(gdf1106, k=8)\n# Metodo Alternativo:\nw = weights.Queen.from_dataframe(gdf1106, use_index=True)\n\n\n# Set seed for reproducibility\nnp.random.seed(123456)\n# Calculate Moran's I for each variable\nmi_results = [\n    Moran(gdf1106[variable], w) for variable in cluster_variables\n    ]\n# Structure results as a list of tuples\nmi_results = [\n    (variable, res.I, res.p_sim)\n    for variable, res in zip(cluster_variables, mi_results)\n]\n# Display on table\ntable = pd.DataFrame(\n    mi_results, columns=[\"Variable\", \"Moran's I\", \"P-value\"]\n).set_index(\"Variable\")\ntable\n\n\n\n\n\n\n\n\nMoran's I\nP-value\n\n\nVariable\n\n\n\n\n\n\nN_EDIFICIOS_CLASSICOS\n0.300793\n0.001\n\n\nN_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ\n0.384476\n0.001\n\n\nN_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS\n0.434427\n0.001\n\n\nN_RHABITUAL_PROP_OCUP\n0.420872\n0.001\n\n\nN_RHABITUAL_ARRENDADOS\n0.370622\n0.001\n\n\nN_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25\n0.391527\n0.001\n\n\nN_INDIVIDUOS_0_14\n0.359343\n0.001\n\n\nN_INDIVIDUOS_15_24\n0.371393\n0.001\n\n\nN_INDIVIDUOS_65_OU_MAIS\n0.420039\n0.001\n\n\n\n\n\n\n\n\n# Utilizar kdeplot dá um aviso\nimport warnings\n\n# Nao mostrar aviso FutereWarning (não aconselhável)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Mostrar kdeplot para cada variável\n# _: Convenção Python que mostra que o resultado não está a ser utilizado\n\nsns.pairplot(\n    gdf1106[cluster_variables], kind=\"reg\", diag_kind=\"kde\"\n)\n\n\n\n\n\n# The distance between observations in terms of these variates can be computed easily using\nfrom sklearn import metrics\nmetrics.pairwise_distances(\n    gdf1106[[cluster_variables[0], cluster_variables[5]]].head()\n).round(4)\n\narray([[ 0.    ,  8.6023, 13.0384, 13.8924, 12.2066],\n       [ 8.6023,  0.    , 18.1108, 19.105 , 17.1172],\n       [13.0384, 18.1108,  0.    ,  1.    ,  1.    ],\n       [13.8924, 19.105 ,  1.    ,  0.    ,  2.    ],\n       [12.2066, 17.1172,  1.    ,  2.    ,  0.    ]])\n\n\n\nfrom sklearn.preprocessing import robust_scale\n# And create the db_scaled object which contains only the variables we are interested in, scaled:\ndb_scaled = robust_scale(gdf1106[cluster_variables])\nprint(type(db_scaled))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n9.4.1 Cluster GeoDemograficos\n\n\n9.4.2 K-means\n\n# Initialize KMeans instance\nfrom sklearn.cluster import KMeans\n\n# Initialize KMeans instance\n# Definir n_init explicitemente\nkmeans = KMeans(n_clusters=5,n_init=10)\n\n# Set the seed for reproducibility\nnp.random.seed(1234)\n# Run K-Means algorithm\nk5cls = kmeans.fit(db_scaled)\n\n# Print first five labels\nk5cls.labels_[:5]\n\narray([3, 3, 1, 1, 1])\n\n\nvisualização dos resultados\n\n# Assign labels into a column\ngdf1106[\"k5cls\"] = k5cls.labels_\n# Set up figure and ax\nf, ax = plt.subplots(1, figsize=(9, 9))\n# Plot unique values choropleth including\n# a legend and with no boundary lines\ngdf1106.plot(\n    column=\"k5cls\", categorical=True, legend=True, linewidth=0, ax=ax,\n    legend_kwds={'loc': 'upper left'}\n)\n# Remove axis\nax.set_axis_off()\n# Display the map\nplt.show()\n\n\n\n\nAnálise dos resultados obtidos\n\n# Group data table by cluster label and count observations\nk5sizes = gdf1106.groupby(\"k5cls\").size()\nk5sizes\n\nk5cls\n0     269\n1    1225\n2     110\n3     841\n4     377\ndtype: int64\n\n\nobter a área de cada grupo (dissolve) utilizando o atributo SHAPE_Area\n\n# Dissolve areas by Cluster, aggregate by summing,\n# and keep column for area\nareas = gdf1106.dissolve(by=\"k5cls\", aggfunc=\"sum\")[\"SHAPE_Area\"]\nareas\n\nk5cls\n0    1.147251e+07\n1    5.281033e+07\n2    7.158524e+06\n3    1.926728e+07\n4    9.345682e+06\nName: SHAPE_Area, dtype: float64\n\n\nnº de elementos vs área de cada grupo\n\n# Visualizar o nº de elementos vs a Área total\n\n# Bind cluster figures in a single table\narea_tracts = pd.DataFrame({\"No. Tracts\": k5sizes, \"Area\": areas})\n# Convert raw values into percentages\narea_tracts = area_tracts * 100 / area_tracts.sum()\n# Bar plot\nax = area_tracts.plot.bar()\n# Rename axes\nax.set_xlabel(\"Cluster labels\")\nax.set_ylabel(\"Percentage by cluster\")\n\nplt.show()\n\n\n\n\nCriar um perfil de cada cluster, executamos as seguintes tarefas:\n\nCalcular os médios de cada variável\nArrumar (tidy up) os dados, assegurando que cada linha é uma observação e cada coluna é uma variável\n\nVisualizar a distribuição dos valores das variáveis por grupo\n\n\n# Criar os Perfis de cada Cluster em relação as variáveis de input\n# Mostrar os médios das variáveis em cada cluster\n# Group table by cluster label, keep the variables used\n# for clustering, and obtain their mean\nk5means = gdf1106.groupby(\"k5cls\")[cluster_variables].mean()\n# Transpose the table and print it rounding each value\n# to three decimals\nk5means.T.round(3)\n\n\n\n\n\n\n\nk5cls\n0\n1\n2\n3\n4\n\n\n\n\nN_EDIFICIOS_CLASSICOS\n27.112\n7.980\n63.055\n17.776\n27.236\n\n\nN_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ\n2.584\n2.332\n51.491\n1.860\n20.599\n\n\nN_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS\n24.227\n5.529\n11.318\n15.744\n6.552\n\n\nN_RHABITUAL_PROP_OCUP\n138.301\n13.353\n41.400\n66.421\n20.939\n\n\nN_RHABITUAL_ARRENDADOS\n114.539\n13.180\n49.818\n50.599\n19.419\n\n\nN_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25\n85.810\n7.092\n28.118\n36.056\n12.011\n\n\nN_INDIVIDUOS_0_14\n86.721\n7.330\n28.955\n36.922\n12.406\n\n\nN_INDIVIDUOS_15_24\n67.617\n5.908\n21.355\n28.056\n9.812\n\n\nN_INDIVIDUOS_65_OU_MAIS\n140.792\n15.361\n53.645\n66.652\n24.199\n\n\n\n\n\n\n\n\n# Index db on cluster ID\ntidy_db = gdf1106.set_index(\"k5cls\")\n# Keep only variables used for clustering\ntidy_db = tidy_db[cluster_variables]\n# Stack column names into a column, obtaining\n# a \"long\" version of the dataset\ntidy_db = tidy_db.stack()\n# Take indices into proper columns\ntidy_db = tidy_db.reset_index()\n# Rename column names\ntidy_db = tidy_db.rename(\n    columns={\"level_1\": \"Attribute\", 0: \"Values\"}\n)\n# Check out result\ntidy_db.head()\n\n\n\n\n\n\n\n\nk5cls\nAttribute\nValues\n\n\n\n\n0\n3\nN_EDIFICIOS_CLASSICOS\n15.0\n\n\n1\n3\nN_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ\n1.0\n\n\n2\n3\nN_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS\n14.0\n\n\n3\n3\nN_RHABITUAL_PROP_OCUP\n30.0\n\n\n4\n3\nN_RHABITUAL_ARRENDADOS\n45.0\n\n\n\n\n\n\n\n\n# hows the distribution of each cluster’s values for each variable. \n# This gives us the full distributional profile of each cluster:\n# Scale fonts to make them more readable\nsns.set(font_scale=1.5)\n# Setup the facets\nfacets = sns.FacetGrid(\n    data=tidy_db,\n    col=\"Attribute\",\n    hue=\"k5cls\",\n    sharey=False,\n    sharex=False,\n    aspect=2,\n    col_wrap=3,\n)\n# Build the plot from `sns.kdeplot`\nfacets.map(sns.kdeplot, \"Values\", shade=True).add_legend()\n\n\n\n\n\n\n9.4.3 Hierarchical Clustering\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Set seed for reproducibility\nnp.random.seed(0)\n# Initialize the algorithm\nmodel = AgglomerativeClustering(linkage=\"ward\", n_clusters=5)\n# Run clustering (input dataset é sempre o db_scaled com valores estandarizados)\nmodel.fit(db_scaled)\n# Assign labels to main data table\ngdf1106[\"ward5\"] = model.labels_\n\nward5sizes = gdf1106.groupby(\"ward5\").size()\nward5sizes\n\nward5\n0     578\n1      93\n2    1340\n3     151\n4     660\ndtype: int64\n\n\nvisualizar dados\n\n# Index db on cluster ID\ntidy_db = gdf1106.set_index(\"ward5\")\n# Keep only variables used for clustering\ntidy_db = tidy_db[cluster_variables]\n# Stack column names into a column, obtaining\n# a \"long\" version of the dataset\ntidy_db = tidy_db.stack()\n# Take indices into proper columns\ntidy_db = tidy_db.reset_index()\n# Rename column names\ntidy_db = tidy_db.rename(\n    columns={\"level_1\": \"Attribute\", 0: \"Values\"}\n)\n# Check out result\ntidy_db.head()\n\n\n\n\n\n\n\n\nward5\nAttribute\nValues\n\n\n\n\n0\n2\nN_EDIFICIOS_CLASSICOS\n15.0\n\n\n1\n2\nN_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ\n1.0\n\n\n2\n2\nN_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS\n14.0\n\n\n3\n2\nN_RHABITUAL_PROP_OCUP\n30.0\n\n\n4\n2\nN_RHABITUAL_ARRENDADOS\n45.0\n\n\n\n\n\n\n\n\n# Setup the facets\nfacets = sns.FacetGrid(\n    data=tidy_db,\n    col=\"Attribute\",\n    hue=\"ward5\",\n    sharey=False,\n    sharex=False,\n    aspect=2,\n    col_wrap=3,\n)\n# Build the plot as a `sns.kdeplot`\nfacets.map(sns.kdeplot, \"Values\", shade=True).add_legend()\n\n\n\n\ncomparação dos 2 resultados\n\ngdf1106[\"ward5\"] = model.labels_\n# Set up figure and ax\nf, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n### K-Means ###\nax = axs[0]\n# Plot unique values choropleth including\n# a legend and with no boundary lines\ngdf1106.plot(\n    column=\"ward5\",\n    categorical=True,\n    cmap=\"Set3\",\n    legend=True,\n    linewidth=0,\n    ax=ax,\n    legend_kwds={'loc': 'upper left'}\n)\n# Remove axis\nax.set_axis_off()\n# Add title\nax.set_title(\"K-Means solution ($k=5$)\")\n\n### AHC ###\nax = axs[1]\n# Plot unique values choropleth including\n# a legend and with no boundary lines\ngdf1106.plot(\n    column=\"k5cls\",\n    categorical=True,\n    cmap=\"Set3\",\n    legend=True,\n    linewidth=0,\n    ax=ax,\n    legend_kwds={'loc': 'upper left'}\n)\n# Remove axis\nax.set_axis_off()\n# Add title\nax.set_title(\"AHC solution ($k=5$)\")\n\n# Display the map\nplt.show()\n\n\n\n\n\n\n9.4.4 Regionalization\nCriar novos Cluster\n\nIncluir matriz de vizinhança\nconnectivity = w.sparse\n\n\n# Set the seed for reproducibility\nnp.random.seed(123456)\n# Specify cluster model with spatial constraint\nmodel = AgglomerativeClustering(\n    linkage=\"ward\", connectivity=w.sparse, n_clusters=5\n)\n# Fit algorithm to the data\nmodel.fit(db_scaled)\n\nAgglomerativeClustering(connectivity=&lt;2822x2822 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 15732 stored elements in Compressed Sparse Row format&gt;,\n                        n_clusters=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  AgglomerativeClustering?Documentation for AgglomerativeClusteringiFittedAgglomerativeClustering(connectivity=&lt;2822x2822 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 15732 stored elements in Compressed Sparse Row format&gt;,\n                        n_clusters=5) \n\n\nvisualizar resultados\n\ngdf1106[\"ward5wq\"] = model.labels_\n# Set up figure and ax\nf, ax = plt.subplots(1, figsize=(9, 9))\n# Plot unique values choropleth including a legend and with no boundary lines\ngdf1106.plot(\n    column=\"ward5wq\",\n    categorical=True,\n    legend=True,\n    linewidth=0,\n    ax=ax,\n    legend_kwds={'loc': 'upper left'}\n)\n# Remove axis\nax.set_axis_off()\n# Display the map\nplt.show()\n\n# Guardar o mapa:\nf.savefig(r\"data\\geo\\mapa_clusters1.png\")\n\n\n\n\nRepetir o processo com outra matriz de vizinhança\n\n# Generate W from the GeoDataFrame\nw = weights.distance.KNN.from_dataframe(gdf1106, k=8)\n# Metodo Alternativo:\n#w = weights.Queen.from_dataframe(gdf1106, use_index=True)\n\n\n\n9.4.5 Outros metodos\nMedida compactness\n\nresults = []\nfor cluster_type in (\"k5cls\", \"ward5\", \"ward5wq\", \"ward5wknn\"):\n    # compute the region polygons using a dissolve\n    regions = db[[cluster_type, \"geometry\"]].dissolve(by=cluster_type)\n    # compute the actual isoperimetric quotient for these regions\n    ipqs = (\n        regions.area * 4 * numpy.pi / (regions.boundary.length ** 2)\n    )\n    # cast to a dataframe\n    result = ipqs.to_frame(cluster_type)\n    results.append(result)\n# stack the series together along columns\npandas.concat(results, axis=1)\n\n\nimport numpy\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\n\n# gdf1106['IND65'] = gdf1106.N_INDIVIDUOS_65_OU_MAIS/gdf1106.N_INDIVIDUOS\ngdf1106['compact'] = gdf1106.geometry.area * 4  * numpy.pi / (gdf1106.geometry.boundary.length ** 2)\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 3}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf1106.plot(column='compact', \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=9, \n                      cmap='PuBu', \n                      legend=True,\n                      edgecolor = 'None', # sem outline\n                      legend_kwds  = lgnd_kwds)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Compactness')\nplt.show()\n\n\n\n\n\nprint(gdf1106.head())\n\n   OBJECTID     BGRI2021 DT21 DTMN21 DTMNFR21 DTMNFRSEC21 SECNUM21 SSNUM21  \\\n0     30243  11065602301   11   1106   110656   110656023      023      01   \n1     30244  11065700203   11   1106   110657   110657002      002      03   \n2     30311  11065801011   11   1106   110658   110658010      010      11   \n3     30312  11065801012   11   1106   110658   110658010      010      12   \n4     30313  11065801013   11   1106   110658   110658010      010      13   \n\n  SECSSNUM21    SUBSECCAO  ... N_INDIVIDUOS_15_24 N_INDIVIDUOS_25_64  \\\n0      02301  11065602301  ...               31.0              127.0   \n1      00203  11065700203  ...               24.0               85.0   \n2      01011  11065801011  ...                5.0                9.0   \n3      01012  11065801012  ...                2.0                7.0   \n4      01013  11065801013  ...                3.0                8.0   \n\n  N_INDIVIDUOS_65_OU_MAIS  SHAPE_Length   SHAPE_Area  \\\n0                    30.0    409.853268  9657.766943   \n1                    62.0    368.048569  8445.226728   \n2                     3.0    239.674582  2582.412127   \n3                     7.0    250.811945  3077.844919   \n4                     4.0    238.956181  2555.620562   \n\n                                            geometry  k5cls  ward5  ward5wq  \\\n0  POLYGON ((-86809.545 -103264.238, -86801.039 -...      3      2        2   \n1  POLYGON ((-88183.921 -103236.850, -88218.744 -...      3      2        2   \n2  POLYGON ((-94424.359 -107038.246, -94495.548 -...      1      2        0   \n3  POLYGON ((-94547.783 -107023.019, -94626.237 -...      1      2        0   \n4  POLYGON ((-94440.784 -107017.804, -94514.143 -...      1      2        0   \n\n    compact  \n0  0.729871  \n1  0.783449  \n2  0.564926  \n3  0.611801  \n4  0.562432  \n\n[5 rows x 52 columns]\n\n\nKriging com o package pykrige\n\nimport numpy as np\nfrom pykrige.ok import OrdinaryKriging\nimport pykrige.kriging_tools as kt\nimport matplotlib.pyplot as plt  \n\n# Sample data points\ndata = np.array(\n    [\n        [0.3, 1.2, 0.5],\n        [1.1, 3.2, 0.4],\n        [1.8, 0.8, 0.6],\n        [2.8, 2.6, 0.7],\n        [3.2, 0.3, 0.8],\n    ]\n)  # [x, y, z]\n\n# Define the grid to interpolate onto\ngridx = np.arange(0.0, 4.1, 0.1)\ngridy = np.arange(0.0, 4.1, 0.1)\n\n# Create an Ordinary Kriging object\nOK = OrdinaryKriging(\n    data[:, 0],  # X coordinates\n    data[:, 1],  # Y coordinates\n    data[:, 2],  # Z values\n    variogram_model=\"spherical\",  # Variogram model (can also use \"linear\" gaussian\" or \"spherical\")\n    verbose=False,\n    enable_plotting=True,  # Enable plotting of the variogram (optional)\n)\n\n# Execute Ordinary Kriging on the defined grid\n# `z` contains the interpolated values\n# `ss` contains the standard deviation at each grid point\nz, ss = OK.execute(\"grid\", gridx, gridy)\n\n# Writes the kriged grid to an ASCII grid file and plot it.\nkt.write_asc_grid(gridx, gridy, z, filename=\"data\\geo\\output.asc\")\nplt.imshow(z)\nplt.show()\n\n&lt;&gt;:37: SyntaxWarning: invalid escape sequence '\\g'\n&lt;&gt;:37: SyntaxWarning: invalid escape sequence '\\g'\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_9296\\373054154.py:37: SyntaxWarning: invalid escape sequence '\\g'\n  kt.write_asc_grid(gridx, gridy, z, filename=\"data\\geo\\output.asc\")\n\n\n\n\n\n\n\n\n\n\n\n9.4.6 Exercício\nadaptar o código anterior para outro municipio\npreparação dos dados\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Caminho para o arquivo GeoPackage municipio Valongo\ngpk = r'data\\geo\\BGRI2021_1315.gpkg'\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdf1315 = gpd.read_file(gpk)\n\n# Simplificar a geografia para uma precisão de 5 metros\ngdf1315['geometry'] = gdf1315['geometry'].simplify(tolerance=5)\n\n# # Visualizar o GeoDataFrame\n# gdf1315.plot(column = 'DTMNFR21',\n#               legend = False)\n              \n# Nomes Atributos\ngdf1315.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 1001 entries, 0 to 1000\nData columns (total 48 columns):\n #   Column                                                         Non-Null Count  Dtype   \n---  ------                                                         --------------  -----   \n 0   OBJECTID                                                       1001 non-null   int64   \n 1   BGRI2021                                                       1001 non-null   object  \n 2   DT21                                                           1001 non-null   object  \n 3   DTMN21                                                         1001 non-null   object  \n 4   DTMNFR21                                                       1001 non-null   object  \n 5   DTMNFRSEC21                                                    1001 non-null   object  \n 6   SECNUM21                                                       1001 non-null   object  \n 7   SSNUM21                                                        1001 non-null   object  \n 8   SECSSNUM21                                                     1001 non-null   object  \n 9   SUBSECCAO                                                      1001 non-null   object  \n 10  NUTS1                                                          1001 non-null   object  \n 11  NUTS2                                                          1001 non-null   object  \n 12  NUTS3                                                          1001 non-null   object  \n 13  N_EDIFICIOS_CLASSICOS                                          1001 non-null   float64 \n 14  N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ                            1001 non-null   float64 \n 15  N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS                  1001 non-null   float64 \n 16  N_EDIFICIOS_EXCLUSIV_RESID                                     1001 non-null   float64 \n 17  N_EDIFICIOS_1_OU_2_PISOS                                       1001 non-null   float64 \n 18  N_EDIFICIOS_3_OU_MAIS_PISOS                                    1001 non-null   float64 \n 19  N_EDIFICIOS_CONSTR_ANTES_1945                                  1001 non-null   float64 \n 20  N_EDIFICIOS_CONSTR_1946_1980                                   1001 non-null   float64 \n 21  N_EDIFICIOS_CONSTR_1981_2000                                   1001 non-null   float64 \n 22  N_EDIFICIOS_CONSTR_2001_2010                                   1001 non-null   float64 \n 23  N_EDIFICIOS_CONSTR_2011_2021                                   1001 non-null   float64 \n 24  N_EDIFICIOS_COM_NECESSIDADES_REPARACAO                         1001 non-null   float64 \n 25  N_ALOJAMENTOS_TOTAL                                            1001 non-null   float64 \n 26  N_ALOJAMENTOS_FAMILIARES                                       1001 non-null   float64 \n 27  N_ALOJAMENTOS_FAM_CLASS_RHABITUAL                              1001 non-null   float64 \n 28  N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA              1001 non-null   float64 \n 29  N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS                           1001 non-null   float64 \n 30  N_RHABITUAL_COM_ESTACIONAMENTO                                 1001 non-null   float64 \n 31  N_RHABITUAL_PROP_OCUP                                          1001 non-null   float64 \n 32  N_RHABITUAL_ARRENDADOS                                         1001 non-null   float64 \n 33  N_AGREGADOS_DOMESTICOS_PRIVADOS                                1001 non-null   float64 \n 34  N_ADP_1_OU_2_PESSOAS                                           1001 non-null   float64 \n 35  N_ADP_3_OU_MAIS_PESSOAS                                        1001 non-null   float64 \n 36  N_NUCLEOS_FAMILIARES                                           1001 non-null   float64 \n 37  N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25  1001 non-null   float64 \n 38  N_INDIVIDUOS                                                   1001 non-null   float64 \n 39  N_INDIVIDUOS_H                                                 1001 non-null   float64 \n 40  N_INDIVIDUOS_M                                                 1001 non-null   float64 \n 41  N_INDIVIDUOS_0_14                                              1001 non-null   float64 \n 42  N_INDIVIDUOS_15_24                                             1001 non-null   float64 \n 43  N_INDIVIDUOS_25_64                                             1001 non-null   float64 \n 44  N_INDIVIDUOS_65_OU_MAIS                                        1001 non-null   float64 \n 45  SHAPE_Length                                                   1001 non-null   float64 \n 46  SHAPE_Area                                                     1001 non-null   float64 \n 47  geometry                                                       1001 non-null   geometry\ndtypes: float64(34), geometry(1), int64(1), object(12)\nmemory usage: 375.5+ KB\n\n\n\n# Incluir os nomes das variáveis que devem ser utilizados para criar os agrupamentos (clusters)\ncluster_variables = [\n    gdf1315.columns[24],  # \n    gdf1315.columns[25],  # \n    gdf1315.columns[26],  # \n    gdf1315.columns[27],  # \n    gdf1315.columns[28],\n    gdf1315.columns[38],\n    gdf1315.columns[39],\n    gdf1315.columns[40]\n]\n\n\n# Tratamento NaN\n# Contar o número total de NaNs no DataFrame\ntotal_nans = gdf1315.isna().sum().sum()\nprint('Número total de registros com NaN:', total_nans)\n\n# Contar o número de NaNs em cada coluna\nnans_por_coluna = gdf1315.isna().sum()\nprint('Número de registros com NaN por coluna:\\n', nans_por_coluna)\n\n# Preencher com valor 0\ngdf1315 = gdf1315.fillna(0)\n\nNúmero total de registros com NaN: 0\nNúmero de registros com NaN por coluna:\n OBJECTID                                                         0\nBGRI2021                                                         0\nDT21                                                             0\nDTMN21                                                           0\nDTMNFR21                                                         0\nDTMNFRSEC21                                                      0\nSECNUM21                                                         0\nSSNUM21                                                          0\nSECSSNUM21                                                       0\nSUBSECCAO                                                        0\nNUTS1                                                            0\nNUTS2                                                            0\nNUTS3                                                            0\nN_EDIFICIOS_CLASSICOS                                            0\nN_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ                              0\nN_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS                    0\nN_EDIFICIOS_EXCLUSIV_RESID                                       0\nN_EDIFICIOS_1_OU_2_PISOS                                         0\nN_EDIFICIOS_3_OU_MAIS_PISOS                                      0\nN_EDIFICIOS_CONSTR_ANTES_1945                                    0\nN_EDIFICIOS_CONSTR_1946_1980                                     0\nN_EDIFICIOS_CONSTR_1981_2000                                     0\nN_EDIFICIOS_CONSTR_2001_2010                                     0\nN_EDIFICIOS_CONSTR_2011_2021                                     0\nN_EDIFICIOS_COM_NECESSIDADES_REPARACAO                           0\nN_ALOJAMENTOS_TOTAL                                              0\nN_ALOJAMENTOS_FAMILIARES                                         0\nN_ALOJAMENTOS_FAM_CLASS_RHABITUAL                                0\nN_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA                0\nN_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS                             0\nN_RHABITUAL_COM_ESTACIONAMENTO                                   0\nN_RHABITUAL_PROP_OCUP                                            0\nN_RHABITUAL_ARRENDADOS                                           0\nN_AGREGADOS_DOMESTICOS_PRIVADOS                                  0\nN_ADP_1_OU_2_PESSOAS                                             0\nN_ADP_3_OU_MAIS_PESSOAS                                          0\nN_NUCLEOS_FAMILIARES                                             0\nN_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25    0\nN_INDIVIDUOS                                                     0\nN_INDIVIDUOS_H                                                   0\nN_INDIVIDUOS_M                                                   0\nN_INDIVIDUOS_0_14                                                0\nN_INDIVIDUOS_15_24                                               0\nN_INDIVIDUOS_25_64                                               0\nN_INDIVIDUOS_65_OU_MAIS                                          0\nSHAPE_Length                                                     0\nSHAPE_Area                                                       0\ngeometry                                                         0\ndtype: int64\n\n\n\n# Mostrar como mapas temáticos os valores dos atributos escolhidos  \n\nf, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n# Make the axes accessible with single indexing\naxs = axs.flatten()\n# Start a loop over all the variables of interest\nfor i, col in enumerate(cluster_variables):\n    # select the axis where the map will go\n    ax = axs[i]\n    # Plot the map\n    gdf1315.plot(\n        column=col,\n        ax=ax,\n        scheme=\"Quantiles\",\n        linewidth=0,\n        cmap=\"RdPu\",\n    )\n    # Remove axis clutter\n    ax.set_axis_off()\n    # Set the axis title to the name of variable being plotted\n    ax.set_title(col)\n# Display the figure\nplt.show()\n\n\n\n\nCalcular Moran I para todas as variáveis\n\n# Generate W from the GeoDataFrame\n# w = weights.distance.KNN.from_dataframe(gdf1315, k=8)\n# Metodo Alternativo:\nw = weights.Queen.from_dataframe(gdf1315, use_index=True)\n\n\n# Set seed for reproducibility\nnp.random.seed(123456)\n# Calculate Moran's I for each variable\nmi_results = [\n    Moran(gdf1315[variable], w) for variable in cluster_variables\n    ]\n# Structure results as a list of tuples\nmi_results = [\n    (variable, res.I, res.p_sim)\n    for variable, res in zip(cluster_variables, mi_results)\n]\n# Display on table\ntable = pd.DataFrame(\n    mi_results, columns=[\"Variable\", \"Moran's I\", \"P-value\"]\n).set_index(\"Variable\")\ntable\n\n\n\n\n\n\n\n\nMoran's I\nP-value\n\n\nVariable\n\n\n\n\n\n\nN_EDIFICIOS_COM_NECESSIDADES_REPARACAO\n0.197635\n0.001\n\n\nN_ALOJAMENTOS_TOTAL\n0.278853\n0.001\n\n\nN_ALOJAMENTOS_FAMILIARES\n0.278712\n0.001\n\n\nN_ALOJAMENTOS_FAM_CLASS_RHABITUAL\n0.284710\n0.001\n\n\nN_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA\n0.167532\n0.001\n\n\nN_INDIVIDUOS\n0.264845\n0.001\n\n\nN_INDIVIDUOS_H\n0.255916\n0.001\n\n\nN_INDIVIDUOS_M\n0.270961\n0.001\n\n\n\n\n\n\n\n\n# Utilizar kdeplot dá um aviso\nimport warnings\n\n# Nao mostrar aviso FutereWarning (não aconselhável)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Mostrar kdeplot para cada variável\n# _: Convenção Python que mostra que o resultado não está a ser utilizado\n\nsns.pairplot(\n    gdf1315[cluster_variables], kind=\"reg\", diag_kind=\"kde\"\n)\n\n\n\n\n\n# The distance between observations in terms of these variates can be computed easily using\nfrom sklearn import metrics\nmetrics.pairwise_distances(\n    gdf1315[[cluster_variables[0], cluster_variables[5]]].head()\n).round(4)\n\narray([[  0.    ,  12.3693,   9.0554,  25.    , 458.1572],\n       [ 12.3693,   0.    ,   3.6056,  25.0599, 461.    ],\n       [  9.0554,   3.6056,   0.    ,  25.632 , 459.0098],\n       [ 25.    ,  25.0599,  25.632 ,   0.    , 483.149 ],\n       [458.1572, 461.    , 459.0098, 483.149 ,   0.    ]])\n\n\n\nfrom sklearn.preprocessing import robust_scale\n# And create the db_scaled object which contains only the variables we are interested in, scaled:\ndb_scaled = robust_scale(gdf1315[cluster_variables])\nprint(type(db_scaled))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\nimport numpy\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\n\n# gdf1106['IND65'] = gdf1106.N_INDIVIDUOS_65_OU_MAIS/gdf1106.N_INDIVIDUOS\ngdf1315['compact'] = gdf1315.geometry.area * 4  * numpy.pi / (gdf1315.geometry.boundary.length ** 2)\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 3}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf1315.plot(column='compact', \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=9, \n                      cmap='PuBu', \n                      legend=True,\n                      edgecolor = 'None', # sem outline\n                      legend_kwds  = lgnd_kwds)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Compactness')\nplt.show()\n\n\n\n\n\nprint(gdf1315.head())\n\n   OBJECTID     BGRI2021 DT21 DTMN21 DTMNFR21 DTMNFRSEC21 SECNUM21 SSNUM21  \\\n0    125534  13150600202   13   1315   131506   131506002      002      02   \n1    125535  13150300506   13   1315   131503   131503005      005      06   \n2    125536  13150600203   13   1315   131506   131506002      002      03   \n3    125537  13150600204   13   1315   131506   131506002      002      04   \n4    125538  13150300907   13   1315   131503   131503009      009      07   \n\n  SECSSNUM21    SUBSECCAO  ... N_INDIVIDUOS_H N_INDIVIDUOS_M  \\\n0      00202  13150600202  ...           19.0           20.0   \n1      00506  13150300506  ...           18.0           18.0   \n2      00203  13150600203  ...           20.0           18.0   \n3      00204  13150600204  ...            7.0            7.0   \n4      00907  13150300907  ...          247.0          250.0   \n\n  N_INDIVIDUOS_0_14  N_INDIVIDUOS_15_24  N_INDIVIDUOS_25_64  \\\n0               3.0                 5.0                28.0   \n1               4.0                 4.0                21.0   \n2               5.0                 1.0                23.0   \n3               2.0                 3.0                 6.0   \n4              76.0                55.0               279.0   \n\n   N_INDIVIDUOS_65_OU_MAIS  SHAPE_Length     SHAPE_Area  \\\n0                      3.0    523.018918    9289.852833   \n1                      7.0   1109.669756   50141.267236   \n2                      9.0    511.977438   15032.503593   \n3                      3.0    553.145862   10797.658643   \n4                     87.0   2328.672435  260550.923540   \n\n                                            geometry   compact  \n0  POLYGON ((-27481.127 171972.950, -27461.897 17...  0.411844  \n1  POLYGON ((-34607.059 171934.156, -34553.823 17...  0.517489  \n2  POLYGON ((-28357.470 171987.554, -28332.531 17...  0.715822  \n3  POLYGON ((-27497.017 172000.233, -27486.203 17...  0.451444  \n4  POLYGON ((-33995.356 171989.441, -33899.639 17...  0.611464  \n\n[5 rows x 49 columns]\n\n\nKriging com o package pykrige\n\nimport numpy as np\nfrom pykrige.ok import OrdinaryKriging\nimport pykrige.kriging_tools as kt\nimport matplotlib.pyplot as plt  \n\n# Sample data points\ndata = np.array(\n    [\n        [0.3, 1.2, 0.5],\n        [1.1, 3.2, 0.4],\n        [1.8, 0.8, 0.6],\n        [2.8, 2.6, 0.7],\n        [3.2, 0.3, 0.8],\n    ]\n)  # [x, y, z]\n\n# Define the grid to interpolate onto\ngridx = np.arange(0.0, 4.1, 0.1)\ngridy = np.arange(0.0, 4.1, 0.1)\n\n# Create an Ordinary Kriging object\nOK = OrdinaryKriging(\n    data[:, 0],  # X coordinates\n    data[:, 1],  # Y coordinates\n    data[:, 2],  # Z values\n    variogram_model=\"spherical\",  # Variogram model (can also use \"linear\" gaussian\" or \"spherical\")\n    verbose=False,\n    enable_plotting=True,  # Enable plotting of the variogram (optional)\n)\n\n# Execute Ordinary Kriging on the defined grid\n# `z` contains the interpolated values\n# `ss` contains the standard deviation at each grid point\nz, ss = OK.execute(\"grid\", gridx, gridy)\n\n# Writes the kriged grid to an ASCII grid file and plot it.\nkt.write_asc_grid(gridx, gridy, z, filename=\"data\\geo\\output.asc\")\nplt.imshow(z)\nplt.show()\n\n&lt;&gt;:37: SyntaxWarning: invalid escape sequence '\\g'\n&lt;&gt;:37: SyntaxWarning: invalid escape sequence '\\g'\nC:\\Users\\bruno.lima\\AppData\\Local\\Temp\\ipykernel_9296\\373054154.py:37: SyntaxWarning: invalid escape sequence '\\g'\n  kt.write_asc_grid(gridx, gridy, z, filename=\"data\\geo\\output.asc\")"
  },
  {
    "objectID": "900-mod9.html#json-api-ine",
    "href": "900-mod9.html#json-api-ine",
    "title": "9  Dados Geográficos",
    "section": "9.5 JSON / API INE",
    "text": "9.5 JSON / API INE\n\n9.5.1 JavaScript Object Notation\n\nformato de dados leve e eficiente\ndados estruturados\namplamente utilizado\n\nútil para:\n\nrecolha de dados\npreparação de dados\nanálise de dados\nvisualização de dados\n\nJSON vs XML\n\nUm objecto JSON é um conjunto de pares de chave e valor encerrados por chaves.\npackages python para ler JSON: json, pandas, requests, jsonpath-ng, jsonschema\n\n# Ler JSON de um Dicionario\nimport pprint # para visualizar de forma mais amigavel os dados\nimport json\n\n# Criar um Dictionary (exemplo dados municipios)\nmunicipios = {\n    \"Almada\": {\n        \"populacao\": 177400        \n    },\n    \"Cascais\": {\n        \"populacao\": 214124        \n    },\n    \"Seixal\": {\n        \"populacao\": 160000\n    },\n    \"Entroncamento\": {\n        \"populacao\": 20141\n    },\n    \"Cadaval\": {\n        \"populacao\": 13372\n    },\n    \"Sintra\": {\n        \"populacao\": 385606\n    }\n}\n\nprint(type(municipios))\n\n# Converter o Dictionary para uma string JSON\nmn_json = json.dumps(municipios)\nprint(type(mn_json))\n\n# Fazer Load do JSON\ndata_json = json.loads(mn_json)\nprint(type(data_json))\n\n# Mostrar População Cascais\nprint (\"População Cascais:\",data_json[\"Cascais\"][\"populacao\"])\n\n# Imprimir o Type do Object Devolvido\nprint('Tipo Objecto:', type(data_json))\n\n# Imprimir o objecto com PrettyPrinter\npp = pprint.PrettyPrinter(indent=2)\n\n# Imprimir o dicionário\npp.pprint(data_json)\n\n&lt;class 'dict'&gt;\n&lt;class 'str'&gt;\n&lt;class 'dict'&gt;\nPopulação Cascais: 214124\nTipo Objecto: &lt;class 'dict'&gt;\n{ 'Almada': {'populacao': 177400},\n  'Cadaval': {'populacao': 13372},\n  'Cascais': {'populacao': 214124},\n  'Entroncamento': {'populacao': 20141},\n  'Seixal': {'populacao': 160000},\n  'Sintra': {'populacao': 385606}}\n\n\n\n# Criar JSOn a partir de uma Listagem:\n# import json\n# import pprint\n\n# Exemplo cidades de Portugal\ncidades = [\"Lisboa\", \"Porto\", \"Vila Nova de Gaia\", \"Amadora\", \"Braga\", \"Funchal\", \"Coimbra\", \"Almada\", \"Setúbal\", \"Agualva-Cacém\"]\n\n# Converter a lista para uma string JSON\ncidades_json = json.dumps(cidades)\nprint('Tipo Objecto após json.dumps:', type(cidades_json))\n\n# Converter a string JSON de volta para uma lista\ndata_json = json.loads(cidades_json)\n\n\n# Imprimir o Type do Object Devolvido\nprint('Tipo Objecto após json.loads:', type(data_json))\n\n# Imprimir os dados carregados\n# Criar um objeto PrettyPrinter\npp = pprint.PrettyPrinter(indent=4)\n\n# Imprimir o dicionário\npp.pprint(data_json)\n\nTipo Objecto após json.dumps: &lt;class 'str'&gt;\nTipo Objecto após json.loads: &lt;class 'list'&gt;\n[   'Lisboa',\n    'Porto',\n    'Vila Nova de Gaia',\n    'Amadora',\n    'Braga',\n    'Funchal',\n    'Coimbra',\n    'Almada',\n    'Setúbal',\n    'Agualva-Cacém']\n\n\nler dados de um ficheiro\n\n# exemplo de um ficheiro com os municipios de Madrid\nimport json\nimport pandas as pd\n# Ler Dados de um Ficheiro no computador (Municipios de Provincia de Madrid)\njsonfile = r\"data\\geo\\municipio_comunidad_madrid.json\"\n\n# Abrir Ficheiro e fazer Load\nwith open(jsonfile, 'r') as f:\n    json_data = json.load(f)\n\nprint(type(json_data))\n    \n# Verificar Tipo de Dados Devolvido e mostrar informação\nif isinstance(json_data, list):\n    print(\"JSON object is a list.\")\n    if json_obj:\n        print(\"Numero Registos:\", len(json_data))\n        print(\"Registo Exemplo:\", json_data[0])\nelif isinstance(json_data, dict):\n    print(\"JSON object is a dictionary.\")\n    print(\"Keys do Object:\", list(json_data.keys()))\nelse:\n    print(\"Unknown JSON object type.\")\n\nnovoelem = json_data[\"data\"]  \nprint (type(novoelem))\n\ndf = pd.DataFrame(novoelem)\nprint(df.info())\n\n&lt;class 'dict'&gt;\nJSON object is a dictionary.\nKeys do Object: ['data']\n&lt;class 'list'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 179 entries, 0 to 178\nData columns (total 7 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   municipio_codigo      179 non-null    object \n 1   densidad_por_km2      179 non-null    float64\n 2   municipio_codigo_ine  179 non-null    object \n 3   nuts4_nombre          179 non-null    object \n 4   municipio_nombre      179 non-null    object \n 5   nuts4_codigo          179 non-null    object \n 6   superficie_km2        179 non-null    float64\ndtypes: float64(2), object(5)\nmemory usage: 9.9+ KB\nNone\n\n\nler dados de um URL\n\nimport json\nimport requests\n\nproxies = {\n  'http': 'http://proxy.ine.pt:8080',\n  'https': 'http://proxy.ine.pt:8080',\n}\n\n\nurl = \"https://datos.comunidad.madrid/catalogo/dataset/032474a0-bf11-4465-bb92-392052962866/resource/301aed82-339b-4005-ab20-06db41ee7017/download/municipio_comunidad_madrid.json\"\n\n# Make an HTTP GET request to fetch the JSON data from the URL\n# \nresponse = requests.get(url)#, proxies=proxies)\n\n# Verificar Resposta\n# Respostas Possiveis\nif response.status_code == 200:\n    #Obter JSON response\n    json_data = response.json()\nelse:\n    print(\"Failed to fetch data. Status code:\", response.status_code)\n    exit()\n\n\n# Verificar Tipo de Dados Devolvido e mostrar informação\nif isinstance(json_data, list):\n    print(\"JSON object is a list.\")\n    if json_data:\n        print(\"Numero Registos:\", len(json_data))\n        print(\"Registo Exemplo:\", json_data[0])\nelif isinstance(json_data, dict):\n    print(\"JSON object is a dictionary.\")\n    print(\"Keys do Object:\", list(json_data.keys()))\nelse:\n    print(\"Unknown JSON object type.\")\n\nJSON object is a dictionary.\nKeys do Object: ['data']\n\n\n\n\n9.5.2 converter para pandas DataFrame\nler dados de uma listagem normal\n\n  import pandas as pd\n# Ler Informacao JSOn Municipios:\ndf_mn = pd.read_json(r'data\\geo\\municipios.json')\n\nprint(df_mn.info())\nprint('Primeiro Municipio',df_mn['municipio'][0])\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   municipio  5 non-null      object\n 1   populacao  5 non-null      int64 \ndtypes: int64(1), object(1)\nmemory usage: 212.0+ bytes\nNone\nPrimeiro Municipio Almada\n\n\nnested data\n\nimport pandas as pd\n# Ler Informacao JSOn Municipios:\ndf_frmn = pd.read_json(r\"data\\geo\\municipiosfreguesias.json\")\nprint(df_frmn.info())\nprint('Primeiro Municipio',df_frmn['municipio'][0])\n\n# Ver o tipo do atributo freguesias - Series\nprint(type(df_frmn['freguesias']))\n\n# Selecionar o primeiro municipio\ndf_frmn = df_frmn.loc[df_frmn['municipio'] == 'Almada']\n\n# Mostrar os valores das freguesias\nprint(df_frmn['freguesias'])\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   municipio   5 non-null      object\n 1   populacao   5 non-null      int64 \n 2   freguesias  5 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 252.0+ bytes\nNone\nPrimeiro Municipio Almada\n&lt;class 'pandas.core.series.Series'&gt;\n0    [{'nome': 'Almada', 'populacao': 53194}, {'nom...\nName: freguesias, dtype: object\n\n\nutilizar json_normalize()\n\n# Utilizar a funcao json_normalize para criar um DF apenas das freguesias\n\n# Load the JSON file\nwith open(r\"data\\geo\\municipiosfreguesias.json\", \"r\", encoding=\"utf-8\") as f:\n    municipalities = json.load(f)\n\n# Convert the JSON into a DataFrame\ndf = pd.json_normalize(municipalities, record_path=['freguesias'])\n\n# Print the DataFrame\nprint(df)\nprint(df.info())\n\n                   nome  populacao\n0                Almada      53194\n1              Cacilhas      20540\n2              Caparica      29222\n3     Costa da Caparica      33477\n4    Laranjeiro e Feijó      19910\n5   Marinha da Caparica      10090\n6               Pichela      10012\n7               Sobreda      10084\n8           Alcabideche      26613\n9   Carcavelos e Parede      74302\n10              Cascais      36744\n11              Estoril      40926\n12              Guincho      24759\n13                Amora      41588\n14     Gandra e Samouco      13069\n15              Palmela      23698\n16               Seixal     120464\n17        Entroncamento      39343\n18              Cadaval      12889\n19   Ferreira do Zêzere      10890\n20             Lourinhã      11729\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 21 entries, 0 to 20\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   nome       21 non-null     object\n 1   populacao  21 non-null     int64 \ndtypes: int64(1), object(1)\nmemory usage: 468.0+ bytes\nNone\n\n\ncriar dataframe utilizando um loop\n\n# Alternativa para criar Informação das Freguesias\n# Ler os dados\nwith open(r\"data\\geo\\municipiosfreguesias.json\", \"r\", encoding=\"utf-8\") as f:\n    municipalities = json.load(f)\n    \n# Criar Lista das linhas do INPUT\nrows = []\n\n# Percorrer os municipios\nfor municipality in municipalities:\n    # Iterate over the freguesias\n    for freguesia in municipality['freguesias']:\n        # Criar novo registo\n        row = {\n            'municipio': municipality['municipio'],\n            'freguesia': freguesia['nome'],\n            'populacao': freguesia['populacao']\n        }\n\n        # Adicionar Registo a Lista\n        rows.append(row)\n\n# Criar DataFame\ndf = pd.DataFrame(rows)\n\n# Print the DataFrame\nprint(df)\n\n        municipio            freguesia  populacao\n0          Almada               Almada      53194\n1          Almada             Cacilhas      20540\n2          Almada             Caparica      29222\n3          Almada    Costa da Caparica      33477\n4          Almada   Laranjeiro e Feijó      19910\n5          Almada  Marinha da Caparica      10090\n6          Almada              Pichela      10012\n7          Almada              Sobreda      10084\n8         Cascais          Alcabideche      26613\n9         Cascais  Carcavelos e Parede      74302\n10        Cascais              Cascais      36744\n11        Cascais              Estoril      40926\n12        Cascais              Guincho      24759\n13         Seixal                Amora      41588\n14         Seixal     Gandra e Samouco      13069\n15         Seixal              Palmela      23698\n16         Seixal               Seixal     120464\n17  Entroncamento        Entroncamento      39343\n18        Cadaval              Cadaval      12889\n19        Cadaval   Ferreira do Zêzere      10890\n20        Cadaval             Lourinhã      11729\n\n\n\n\n9.5.3 Exercício\nA partir do objecto pt_info\n\nConverter o dicionário para uma string JSON\n\nConverter a string JSON de volta para um dicionário\nObter o tipo do objecto devolvido\nImprimir as chaves\nConverter para DataFrame\nFazer algumas pesquisas\n\nPopulação de Vila Nova de Gaia\nArea de Portugal\n\n\n\n# Dictionary com informcao Portugal:\n# Também é Nested\npt_info = {\n  \"country\": \"Portugal\",\n  \"capital\": \"Lisbon\",\n  \"population\": 10347892,\n  \"area\": 92212,\n  \"language\": \"Portuguese\",\n  \"currency\": \"Euro\",\n  \"cities\": [\n    {\n      \"name\": \"Lisbon\",\n      \"population\": 504762,\n      \"region\": \"Lisboa\"\n    },\n    {\n      \"name\": \"Porto\",\n      \"population\": 219419,\n      \"region\": \"Norte\"\n    },\n    {\n      \"name\": \"Vila Nova de Gaia\",\n      \"population\": 301877,\n      \"region\": \"Norte\"\n    },\n    {\n      \"name\": \"Matosinhos\",\n      \"population\": 174339,\n      \"region\": \"Norte\"\n    },\n    {\n      \"name\": \"Almada\",\n      \"population\": 174033,\n      \"region\": \"Lisboa\"\n    }\n  ]\n}\n\nprint(type(pt_info))\n\n&lt;class 'dict'&gt;\n\n\n\n# Converter o Dictionary para uma string JSON\nmn_json = json.dumps(pt_info)\nprint(type(mn_json))\n\n# Fazer Load do JSON\ndata_json = json.loads(mn_json)\nprint(type(data_json))\n\n# Imprimir o Type do Object Devolvido\nprint('Tipo Objecto:', type(data_json))\n\n# Imprimir o objecto com PrettyPrinter\npp = pprint.PrettyPrinter(indent=2)\n\n# Imprimir o dicionário\npp.pprint(data_json)\n\n# Obter o Nome da primeira cidade \nprint(data_json['cities'][0]['name'])\n\n# população do Porto\nprint('População do Porto: ',data_json['cities']['name' == 'Porto']['population'])\n\n# 4. Imprimir as chaves \n# Verificar Tipo de Dados Devolvido e mostrar informação\nif isinstance(data_json, list):\n    print(\"JSON object is a list.\")\n    if json_obj:\n        print(\"Numero Registos:\", len(data_json))\n        print(\"Registo Exemplo:\", data_json[0])\nelif isinstance(data_json, dict):\n    print(\"JSON object is a dictionary.\")\n    print(\"Keys do Object:\", list(data_json.keys()))\nelse:\n    print(\"Unknown JSON object type.\")\n\n# 5. Converter para DataFrame\ndf = pd.json_normalize(data_json, record_path=['cities'])\nprint(df)\n\n\n# 6. Fazer algumas pesquisas\n#  - População de Vila Nova de Gaia\n#  - Area de Portugal\npopulacao_vila_nova_de_gaia = df[df['name'] == 'Vila Nova de Gaia']['population'].values[0]\nprint(\"População de Vila Nova de Gaia:\", populacao_vila_nova_de_gaia)\n\narea_portugal = data_json['area']\nprint(\"Área de Portugal:\", area_portugal)\n\n&lt;class 'str'&gt;\n&lt;class 'dict'&gt;\nTipo Objecto: &lt;class 'dict'&gt;\n{ 'area': 92212,\n  'capital': 'Lisbon',\n  'cities': [ {'name': 'Lisbon', 'population': 504762, 'region': 'Lisboa'},\n              {'name': 'Porto', 'population': 219419, 'region': 'Norte'},\n              { 'name': 'Vila Nova de Gaia',\n                'population': 301877,\n                'region': 'Norte'},\n              {'name': 'Matosinhos', 'population': 174339, 'region': 'Norte'},\n              {'name': 'Almada', 'population': 174033, 'region': 'Lisboa'}],\n  'country': 'Portugal',\n  'currency': 'Euro',\n  'language': 'Portuguese',\n  'population': 10347892}\nLisbon\nPopulação do Porto:  504762\nJSON object is a dictionary.\nKeys do Object: ['country', 'capital', 'population', 'area', 'language', 'currency', 'cities']\n                name  population  region\n0             Lisbon      504762  Lisboa\n1              Porto      219419   Norte\n2  Vila Nova de Gaia      301877   Norte\n3         Matosinhos      174339   Norte\n4             Almada      174033  Lisboa\nPopulação de Vila Nova de Gaia: 301877\nÁrea de Portugal: 92212"
  },
  {
    "objectID": "900-mod9.html#json-ine",
    "href": "900-mod9.html#json-ine",
    "title": "9  Dados Geográficos",
    "section": "9.6 JSON INE",
    "text": "9.6 JSON INE\n\nimport requests\nimport os\n# Ler Dados Inicial para JSON\n# Indicador 0008074: Taxa de criminalida, último ano, todos os níveis geográficos, indicador \n# Categorias no SMI do Dim3: http://smi-i.ine.pt/Versao/Detalhes/902 \nproxies = {\n  'http': 'http://proxy.ine.pt:8080',\n  'https': 'http://proxy.ine.pt:8080',\n}\n\n# os.environ['http_proxy'] = 'http://proxy.ine.pt:8080'\n# os.environ['https_proxy'] = 'http://proxy.ine.pt:8080'\n\n# Dim1=T: Dados de todos os anos\nurl = \"https://www.ine.pt/ine/json_indicador/pindica.jsp?op=2&varcd=0008074&Dim1=T&Dim3=3&lang=PT\"\nprint(url)\n# Make an HTTP GET request to fetch the JSON data from the URL\nresponse = requests.get(url)#, proxies=proxies)\n\n# Verificar Resposta\n# Respostas Possiveis\nif response.status_code == 200:\n    #Obter JSON response\n    json_data = response.json()\nelse:\n    print(\"Failed to fetch data. Status code:\", response.status_code)\n    exit()\n\n\n# Verificar Tipo de Dados Devolvido e mostrar informação\nif isinstance(json_data, list):\n    print(\"JSON object is a list.\")\n    if json_data:\n        print(\"Numero Registos:\", len(json_data))\n        #print(\"Registo Exemplo:\", json_data[0])\n        print(\"Tipo 1º elementos:\", type(json_data[0]))\nelif isinstance(json_data, dict):\n    print(\"JSON object is a dictionary.\")\n    print(\"Keys do Object:\", list(json_data.keys()))\nelse:\n    print(\"Unknown JSON object type.\")\n\n    \n# Obter tipo de keys no dictionary\nprint(\"Keys existentes:\", list(json_data[0].keys()))\n\n# Key com os dados\n\nhttps://www.ine.pt/ine/json_indicador/pindica.jsp?op=2&varcd=0008074&Dim1=T&Dim3=3&lang=PT\n\n\nJSON object is a list.\nNumero Registos: 1\nTipo 1º elementos: &lt;class 'dict'&gt;\nKeys existentes: ['IndicadorCod', 'IndicadorDsg', 'MetaInfUrl', 'DataExtracao', 'DataUltimoAtualizacao', 'UltimoPref', 'Dados', 'Sucesso']\n\n\nanalisar o objecto JSON devolvido\n\n# Para poder Importar será necessário de fazer uma análise do Objeto Devolvido\n\n# Obter Keys no Dados\n# Existe um Key para Cada Ano\nprint(\"Keys existentes nos Dados:\", list(json_data[0]['Dados'].keys()) )\n\n# Ver Tipo de conteudo 2022:\nprint(\"Tipo Objecto:\", type(json_data[0]['Dados']['2022']) )\n\n# Tipo é Listagem de Dictionary's\n# Ver conteudo e informação 1º elemento\nprint(\"Tipo primeiro elemento:\", type(json_data[0]['Dados']['2022'][0]), 'Numero Elementos:', len(json_data[0]['Dados']['2022']) )\n# Atributos de cada dictionary\nprint(\"Keys existentes no Ano:\", list(json_data[0]['Dados']['2022'][0].keys()) )\n\nKeys existentes nos Dados: ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\nTipo Objecto: &lt;class 'list'&gt;\nTipo primeiro elemento: &lt;class 'dict'&gt; Numero Elementos: 344\nKeys existentes no Ano: ['geocod', 'geodsg', 'dim_3', 'dim_3_t', 'ind_string', 'valor']\n\n\nmostrar ano\n\n# Fazer um Loop por todos os anos\nfor ky in list(json_data[0]['Dados'].keys()):\n    print(ky)\n\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n\n\ncriar dataframe dos dados\n\nimport geopandas as gpd\n\ndadosmn = r\"data\\geo\\GPK_CAOP_MN.gpkg\"\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdfmn = gpd.read_file(dadosmn, encoding='utf-8')\nprint(gdfmn.info())\nprint(gdfmn.head())\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 308 entries, 0 to 307\nData columns (total 12 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   OBJECTID    308 non-null    int64   \n 1   DTMN        308 non-null    object  \n 2   DTMNDSG     308 non-null    object  \n 3   ILHA        30 non-null     object  \n 4   NUTS3_02    308 non-null    object  \n 5   NUTS3_15    308 non-null    object  \n 6   NUTS3_24    308 non-null    object  \n 7   OBJECTID_1  278 non-null    float64 \n 8   NUTS1_15    278 non-null    object  \n 9   NUTS2_15    278 non-null    object  \n 10  DISTRITO    278 non-null    object  \n 11  geometry    308 non-null    geometry\ndtypes: float64(1), geometry(1), int64(1), object(9)\nmemory usage: 29.0+ KB\nNone\n   OBJECTID  DTMN                DTMNDSG                ILHA NUTS3_02  \\\n0         1  4901                  Corvo       Ilha do Corvo      200   \n1         2  4801       Lajes das Flores     Ilha das Flores      200   \n2         3  4802  Santa Cruz das Flores     Ilha das Flores      200   \n3         1  4201                  Lagoa  Ilha de São Miguel      200   \n4         2  4202               Nordeste  Ilha de São Miguel      200   \n\n  NUTS3_15 NUTS3_24  OBJECTID_1 NUTS1_15 NUTS2_15 DISTRITO  \\\n0    PT200      200         NaN     None     None     None   \n1    PT200      200         NaN     None     None     None   \n2    PT200      200         NaN     None     None     None   \n3      200      200         NaN     None     None     None   \n4      200      200         NaN     None     None     None   \n\n                                            geometry  \n0  MULTIPOLYGON (((-3463610.566 4817991.232, -346...  \n1  MULTIPOLYGON (((-3479481.263 4792025.893, -347...  \n2  MULTIPOLYGON (((-3474231.695 4797050.671, -347...  \n3  MULTIPOLYGON (((-2845106.248 4547845.673, -284...  \n4  MULTIPOLYGON (((-2812174.924 4560067.448, -281...  \n\n\n\nimport pandas as pd\n# Os dados para importar dzem respeito a uma listagem de dictionary's. \n\n# Os dados podem ser importados a partir destas listagem com função pd.DataFrame()\n# Para assegurar o tipo de dados deveria ser especificado o tipo de atributos das colunas \ncolumns = [\"geocod\", \"geodsg\", \"dim_3\", \"dim_3_t\", \"valor\"]\ndata_types = {\"geocod\": str, \"geodsg\": str, \"dim_3\": str, \"dim_3_t\": str, \"valor\": float}\n\n# Convert the list of dictionaries to a Pandas DataFrame\ndf_ine = pd.DataFrame(json_data[0]['Dados']['2022'], columns=columns).astype(data_types)\n\n# Mostrar o Resultado:\nprint(df_ine.info())\nprint(df_ine.head(8))\nprint('Numero registos:',len(df_ine))\n\n# Novo Cell - filter NUTS3 (length geocod == 3):\n# Alternativa - seria criar uma listagem unica a 34]\ndf_nuts3 = df_ine[df_ine['geocod'].str.len() == 3]\nprint(df_nuts3.info())\n\n#df_nuts3['codmn'] = df_nuts3['geocod'].str[-4:]\n# df_nuts3['geocod'].str[-4:]\n#print(df_nuts3.head(10))\n\n# df_mn = df_ine[df_ine['geocod'].str.len() == 7]\n# print(df_mn.info())\n# df_mn['codmn'] = df_mn['geocod'].str[-4:]\n# print(df_mn.head(10))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 5 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   geocod   344 non-null    object \n 1   geodsg   344 non-null    object \n 2   dim_3    344 non-null    object \n 3   dim_3_t  344 non-null    object \n 4   valor    310 non-null    float64\ndtypes: float64(1), object(4)\nmemory usage: 13.6+ KB\nNone\n    geocod        geodsg dim_3                                   dim_3_t  \\\n0  11E0412       Vinhais     3  Furto de veículo e em veículo motorizado   \n1       15       Algarve     3  Furto de veículo e em veículo motorizado   \n2      150       Algarve     3  Furto de veículo e em veículo motorizado   \n3  1500801     Albufeira     3  Furto de veículo e em veículo motorizado   \n4  1500802      Alcoutim     3  Furto de veículo e em veículo motorizado   \n5  1500803       Aljezur     3  Furto de veículo e em veículo motorizado   \n6  1500804  Castro Marim     3  Furto de veículo e em veículo motorizado   \n7  1500805          Faro     3  Furto de veículo e em veículo motorizado   \n\n   valor  \n0    0.0  \n1    4.1  \n2    4.1  \n3    5.1  \n4    1.6  \n5   10.4  \n6    3.7  \n7    3.3  \nNumero registos: 344\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 25 entries, 2 to 342\nData columns (total 5 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   geocod   25 non-null     object \n 1   geodsg   25 non-null     object \n 2   dim_3    25 non-null     object \n 3   dim_3_t  25 non-null     object \n 4   valor    22 non-null     float64\ndtypes: float64(1), object(4)\nmemory usage: 1.2+ KB\nNone\n\n\nvisualizar os dados como um mapa\n\n# Mostrar valores unicos chave\nimport numpy as np\nprint(np.sort(df_nuts3.geocod.unique()))\nprint(df_nuts3[['geocod','geodsg','valor']])\n\n# Corrigir Valores NaN\ndf_nuts3 = df_nuts3.fillna(0)\n\n['111' '112' '119' '11A' '11B' '11C' '11D' '11E' '150' '16B' '16D' '16E'\n '16F' '16G' '16H' '16I' '16J' '170' '181' '184' '185' '186' '187' '200'\n '300']\n    geocod                        geodsg  valor\n2      150                       Algarve    4.1\n22     16B                         Oeste    2.4\n34     16D              Região de Aveiro    2.3\n46     16E             Região de Coimbra    1.8\n51     111                    Alto Minho    1.7\n62     112                        Cávado    2.7\n69     119                           Ave    1.6\n78     11A   Área Metropolitana do Porto    4.7\n79     11C                Tâmega e Sousa    1.8\n87     11D                         Douro    1.3\n107    16G              Viseu Dão Lafões    1.1\n122    16H                   Beira Baixa    NaN\n129    16I                    Médio Tejo    NaN\n151    181              Alentejo Litoral    2.5\n157    184                Baixo Alentejo    1.5\n177    16F              Região de Leiria    2.0\n196    185               Lezíria do Tejo    2.6\n209    11B                   Alto Tâmega    1.1\n228    186                 Alto Alentejo    1.2\n240    300    Região Autónoma da Madeira    1.9\n257    187              Alentejo Central    1.4\n274    200    Região Autónoma dos Açores    2.3\n299    16J     Beiras e Serra da Estrela    NaN\n316    170  Área Metropolitana de Lisboa    3.3\n342    11E      Terras de Trás-os-Montes    0.7\n\n\n\n# Import packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\n\n# Definir Figura e Axis\nf, ax = plt.subplots(1, figsize=(9, 9))\n\n\n# Ler os dados NUTS3\ngpk = r\"data\\geo\\GPK_NUTS3.gpkg\"\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdfnuts3 = gpd.read_file(gpk)\nprint(gdfnuts3.info())\n# Selecionar Dados Portugal Continental:\n# Fazer Seleção da NUTS1, Atributo NUTS3\n# Sem seleção a area da visualização é muito grande\ngdf_nuts3_sel = gdfnuts3[gdfnuts3['NUTS3'].str.startswith('1')]\n\nprint(gdf_nuts3_sel.info())\n\n# Fazer Merge dos dados\n# Fazer o Join, especificar: DF\ngdf_nuts3_2 = gdf_nuts3_sel.merge(df_nuts3, left_on='NUTS3', right_on='geocod', how='left')\n\n# Definir Legenda \nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 2}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf_nuts3_2.plot(column=gdf_nuts3_2.valor, \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=7, \n                      cmap='YlGn', \n                      legend=True,\n                      edgecolor = 'dimgray',\n                      legend_kwds  = lgnd_kwds,\n                      ax=ax)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title(json_data[0]['IndicadorDsg']) # usar a designação do indicador no titulo do mapa\nplt.show()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 25 entries, 0 to 24\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   NUTS3      25 non-null     object  \n 1   NUTS3_DSG  25 non-null     object  \n 2   geometry   25 non-null     geometry\ndtypes: geometry(1), object(2)\nmemory usage: 732.0+ bytes\nNone\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 23 entries, 0 to 22\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   NUTS3      23 non-null     object  \n 1   NUTS3_DSG  23 non-null     object  \n 2   geometry   23 non-null     geometry\ndtypes: geometry(1), object(2)\nmemory usage: 736.0+ bytes\nNone\n\n\n\n\n\n\ngdfnuts3.info()\nprint(gdf_nuts3_sel.NUTS3.unique())\nprint(df_nuts3.geocod.unique())\n# Valores unicos NUTS3 \n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 25 entries, 0 to 24\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   NUTS3      25 non-null     object  \n 1   NUTS3_DSG  25 non-null     object  \n 2   geometry   25 non-null     geometry\ndtypes: geometry(1), object(2)\nmemory usage: 732.0+ bytes\n['111' '112' '119' '11A' '11B' '11C' '11D' '11E' '150' '16B' '16D' '16E'\n '16F' '16G' '16H' '16I' '16J' '170' '181' '184' '185' '186' '187']\n['150' '16B' '16D' '16E' '111' '112' '119' '11A' '11C' '11D' '16G' '16H'\n '16I' '181' '184' '16F' '185' '11B' '186' '300' '187' '200' '16J' '170'\n '11E']\n\n\n\n9.6.1 Exercício\nObservações: - Faz a Adaptação do código seguinte para importar outro indicador ao nível de municipio ou NUTS3 - Ler Dados Indicador XXXX de um ano a escolha - Ver pagina SMI Indicadores: https://smi.ine.pt/Indicador?clear=True - Testar url antes de incluir no script - Importar para Pandas Dataframe as áreas NUTS3 - Criar mapa dos resultados\nInformação URL - varcd: código de difusão - Dim1 (periódo de referência): - Ano (de acordo com portal, por exemplo S7A2016&lt;) - Sem valores, devolvido dados último ano - T: Dados de todos os anos - Dim2: - Sem dados: retornados dados todas as geografias - Geografias separados por vírgula - Dim3=T: Informação disponível no SMI\n\nimport requests\nimport os\nimport pandas as pd\n\n# Ler Dados Inicial para JSON\n# Indicador 0008074: Taxa de criminalida, último ano, todos os níveis geográficos, indicador \n# Categorias no SMI do Dim3: http://smi-i.ine.pt/Versao/Detalhes/902 \nproxies = {\n  'http': 'http://proxy.ine.pt:8080',\n  'https': 'http://proxy.ine.pt:8080',\n}\n\n# os.environ['http_proxy'] = 'http://proxy.ine.pt:8080'\n# os.environ['https_proxy'] = 'http://proxy.ine.pt:8080'\n\n# Dim1: Ultimo ano, Dim2: Todas as geografias \nurl = r'https://www.ine.pt/ine/json_indicador/pindica.jsp?op=2&varcd=0008265&lang=PT'\n\n# Make an HTTP GET request to fetch the JSON data from the URL\nresponse = requests.get(url)#, proxies=proxies)\n\n# Verificar Resposta\n# Respostas Possiveis\nif response.status_code == 200:\n    #Obter JSON response\n    json_data = response.json()\nelse:\n    print(\"Failed to fetch data. Status code:\", response.status_code)\n    exit()\n\nprint (type(json_data))\n\n\n# Verificar Tipo de Dados Devolvido e mostrar informação\nif isinstance(json_data, list):\n    print(\"JSON object is a list.\")\n    if json_data:\n        print(\"Numero Registos:\", len(json_data))\n        #print(\"Registo Exemplo:\", json_data[0])\n        print(\"Tipo 1º elementos:\", type(json_data[0]))\nelif isinstance(json_data, dict):\n    print(\"JSON object is a dictionary.\")\n    print(\"Keys do Object:\", list(json_data.keys()))\nelse:\n    print(\"Unknown JSON object type.\")\n\n    \n# Obter tipo de keys no dictionary\nprint(\"Keys existentes:\", list(json_data[0].keys()))\n\n&lt;class 'list'&gt;\nJSON object is a list.\nNumero Registos: 1\nTipo 1º elementos: &lt;class 'dict'&gt;\nKeys existentes: ['IndicadorCod', 'IndicadorDsg', 'MetaInfUrl', 'DataExtracao', 'DataUltimoAtualizacao', 'UltimoPref', 'Dados', 'Sucesso']\n\n\n\n# Fazer um Loop por todos os anos\nfor ky in list(json_data[0]['Dados'].keys()):\n    print(ky)\n\n2022\n\n\n\n# Obter Keys no Dados\n# Existe um Key para Cada Ano\n# Resultado json_data\nprint(\"Keys existentes nos Dados:\", list(json_data[0]['Dados'].keys()) )\n\n# Ver Tipo de conteudo 2022:\nprint(\"Tipo Objecto:\", type(json_data[0]['Dados']['2022']) )\n\n# Tipo é Listagem de Dictionary's\n# Ver conteudo e informação 1º elemento\nprint(\"Tipo primeiro elemento:\", type(json_data[0]['Dados']['2022'][0]), 'Numero Elementos:', len(json_data[0]['Dados']['2022']) )\n# Atributos de cada dictionary\n# json_data[0] = Conteudo de resposta\n# json_data[0]['Dados'] = Vamos buscar os proprios dados\n# Obter os dados do ano 2022 - deste conteudo podmeos criar DataFrame: json_data[0]['Dados']['2022']\nprint(\"Keys existentes no Ano:\", list(json_data[0]['Dados']['2022'][0].keys()) )\n\nKeys existentes nos Dados: ['2022']\nTipo Objecto: &lt;class 'list'&gt;\nTipo primeiro elemento: &lt;class 'dict'&gt; Numero Elementos: 344\nKeys existentes no Ano: ['geocod', 'geodsg', 'ind_string', 'valor']\n\n\n\n# criar Dataframe:\n# Para assegurar o tipo de dados deveria ser especificado o tipo de atributos das colunas \ncolumns = [\"geocod\", \"geodsg\", \"valor\"]\ndata_types = {\"geocod\": str, \"geodsg\": str, \"valor\": float}\n\n# Convert the list of dictionaries to a Pandas DataFrame\ndf_ine = pd.DataFrame(json_data[0]['Dados']['2022'], columns=columns).astype(data_types)\nprint (df_ine.head())\n\n    geocod      geodsg  valor\n0  2004901       Corvo    4.7\n1  1190314      Vizela    7.5\n2  1120303       Braga    7.8\n3  11C1303  Felgueiras    8.1\n4  11A1310     Paredes    8.2\n\n\n\n# Mostrar os dados ao nivel de NUTS3:\n# Filtragem no DF - Seleção Length 7\ndf_nuts3 = df_ine[df_ine['geocod'].str.len() == 7].copy()\ndf_nuts3['codmn'] = df_nuts3['geocod'].str[-4:]\nprint(df_nuts3.head())\n\n    geocod      geodsg  valor codmn\n0  2004901       Corvo    4.7  4901\n1  1190314      Vizela    7.5  0314\n2  1120303       Braga    7.8  0303\n3  11C1303  Felgueiras    8.1  1303\n4  11A1310     Paredes    8.2  1310\n\n\n\n# ImportR gEOPackage\n# Import packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\n \n# Ler os dados CAOP\ngpk = r\"data\\geo\\GPK_CAOP_MN.gpkg\"\n\n# Ler os dados do GeoPackage para um GeoDataFrame\ngdfnuts3 = gpd.read_file(gpk)\nprint(gdfnuts3.head())\n\n   OBJECTID  DTMN                DTMNDSG                ILHA NUTS3_02  \\\n0         1  4901                  Corvo       Ilha do Corvo      200   \n1         2  4801       Lajes das Flores     Ilha das Flores      200   \n2         3  4802  Santa Cruz das Flores     Ilha das Flores      200   \n3         1  4201                  Lagoa  Ilha de São Miguel      200   \n4         2  4202               Nordeste  Ilha de São Miguel      200   \n\n  NUTS3_15 NUTS3_24  OBJECTID_1 NUTS1_15 NUTS2_15 DISTRITO  \\\n0    PT200      200         NaN     None     None     None   \n1    PT200      200         NaN     None     None     None   \n2    PT200      200         NaN     None     None     None   \n3      200      200         NaN     None     None     None   \n4      200      200         NaN     None     None     None   \n\n                                            geometry  \n0  MULTIPOLYGON (((-3463610.566 4817991.232, -346...  \n1  MULTIPOLYGON (((-3479481.263 4792025.893, -347...  \n2  MULTIPOLYGON (((-3474231.695 4797050.671, -347...  \n3  MULTIPOLYGON (((-2845106.248 4547845.673, -284...  \n4  MULTIPOLYGON (((-2812174.924 4560067.448, -281...  \n\n\n\n# Merge dos Dados:\ngdf_nuts3_2 = gdfnuts3.merge(df_nuts3, left_on='DTMN', right_on='codmn', how='left')\nprint(gdf_nuts3_2.head())\n\n   OBJECTID  DTMN                DTMNDSG                ILHA NUTS3_02  \\\n0         1  4901                  Corvo       Ilha do Corvo      200   \n1         2  4801       Lajes das Flores     Ilha das Flores      200   \n2         3  4802  Santa Cruz das Flores     Ilha das Flores      200   \n3         1  4201                  Lagoa  Ilha de São Miguel      200   \n4         2  4202               Nordeste  Ilha de São Miguel      200   \n\n  NUTS3_15 NUTS3_24  OBJECTID_1 NUTS1_15 NUTS2_15 DISTRITO  \\\n0    PT200      200         NaN     None     None     None   \n1    PT200      200         NaN     None     None     None   \n2    PT200      200         NaN     None     None     None   \n3      200      200         NaN     None     None     None   \n4      200      200         NaN     None     None     None   \n\n                                            geometry   geocod  \\\n0  MULTIPOLYGON (((-3463610.566 4817991.232, -346...  2004901   \n1  MULTIPOLYGON (((-3479481.263 4792025.893, -347...  2004801   \n2  MULTIPOLYGON (((-3474231.695 4797050.671, -347...  2004802   \n3  MULTIPOLYGON (((-2845106.248 4547845.673, -284...  2004201   \n4  MULTIPOLYGON (((-2812174.924 4560067.448, -281...  2004202   \n\n                  geodsg  valor codmn  \n0                  Corvo    4.7  4901  \n1       Lajes das Flores   13.3  4801  \n2  Santa Cruz das Flores   13.5  4802  \n3                  Lagoa    8.8  4201  \n4               Nordeste   17.1  4202  \n\n\n\n# Definir Figura e Axis\nf, ax = plt.subplots(1, figsize=(9, 9))\n\n# Mostrar Dados \n# Definir Legenda \n\nlgnd_kwds = {'loc': 'upper left', \n             'bbox_to_anchor': (1, 1.03), \n             'ncol': 2}\n\n# Generate the choropleth and store the axis\n# natural_breaks\nax = gdf_nuts3_2.plot(column=gdf_nuts3_2.valor, \n                      scheme='quantiles', # natural_breaks, quantiles, equal_interval \n                      k=7, \n                      cmap='YlGn', \n                      legend=True,\n                      edgecolor = 'dimgray',\n                      legend_kwds  = lgnd_kwds,\n                      ax=ax)\n \n# Remover frames, ticks e tick labels do axis\nax.set_axis_off()\n\nplt.title('Taxa bruta de mortalidade (‰) por Local de residência (NUTS - 2013)')\nplt.show()"
  },
  {
    "objectID": "900-mod9.html#geocoding",
    "href": "900-mod9.html#geocoding",
    "title": "9  Dados Geográficos",
    "section": "9.7 Geocoding",
    "text": "9.7 Geocoding\nAPI com serviços REST\n\nAPI Keys google e BING (vão ser eliminadas após a formação)\n\nGeoCode Key BING: At0TxnfnmV0hqD99JAtRPIZfPfQarPox_JCIPgRERq-cY99c1HLvqryhnkMLwIK0\nGeoCode Key Google: AIzaSyC-tGOoI4QrYNS3AgRuzOOMb_51Gd0RTic\n\n\n# definir as variaveis proxy\n\nimport os\nos.environ['http_proxy'] = 'http://proxy.ine.pt:8080'\nos.environ['https_proxy'] = 'http://proxy.ine.pt:8080'\n\nobter longitude e latitudede uma morada com o Google Maps API\n\n# Incluir Controlo de Resposta - Invalido API Key\n\nimport requests\nimport random, time\nimport pprint\n\nproxies = {\n  'http': 'http://proxy.ine.pt:8080',\n  'https': 'http://proxy.ine.pt:8080',\n}\n\n\nAPI_KEY = \"AIzaSyC-tGOoI4QrYNS3AgRuzOOMb_51Gd0RTic\"\n\ndef geocode_address(address):\n    url = \"https://maps.googleapis.com/maps/api/geocode/json?address=\" + address + \"&key=\" + API_KEY\n    print(url)\n    response = requests.get(url)#, proxies=proxies)\n    # Mostrar Resposta JSON (para fim demonstrativos)\n    print(response)\n    # Atenção - ao utilizar chave errado - Response é differente     \n    if response.status_code == 200:\n        data = response.json()\n        pp = pprint.PrettyPrinter(indent=4)\n        pp.pprint(data)\n        latitude = data[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n        longitude = data[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n        return latitude, longitude\n    else:\n        return None\n\nlatitude, longitude = geocode_address(\"Rua João Morais Barbosa 12, Lisboa\")\nprint(latitude, longitude) \n\n# Para Assegurar de não ultrapassar o limite de 2 pedidos por segundo seria necessário acresentar codigo deste tipo:\ntime.sleep(random.uniform(0, 3)+0.1)\n\nhttps://maps.googleapis.com/maps/api/geocode/json?address=Rua João Morais Barbosa 12, Lisboa&key=AIzaSyC-tGOoI4QrYNS3AgRuzOOMb_51Gd0RTic\n\n\n&lt;Response [200]&gt;\n{   'results': [   {   'address_components': [   {   'long_name': '12',\n                                                     'short_name': '12',\n                                                     'types': [   'street_number']},\n                                                 {   'long_name': 'Rua João '\n                                                                  'Morais '\n                                                                  'Barbosa',\n                                                     'short_name': 'R. João '\n                                                                   'Morais '\n                                                                   'Barbosa',\n                                                     'types': ['route']},\n                                                 {   'long_name': 'Lisboa',\n                                                     'short_name': 'Lisboa',\n                                                     'types': [   'locality',\n                                                                  'political']},\n                                                 {   'long_name': 'Carnide',\n                                                     'short_name': 'Carnide',\n                                                     'types': [   'administrative_area_level_3',\n                                                                  'political']},\n                                                 {   'long_name': 'Lisboa',\n                                                     'short_name': 'Lisboa',\n                                                     'types': [   'administrative_area_level_2',\n                                                                  'political']},\n                                                 {   'long_name': 'Lisboa',\n                                                     'short_name': 'Lisboa',\n                                                     'types': [   'administrative_area_level_1',\n                                                                  'political']},\n                                                 {   'long_name': 'Portugal',\n                                                     'short_name': 'PT',\n                                                     'types': [   'country',\n                                                                  'political']},\n                                                 {   'long_name': '1600-416',\n                                                     'short_name': '1600-416',\n                                                     'types': ['postal_code']}],\n                       'formatted_address': 'R. João Morais Barbosa 12, '\n                                            '1600-416 Lisboa, Portugal',\n                       'geometry': {   'location': {   'lat': 38.7680256,\n                                                       'lng': -9.1838724},\n                                       'location_type': 'ROOFTOP',\n                                       'viewport': {   'northeast': {   'lat': 38.7693255802915,\n                                                                        'lng': -9.1825668697085},\n                                                       'southwest': {   'lat': 38.76662761970851,\n                                                                        'lng': -9.185264830291505}}},\n                       'place_id': 'ChIJS_NdXMkyGQ0RlXVKqq__eac',\n                       'plus_code': {   'compound_code': 'QR98+6F Lisbon, '\n                                                         'Portugal',\n                                        'global_code': '8CCGQR98+6F'},\n                       'types': ['street_address']}],\n    'status': 'OK'}\n38.7680256 -9.1838724\n\n\n\n9.7.1 Geopandas\n\nimport geopandas as gpd\n\ngeocode de uma morada\n\n# GeoReference Morada simples:\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nimport geopandas as gpd\nimport pandas as pd\nimport folium\n# import os\n# os.environ['http_proxy'] = 'http://proxy.ine.pt:8080'\n# os.environ['https_proxy'] = 'http://proxy.ine.pt:8080'\n\n\n# Chamar Função GeoCode\n# Rua  Prof Luciano Mota vieira 42, Ponta Delgada\nres_geo = gpd.tools.geocode(\"Rua Prof Luciano Mota vieira 42, Ponta Delgada\",\n                            provider = \"nominatim\", \n                            user_agent=\"Intro Geocode\")\n\nprint(res_geo)\n\n# # Atenção esta linha dá um erro caso não foi obtido nenhum resultado\n# # Sera necessário fazer a validação de existencia de Geometria (utiliza Shapely)\n# if (not res_geo['geometry'][0].is_empty):\n#     print ('Visualizar')\n#     res_geo.explore(marker_type = 'marker',edgecolor = 'black')\n# else:\n#     print('Nao foi obtido nenhum resultado')\n# \n# print(len(res_geo))\n# res_geo.explore(marker_type = 'marker',edgecolor = 'black')\n\n                     geometry  \\\n0  POINT (-25.67936 37.74747)   \n\n                                             address  \n0  42, Rua Professor Luciano Mota Vieira, Ponta D...  \n\n\ngeocode de um ficheiro com moradas\n\n# GeoReference Morada simples:\nimport geopandas as gpd\nimport pandas as pd\nimport os\nfrom shapely.geometry import Point\n\n# os.environ['http_proxy'] = 'http://proxy.ine.pt:8080'\n# os.environ['https_proxy'] = 'http://proxy.ine.pt:8080'\n\n# Ler Ficheiros ocm Moradas\ninputfile = r\"data\\geo\\Ensino_Nao_Superior_Amadora.xlsx\"\npd_escolas = pd.read_excel(inputfile)\n\n# Criar nova coluna morada\npd_escolas['morada2'] = pd_escolas['MORADA'].astype(str) + ' ' + pd_escolas['CTT_COD'].astype(str) + ' ' + pd_escolas['CTT_AUX'].astype(str) + ' ' + pd_escolas['LOCALIDADE'].astype(str)\n\n\n# Fazer Seleção de apenas alguns registos:\npd_escolas = pd_escolas.head(10)\n\n# Chamar Função GeoCode\ntry:\n    res_geo = gpd.tools.geocode(pd_escolas.morada2)\nexcept Exception as e:\n    print(f\"Aconteceu um erro a utilizar o geocode() de geopandas: {e}\")    \n    \n    \n#print(res_geo.info())\n#print(pd_escolas.head())\n\nprint (f\"Nº de Registos do ficheiro: {len(pd_escolas)}\")\n\nNº de Registos do ficheiro: 10\n\n\nselecionar os registos com geometria\n\nfrom shapely.geometry import Point\nimport geopandas as gpd\n\n# Mostrar Resultado:\n# Selecionar Pontos com Geometria:\n# Crie uma máscara booleana para identificar geometrias válidas (resultado Pandas Series)\n# ~: Siginifca not (será seleccionado o inverso) - \nmask_valid_geometry = ~res_geo['geometry'].apply(lambda x: x.is_empty)\n\n# Selecione os registros com geometria válida\nres_geo_valid = res_geo[mask_valid_geometry]\n\n\nprint (f\"Nº de Registos do ficheiro: {len(pd_escolas)}\",\"\\n\",\n      f\"Nº de Registos resultado: {len(res_geo_valid)}\")\n\n\n# Mostrar a geografia obtida\nres_geo_valid.explore(marker_type = 'marker',edgecolor = 'black')\n\nNº de Registos do ficheiro: 10 \n Nº de Registos resultado: 7\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n9.7.2 Geopy\ngeocode com nomination\n\nfrom geopy.geocoders import Nominatim\n\n# Morada para geocode\naddress = \"Rua do comercio 42, vilar formoso\"\n\n# Inicialização do geocodificador com o serviço Nominatim\ngeolocator = Nominatim(user_agent=\"my_geocoder\")\n\n# Geocode da morada\n# Opções para mostrar: addressdetails =True, limit = 4, extratags  = Tru\n# \nlocation = geolocator.geocode(address)\nprint(location)\n\n# Verificando o tipo de resultado JSON retornado\nif location:\n    print(f\"Latitude: {location.latitude}, Longitude: {location.longitude}\")\n    print(f\"Tipo de objeto JSON retornado: {type(location.raw)}\")\n    print(\"Exemplo de parte do JSON retornado:\")\n    print(location.raw)\nelse:\n    print(\"Morada não encontrada ou geocodificação não foi possível.\")\n\nRua do Comércio, Vilar Formoso, Almeida, Guarda, 6530-270, Portugal\nLatitude: 40.6094181, Longitude: -6.8305004\nTipo de objeto JSON retornado: &lt;class 'dict'&gt;\nExemplo de parte do JSON retornado:\n{'place_id': 252240418, 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://osm.org/copyright', 'osm_type': 'way', 'osm_id': 72533753, 'lat': '40.6094181', 'lon': '-6.8305004', 'class': 'highway', 'type': 'residential', 'place_rank': 26, 'importance': 0.10000999999999993, 'addresstype': 'road', 'name': 'Rua do Comércio', 'display_name': 'Rua do Comércio, Vilar Formoso, Almeida, Guarda, 6530-270, Portugal', 'boundingbox': ['40.6067694', '40.6119459', '-6.8322906', '-6.8288417']}\n\n\ngeocode com google\n\nfrom geopy.geocoders import GoogleV3\n\n# Sua chave de API do Google Maps\napi_key = 'AIzaSyC-tGOoI4QrYNS3AgRuzOOMb_51Gd0RTic'\n\n# Endereço para geocode\naddress = \"Av Antonio José Almeida, Lisboa\"\n\n# Inicialização do geocodificador com o serviço Google Maps usando a chave de API\ngeolocator = GoogleV3(api_key=api_key)\n\n\n# Geocode da morada\nlocation = geolocator.geocode(address)\nprint (location)\n\n# Verificando os resultados\nif location:\n    print(f\"Latitude: {location.latitude}, Longitude: {location.longitude}\")\n    print(location.raw)\nelse:\n    print(\"Morada não encontrada ou geocodificação não foi possível.\")\n\nAv. de António José de Almeida, 1000 Lisboa, Portugal\nLatitude: 38.7379621, Longitude: -9.1403984\n{'address_components': [{'long_name': 'Avenida de António José de Almeida', 'short_name': 'Av. de António José de Almeida', 'types': ['route']}, {'long_name': 'Lisboa', 'short_name': 'Lisboa', 'types': ['locality', 'political']}, {'long_name': 'Lisboa', 'short_name': 'Lisboa', 'types': ['administrative_area_level_2', 'political']}, {'long_name': 'Lisboa', 'short_name': 'Lisboa', 'types': ['administrative_area_level_1', 'political']}, {'long_name': 'Portugal', 'short_name': 'PT', 'types': ['country', 'political']}, {'long_name': '1000', 'short_name': '1000', 'types': ['postal_code', 'postal_code_prefix']}], 'formatted_address': 'Av. de António José de Almeida, 1000 Lisboa, Portugal', 'geometry': {'bounds': {'northeast': {'lat': 38.7387774, 'lng': -9.136275999999999}, 'southwest': {'lat': 38.7370445, 'lng': -9.1438857}}, 'location': {'lat': 38.7379621, 'lng': -9.1403984}, 'location_type': 'GEOMETRIC_CENTER', 'viewport': {'northeast': {'lat': 38.73925993029149, 'lng': -9.136275999999999}, 'southwest': {'lat': 38.7365619697085, 'lng': -9.1438857}}}, 'place_id': 'ChIJa4H4YKEzGQ0RwerVqeaRcMY', 'types': ['route']}\n\n\ngeocode com bing\n\nfrom geopy.geocoders import Bing\n\n# Sua chave de API do Bing Maps\nbing_api_key = 'At0TxnfnmV0hqD99JAtRPIZfPfQarPox_JCIPgRERq-cY99c1HLvqryhnkMLwIK0'\n\n# Endereço para geocode\naddress = \"Rua da urbanização do tanque 8, Funchal\"\n\n# Inicialização do geocodificador com o serviço Bing Maps usando a chave de API\ngeolocator = Bing(api_key=bing_api_key)\n\n# Geocode da morada\nlocation = geolocator.geocode(address)\n\nprint(location.raw,'\\n')\n\n# Verificando os resultados\nif location:\n    print(f\"Latitude: {location.latitude}, Longitude: {location.longitude}\")\nelse:\n    print(\"Morada não encontrada ou geocodificação não foi possível.\")\n\n{'__type': 'Location:http://schemas.microsoft.com/search/local/ws/rest/v1', 'bbox': [32.66967721496134, -16.91852369231296, 32.67740265010269, -16.90628726787942], 'name': 'Rua da Urbanização do Tanque 8, 9050 Funchal, Portugal', 'point': {'type': 'Point', 'coordinates': [32.67353993, -16.91240548]}, 'address': {'addressLine': 'Rua da Urbanização do Tanque 8', 'adminDistrict': 'Ilha da Madeira', 'adminDistrict2': 'Funchal', 'countryRegion': 'Portugal', 'formattedAddress': 'Rua da Urbanização do Tanque 8, 9050 Funchal, Portugal', 'locality': 'Funchal', 'postalCode': '9050'}, 'confidence': 'High', 'entityType': 'Address', 'geocodePoints': [{'type': 'Point', 'coordinates': [32.67353993, -16.91240548], 'calculationMethod': 'InterpolationOffset', 'usageTypes': ['Display']}, {'type': 'Point', 'coordinates': [32.6735004, -16.91243093], 'calculationMethod': 'Interpolation', 'usageTypes': ['Route']}], 'matchCodes': ['Good']} \n\nLatitude: 32.67353993, Longitude: -16.91240548\n\n\nimportar um ficheiro inteiro (com bing)\n\nimport pandas as pd\nfrom geopy.geocoders import Bing\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Seus dados\ninputfile = r\"data\\geo\\Ensino_Nao_Superior_Amadora.xlsx\"\nbing_api_key = 'At0TxnfnmV0hqD99JAtRPIZfPfQarPox_JCIPgRERq-cY99c1HLvqryhnkMLwIK0'\n\n# Colunas CTT_COD e CTT_AUX são importados como numero\ncolumns_para_string = ['CTT_COD', 'CTT_AUX']\n\n# Leitura do arquivo Excel especificando os tipos de dados das colunas\ndf = pd.read_excel(inputfile, dtype={col: str for col in columns_para_string})\n\n# Concatenando os atributos desejados para formar o endereço\ndf['endereco'] = df['MORADA'] + ', ' + df['CTT_COD'] + ' ' + df['CTT_AUX'] + ', ' + df['LOCALIDADE']\n\n# Importar apenas alguns registos\ndf = df.head(20)\n\n# Inicializando o geocodificador com o serviço Bing\ngeolocator = Bing(api_key=bing_api_key)\n\n# Função para obter a localização e a qualidade da resposta\ndef get_location_info(address):\n    try:\n        location = geolocator.geocode(address)\n        return location, location.raw['confidence']\n    except:\n        return None, None\n\n# Aplicando a função para obter a localização e a qualidade da resposta\ndf['location_info'] = df['endereco'].apply(get_location_info)\n\n\n# Extraindo as coordenadas e a qualidade da resposta para colunas separadas\n# Atenção a ordem longitude (x) e latitude (y)!\ndf['coordinates'] = df['location_info'].apply(lambda loc: (loc[0].longitude, loc[0].latitude) if loc[0] else None)\ndf['quality'] = df['location_info'].apply(lambda loc: loc[1] if loc[1] else None)\n\n# Criando o GeoDataFrame com base nas coordenadas obtidas\ngeometry = [Point(xy) if xy else None for xy in df['coordinates']]\n# Criar gdf de resultado - com indicação do CRS\ngdfBing = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n\n\n\n# Corrigir para registos onde não existe GeoMetry\n# Neste caso a coluna geometry is Null\nmask_valid_geometry = gdfBing['geometry'].notnull()\n\n# Selecione os registros com geometria válida\ngdfBing = gdfBing[mask_valid_geometry]\n\nprint (f\"Nº de Registos do ficheiro: {len(df)}\",\"\\n\",\n      f\"Nº de Registos resultado: {len(gdfBing)}\")\n\n\n# Mostrar GeoDataFrame resultante\nprint(gdfBing.head())\n\nprint()\ngdfBing.info()\n\nNº de Registos do ficheiro: 20 \n Nº de Registos resultado: 20\n   DTCCFR                                          DSG_ESTAB  \\\n0  111503                JARDIM DE INFÂNCIA DA FUNDAÇÃO AFID   \n1  111503  ESCOLA BÁSICA DO ALTO DO MOINHO, ZAMBUJAL, AMA...   \n2  111503  ESCOLA LUIS MADUREIRA (STª CASA MISERICÓRDIA D...   \n3  111501  CENTRO SOCIAL E PAROQUIAL IMACULADO CORAÇÃO DE...   \n4  111503  ESCOLA BÁSICA ALMEIDA GARRETT, ALFRAGIDE, AMADORA   \n\n                                      MORADA CTT_COD CTT_AUX LOCALIDADE  \\\n0         RUA QUINTA DO PARAÍSO, ALTO MOINHO    2720     502    AMADORA   \n1  ESTRADA DO ZAMBUJAL BAIRRO ALTO DO MOINHO    2700     000     BURACA   \n2     ESTRADA DA PORTELA - QUINTA DAS TORRES    2720     461     BURACA   \n3                LARGO PADRE ADRIANO PEDRALI    2610     129  ALFRAGIDE   \n4               LARGO ROTARY CLUB DA AMADORA    2720     461    AMADORA   \n\n    LATITUDE  LONGITUDE                                           endereco  \\\n0  38.729557  -9.212133  RUA QUINTA DO PARAÍSO, ALTO MOINHO, 2720 502, ...   \n1  38.731007  -9.215389  ESTRADA DO ZAMBUJAL BAIRRO ALTO DO MOINHO, 270...   \n2  38.732370  -9.212916  ESTRADA DA PORTELA - QUINTA DAS TORRES, 2720 4...   \n3  38.733139  -9.220367   LARGO PADRE ADRIANO PEDRALI, 2610 129, ALFRAGIDE   \n4  38.733330  -9.214101    LARGO ROTARY CLUB DA AMADORA, 2720 461, AMADORA   \n\n                                       location_info  \\\n0  ((Rua Quinta do Paraíso, Amadora, Lisboa 2610,...   \n1  ((Lisbon, Portugal, (38.71221924, -9.14500046)...   \n2  ((Buraca, Lisbon, Portugal, (38.74211121, -9.2...   \n3  ((Largo Padre Adriano Pedrali, Alfragide, Lisb...   \n4  ((Largo Rotary Club da Amadora, Amadora, Lisbo...   \n\n                  coordinates quality                   geometry  \n0  (-9.21111378, 38.73031738)    High  POINT (-9.21111 38.73032)  \n1  (-9.14500046, 38.71221924)     Low  POINT (-9.14500 38.71222)  \n2  (-9.20955467, 38.74211121)  Medium  POINT (-9.20955 38.74211)  \n3   (-9.22027038, 38.7335836)  Medium  POINT (-9.22027 38.73358)  \n4  (-9.21499638, 38.73363337)    High  POINT (-9.21500 38.73363)  \n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 20 entries, 0 to 19\nData columns (total 13 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   DTCCFR         20 non-null     int64   \n 1   DSG_ESTAB      20 non-null     object  \n 2   MORADA         20 non-null     object  \n 3   CTT_COD        20 non-null     object  \n 4   CTT_AUX        20 non-null     object  \n 5   LOCALIDADE     20 non-null     object  \n 6   LATITUDE       20 non-null     float64 \n 7   LONGITUDE      20 non-null     float64 \n 8   endereco       20 non-null     object  \n 9   location_info  20 non-null     object  \n 10  coordinates    20 non-null     object  \n 11  quality        20 non-null     object  \n 12  geometry       20 non-null     geometry\ndtypes: float64(2), geometry(1), int64(1), object(9)\nmemory usage: 2.2+ KB\n\n\n\n# Exportar o resultado obtido (coluna location_info):\ndf['location_info'].to_csv(r'data\\geo\\outdfbing.txt', sep='\\t')\n\nvisualizar com explore\n\ngdfBing = gdfBing.drop(columns=['location_info'])\ngdfBing.explore(marker_type = 'marker',edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n9.7.3 Dados OSMX com OSMnx\nOpenStreetMap (OSM)\npesquisa de dados por lugar\n\nimport osmnx as ox\nimport geopandas as gpd\n#import matplotlib.pyplot as plt\n\n# Definir o lugar para qual queremos dados\nplace = \"Porto, PT\"\n\n# Verificar Existencia Place\ntry:\n    result = ox.geocoder.geocode(place)\n    print(f\"The geocoded result for {place} is: {result}\")\nexcept Exception as e:\n    print(f\"O place não existe: {place}\")\n    \ntags = {\"highway\": \"bus_stop\"}\n\ntry:\n    gdf_bus = ox.features_from_place(place, tags)\nexcept Exception as e:\n    print(f\"Não existem elementos para este tag: {tags}\")    \n\n    \nprint(gdf_bus[[\"bench\",'name','network','operator','route_ref','departures_board', 'brand' ]].head(8))\n\ngdf_bus.explore(#column = 'cuisine',\n              legend = True,\n            marker_type = 'marker',\n                  edgecolor = 'black')\n\nThe geocoded result for Porto, PT is: (41.1494512, -8.6107884)\n\n\n                        bench                    name  network operator  \\\nelement_type osmid                                                        \nnode         475347093    yes  Casa da Música (Metro)  Andante     STCP   \n             492290151    yes        Pêro Vaz Caminha  Andante     STCP   \n             492301138    yes        Pêro Vaz Caminha  Andante     STCP   \n             609719937    yes                 Lordelo  Andante     STCP   \n             987217411    NaN             Rua Firmeza  Andante     STCP   \n             987217940    NaN                 Moreira  Andante     STCP   \n             1403538447   yes       Hospital São João  Andante     STCP   \n             1471131735   NaN             Santa Luzia  Andante     STCP   \n\n                        route_ref departures_board brand  \nelement_type osmid                                        \nnode         475347093        NaN              NaN   NaN  \n             492290151        NaN              NaN   NaN  \n             492301138        NaN              NaN   NaN  \n             609719937        NaN              NaN   NaN  \n             987217411        NaN              NaN   NaN  \n             987217940        NaN              NaN   NaN  \n             1403538447       NaN              NaN   NaN  \n             1471131735       NaN              NaN   NaN  \n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nPesquisa de dados por extensão de uma GDF\neste exemplo faz a importação dos restaurantes existentes no OSM\n\nimport osmnx as ox\nimport geopandas as gpd\n#import matplotlib.pyplot as plt\nfrom shapely.geometry import box\n\n# Por exemplo Obter Dados o Extento do GPK que utilizamos os notebooks\n# Processo importar \ngpk = r'data\\geo\\BGRI2021_1106.gpkg'\ngdf1106 = gpd.read_file(gpk,encoding='utf-8')\n\n# Obter a extensão do GeoDataFrame\nxmin, ymin, xmax, ymax = gdf1106.total_bounds\n\n# Criar um poligono que representa a extensão\nextent_polygon = box(xmin, ymin, xmax, ymax)\n\n# Atenção O CRS dos dados do OSM é EPSG 4326 - Necesistamos de transformar o poligono para 4326\n\n# Isto pode ser efetuado em GeoPandas (alternativa package pyproj)\n# Necessário de definir o CRS par apoder fazer a projeção dos dados\nextent_gdf = gpd.GeoDataFrame(geometry=[extent_polygon], crs = gdf1106.crs) \n\n# Mudar a projeção para CRS dos dados OSM - 4326:\nextent_gdf2 = extent_gdf.to_crs('EPSG:4326')\n\n# Esta GDF consiste de 1 registo com o poligono\n\n# Definir Tags   \ntags = {\"amenity\": \"restaurant\"}\n\n# Necessário try e except para validar o input \ntry:\n    gdf_restaurants = ox.features_from_polygon(extent_gdf2['geometry'][0], tags)\nexcept Exception as e:\n    print(f\"Não existem elementos para este tag: {tags}\")\n    \n\n# Vai dar erro se houver problema com tags ou dados existentes    \ngdf_restaurants.explore(column = 'cuisine',\n        legend = True,\n        marker_type = 'marker',\n        edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n9.7.4 Exportar dados\n\ngdf_restaurants.to_file(r'data\\geo\\osm_restaurants1106.gpkg', layer='RESTAURANTS1106', driver=\"GPKG\")\n\n\n\n9.7.5 Exercício\n\nTentar importar mais moradas e melhorar a qualidade do endereço de input\nImportar o Ficheiro “Ensino_Nao_Superior_Amadora.xlsx” utilizando Google ou Nomantim:\n\nVer o codigo de importar utilizando Bing, com a seguinte diferença\n\n# Inicializar o o geocodificador com o serviço Google\ngeolocator = GoogleV3(api_key=google_api_key)\n\n# Funcao get_location_info para geocodificar endereco\ndef get_location_info(address):\n    try:\n        location = geolocator.geocode(address)\n        return location, location.raw['types']\n    except:\n        return None, None\n\nVer o codigo de importar utilizando Bing, com a seguinte diferença\n\n# Inicializar o o geocodificador com o serviço Nominatim\ngeolocator = Nominatim(user_agent=\"my_geocoder\")\n\n# Funcao get_location_info para geocodificar endereco\ndef get_location_info(address):\n    try:\n        location = geolocator.geocode(address)\n        return location, location.raw['osm_type']  # Adjust according to the response structure\n    except:\n        return None, None\n\nAjuda Geocoders GeoPY: https://geopy.readthedocs.io/en/latest/#geocoders\n\nImportar as escolas de Amadora utilizando o OSMnx (tag amenity e school)\n\nVer o exemplo neste notebook\n\nVisualizar os diferentes Resultados obtidos:\n\nÉ possivel de utilizar o MatplotLib para visualizar, codigo exemplo (será necessário adicionar as outras gdf\n\nimport contextily as ctx\nfrom shapely.geometry import Point\n\n# Criar variáveis para a figura\nf, ax = plt.subplots(1, figsize=(9, 9))\n\n# Visualizar a GDF\ngdfBing.plot(legend = False,\n               ax = ax,\n              color= 'green' )\n\n# Add basemap do contextily\nctx.add_basemap(\n    ax,\n    crs=gdfGoogle.crs,\n    source=ctx.providers.CartoDB.VoyagerNoLabels,\n)\n\nAtenção: Cuidado com a quantidade de endereços a georrefenciar\n\n9.7.5.1 Georeferenciar dados com Google\n\nimport pandas as pd\nfrom geopy.geocoders import GoogleV3\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Importar os Dados\ninputfile = r\"data\\geo\\Ensino_Nao_Superior_Amadora.xlsx\"\ngoogle_api_key = 'AIzaSyC-tGOoI4QrYNS3AgRuzOOMb_51Gd0RTic'\n\n# Colunas CTT_COD e CTT_AUX são importados como números\ncolumns_para_string = ['CTT_COD', 'CTT_AUX']\n\n# Ler EXCEl e indicar que colunas CTT_COD e CTT_AUX são texto\ndf = pd.read_excel(inputfile, dtype={col: str for col in columns_para_string})\n\n# Criar nova coluna com endereco\ndf['endereco'] = df['MORADA'] + ', ' + df['CTT_COD'] + ' ' + df['CTT_AUX'] + ', ' + df['LOCALIDADE']\n\n# Importar apenas alguns registos\ndf = df.head(20)\n\n# Inicializar o o geocodificador com o serviço Google\ngeolocator = GoogleV3(api_key=google_api_key)\n\n# Funcao get_location_info para geocodificar endereco\ndef get_location_info(address):\n    try:\n        location = geolocator.geocode(address)\n        return location, location.raw['types']\n    except:\n        return None, None\n\n# Aplicando a função para obter a localização e a qualidade da resposta\ndf['location_info'] = df['endereco'].apply(get_location_info)\n\n# Extraindo as coordenadas e a qualidade da resposta para colunas separadas\n# Atenção a ordem longitude (x) e latitude (y)!\ndf['coordinates'] = df['location_info'].apply(lambda loc: (loc[0].longitude, loc[0].latitude) if loc[0] else None)\ndf['quality'] = df['location_info'].apply(lambda loc: loc[1] if loc[1] else None)\n\n# Criar o GeoDataFrame com base nas coordenadas obtidas\ngeometry = [Point(xy) if xy else None for xy in df['coordinates']]\n# Criar gdf de resultado - com indicação do CRS\ngdfGoogle = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n\n# Corrigir para registos onde não existe GeoMetry\n# Neste caso a coluna geometry is Null\nmask_valid_geometry = gdfGoogle['geometry'].notnull()\n\n# Selecione os registros com geometria válida\ngdfGoogle = gdfGoogle[mask_valid_geometry]\n\nprint (f\"Nº de Registos do ficheiro: {len(df)}\",\"\\n\",\n      f\"Nº de Registos resultado: {len(gdfGoogle)}\")\n\n# Mostrar parte do Resultado\nprint(gdfGoogle.head())\n\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\openpyxl\\packaging\\core.py:99: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  now = datetime.datetime.utcnow()\n\n\nNº de Registos do ficheiro: 20 \n Nº de Registos resultado: 20\n   DTCCFR                                          DSG_ESTAB  \\\n0  111503                JARDIM DE INFÂNCIA DA FUNDAÇÃO AFID   \n1  111503  ESCOLA BÁSICA DO ALTO DO MOINHO, ZAMBUJAL, AMA...   \n2  111503  ESCOLA LUIS MADUREIRA (STª CASA MISERICÓRDIA D...   \n3  111501  CENTRO SOCIAL E PAROQUIAL IMACULADO CORAÇÃO DE...   \n4  111503  ESCOLA BÁSICA ALMEIDA GARRETT, ALFRAGIDE, AMADORA   \n\n                                      MORADA CTT_COD CTT_AUX LOCALIDADE  \\\n0         RUA QUINTA DO PARAÍSO, ALTO MOINHO    2720     502    AMADORA   \n1  ESTRADA DO ZAMBUJAL BAIRRO ALTO DO MOINHO    2700     000     BURACA   \n2     ESTRADA DA PORTELA - QUINTA DAS TORRES    2720     461     BURACA   \n3                LARGO PADRE ADRIANO PEDRALI    2610     129  ALFRAGIDE   \n4               LARGO ROTARY CLUB DA AMADORA    2720     461    AMADORA   \n\n    LATITUDE  LONGITUDE                                           endereco  \\\n0  38.729557  -9.212133  RUA QUINTA DO PARAÍSO, ALTO MOINHO, 2720 502, ...   \n1  38.731007  -9.215389  ESTRADA DO ZAMBUJAL BAIRRO ALTO DO MOINHO, 270...   \n2  38.732370  -9.212916  ESTRADA DA PORTELA - QUINTA DAS TORRES, 2720 4...   \n3  38.733139  -9.220367   LARGO PADRE ADRIANO PEDRALI, 2610 129, ALFRAGIDE   \n4  38.733330  -9.214101    LARGO ROTARY CLUB DA AMADORA, 2720 461, AMADORA   \n\n                                       location_info  \\\n0  ((R. Q.ta do Paraíso, 2610 Amadora, Portugal, ...   \n1  ((Estrada do Zambujal, 2610 Amadora, Portugal,...   \n2  ((Estr. da Portela, 2610 Amadora, Portugal, (3...   \n3  ((Largo Padre Adriano Pedrali, 2610-171 Amador...   \n4  ((Lgo Rotary Club da Amadora, 2610-184 Amadora...   \n\n                coordinates  quality                   geometry  \n0  (-9.2120616, 38.7301915)  [route]  POINT (-9.21206 38.73019)  \n1  (-9.2160293, 38.7335231)  [route]  POINT (-9.21603 38.73352)  \n2  (-9.2126978, 38.7331895)  [route]  POINT (-9.21270 38.73319)  \n3  (-9.2213138, 38.7332385)  [route]  POINT (-9.22131 38.73324)  \n4   (-9.2154748, 38.733811)  [route]  POINT (-9.21547 38.73381)  \n\n\n\n# Apagar atributo location_info \ngdfGoogle = gdfGoogle.drop(columns=['location_info'])\ngdfGoogle.explore(marker_type = 'marker',edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n9.7.5.2 Importar dados Nominatim\n\nimport pandas as pd\nfrom geopy.geocoders import Nominatim\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Importar os Dados\ninputfile = r\"data\\geo\\Ensino_Nao_Superior_Amadora.xlsx\"\n\n# Colunas CTT_COD e CTT_AUX são importados como números\ncolumns_para_string = ['CTT_COD', 'CTT_AUX']\n\n# Ler EXCEl e indicar que colunas CTT_COD e CTT_AUX são texto\ndf = pd.read_excel(inputfile, dtype={col: str for col in columns_para_string})\n\n# Criar nova coluna com endereco\ndf['endereco'] = df['MORADA'] + ', ' + df['CTT_COD'] + ' ' + df['CTT_AUX'] + ', ' + df['LOCALIDADE']\n\n# Importar apenas alguns registos\ndf = df.head(20)\n\n# Inicializar o o geocodificador com o serviço Nominatim\ngeolocator = Nominatim(user_agent=\"my_geocoder\")\n\n# Funcao get_location_info para geocodificar endereco\ndef get_location_info(address):\n    try:\n        location = geolocator.geocode(address)\n        return location, location.raw['osm_type']  # Adjust according to the response structure\n    except:\n        return None, None\n\n# Aplicando a função para obter a localização e a qualidade da resposta\ndf['location_info'] = df['endereco'].apply(get_location_info)\n\n# Extraindo as coordenadas e a qualidade da resposta para colunas separadas\n# Atenção a ordem longitude (x) e latitude (y)!\ndf['coordinates'] = df['location_info'].apply(lambda loc: (loc[0].longitude, loc[0].latitude) if loc[0] else None)\ndf['quality'] = df['location_info'].apply(lambda loc: loc[1] if loc[1] else None)\n\n# Criar o GeoDataFrame com base nas coordenadas obtidas\ngeometry = [Point(xy) if xy else None for xy in df['coordinates']]\n# Criar gdf de resultado - com indicação do CRS\ngdfNominatim = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n\n# Corrigir para registos onde não existe GeoMetry\n# Neste caso a coluna geometry is Null\nmask_valid_geometry = gdfNominatim['geometry'].notnull()\n\n# Selecione os registros com geometria válida\ngdfNominatim = gdfNominatim[mask_valid_geometry]\n\nprint (f\"Nº de Registos do ficheiro: {len(df)}\",\"\\n\",\n      f\"Nº de Registos resultado: {len(gdfNominatim)}\")\n\n\n# Mostrar GDf Resultado\nprint(gdfNominatim.head())\n\nC:\\Users\\bruno.lima\\AppData\\Roaming\\Python\\Python312\\site-packages\\openpyxl\\packaging\\core.py:99: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  now = datetime.datetime.utcnow()\n\n\nNº de Registos do ficheiro: 20 \n Nº de Registos resultado: 5\n    DTCCFR                                          DSG_ESTAB  \\\n4   111503  ESCOLA BÁSICA ALMEIDA GARRETT, ALFRAGIDE, AMADORA   \n11  111501  ESCOLA BÁSICA DA QUINTA GRANDE, ALFRAGIDE, AMA...   \n13  111503        ESCOLA BÁSICA ALICE VIEIRA, BURACA, AMADORA   \n14  111504  ESCOLA BÁSICA PROF. PEDRO D¿OREY DA CUNHA, DAM...   \n17  111503  CENTRO INFANTIL DE SÃO GERARDO - CENTRO SOCIAL...   \n\n                                               MORADA CTT_COD CTT_AUX  \\\n4                        LARGO ROTARY CLUB DA AMADORA    2720     461   \n11                                AV. DAS LARANJEIRAS    2720     334   \n13                            R. PROF. DR. EGAS MONIZ    2610     150   \n14                              R. BERNARDINO MACHADO    2720     066   \n17  RUA DE SANTA FILOMENA - ALTO DA COVA DA MOURA ...    2610     210   \n\n   LOCALIDADE   LATITUDE  LONGITUDE  \\\n4     AMADORA  38.733330  -9.214101   \n11  ALFRAGIDE  38.739212  -9.221827   \n13     BURACA  38.742123  -9.210683   \n14     DAMAIA  38.742447  -9.219668   \n17    AMADORA  38.744495  -9.211735   \n\n                                             endereco  \\\n4     LARGO ROTARY CLUB DA AMADORA, 2720 461, AMADORA   \n11           AV. DAS LARANJEIRAS, 2720 334, ALFRAGIDE   \n13          R. PROF. DR. EGAS MONIZ, 2610 150, BURACA   \n14            R. BERNARDINO MACHADO, 2720 066, DAMAIA   \n17  RUA DE SANTA FILOMENA - ALTO DA COVA DA MOURA ...   \n\n                                        location_info  \\\n4   ((Largo Rotary Club da Amadora, Alfragide, Ama...   \n11  ((Avenida das Laranjeiras, Quinta Grande, Alfr...   \n13  ((Rua Professor Doutor Egas Moniz, Alto da Cov...   \n14  ((Rua Bernardino Machado, Alto da Cova da Mour...   \n17  ((Rua de Santa Filomena, Alto da Cova da Moura...   \n\n                 coordinates quality                   geometry  \n4   (-9.2150865, 38.7338984)     way  POINT (-9.21509 38.73390)  \n11    (-9.2216721, 38.73717)     way  POINT (-9.22167 38.73717)  \n13  (-9.2115064, 38.7416111)     way  POINT (-9.21151 38.74161)  \n14  (-9.2189439, 38.7432611)     way  POINT (-9.21894 38.74326)  \n17  (-9.2117563, 38.7443726)     way  POINT (-9.21176 38.74437)  \n\n\n\n# Google não tem problema com o atributo location_info\ngdfNominatim = gdfNominatim.drop(columns=['location_info'])\ngdfNominatim.explore(marker_type = 'marker',edgecolor = 'black')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n9.7.5.3 Importar dados OSM\n\nimport osmnx as ox\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Definir o lugar para qual queremos dados\nplace = \"Amadora, PT\"\n\n# Verificar Existencia Place\ntry:\n    result = ox.geocoder.geocode(place)\n    print(f\"The geocoded result for {place} is: {result}\")\nexcept Exception as e:\n    print(f\"O place não existe: {place}\")\n    \ntags = {\"amenity\": \"school\"}\n\ntry:\n    gdf_school = ox.features_from_place(place, tags)\nexcept Exception as e:\n    print(f\"Não existem elementos para este tag: {tags}\")    \n\n    \ngdf_school.explore(marker_type = 'marker',\n                  edgecolor = 'black')\n\nThe geocoded result for Amadora, PT is: (38.758959, -9.2365233)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nmostrar todos os resultados\n\nimport contextily as ctx\nfrom shapely.geometry import Point\n\n# Criar variáveis para a figura\nf, ax = plt.subplots(1, figsize=(9, 9))\n\n\n# Visualizar a GDF\ngdfBing.plot(legend = False,\n               ax = ax,\n              color= 'green' )\n\n# Add basemap do contextily\nctx.add_basemap(\n    ax,\n    crs=gdfGoogle.crs,\n    source=ctx.providers.CartoDB.VoyagerNoLabels,\n)\n\n\n\n# Visualizar a GDF\ngdfNominatim.plot(legend = False,\n               ax = ax,\n              color= 'purple' )\n\n# Visualizar a GDF\ngdfGoogle.plot(legend = False,\n               ax = ax,\n              color= 'red' )\n\n\n# Visualizar a GDF\ngdf_school.plot(legend = False,\n               ax = ax,\n              color= 'blue' )\n\n# Add basemap do contextily\nctx.add_basemap(\n    ax,\n    crs=gdfGoogle.crs,\n    source=ctx.providers.CartoDB.VoyagerNoLabels,\n)\n\n\nax.set_axis_off()"
  }
]